-lang2-os1-en Of course. So we have a very simple application. The application have one correct, one message, one string which is to be printed out and it's linked with the library which is using some show function that displays this message somehow, somewhere, probably on the console. And oh sorry, last time I talked about several ways how to link stuff together and how linker works and we ended up about here. So I would like to show you a few examples about how the object files, the products of the compiler look like inside, so that you have a better idea which data needs to be linked together by the linker. The compiler produces some sort of an object file. In Windows, in window ecosystem this is. This uses the cough file format, which is common object file format or something like that. And basically it's like a concatenated bunch of segments of of some sections. This is a list of sections that you can find in the product of the compilation. This is actually actual object file. This is the product of compilation of the example you saw. So this is not an arbitrary file, and this is a real list of all the sections that will be present there. The most important section probably is this one, albeit the name doesn't do this justice because it's just a text. What the hell is text? Text means code in here. So here is the compiled assembly assembly code of the module. So the all the instructions that should print this message are there and the remind the remaining sections data, our data, etcetera. They're related to data stuff. For instance, I will show you data in our data in short order. The initialized data that needs to be loaded as well into the memory will be here, here X data is related to some some data, some stuff that's that's connected to throwing exceptions. That's not important. AP data I forget right now. And debug is for debugging symbols because if you compile your code into an assembly, you lose a lot of information. So if you want to debug the code later, and by debug I mean you really run it step by step or inspect the memory, what's where, which variables have what values, etcetera, then you need some additional information so that you can properly visualize your code. You can properly inspect all the variables and all this additional information which is used only 4 debugging is here in this section. You can try it yourself if you just compile whatever C code you have and then you can use this dump in tool to inspect it. If you're Linux based you can do the same thing with. Sorry I forget the name. The name of the tool is written in the notes, but there is a very similar tool GNU tool you can use on Linux to inspect LF files. LF file is the object file format which is used for Linux ecosystem. Sorry, no cat just dumps whatever is in the file. This is more sophisticated too. That really needs to understand the format and and read it in a more, let's say user friendly fashion. But you, you cannot just dump it as a, as a hex dump or something. So let's start with the data because there's this like a simplest section. The R data means raw data. There are some additional stuff by the way, D means it's it's in hexa. So so D is not. This is a number and you you can count the bytes that they are really D bytes. This is the actual raw data which which you can inspect in hex format or as a text. And this is a hint for you. Yeah, this is the string because the string needs to be stored somewhere. It's a literal which is which needs to be initialized at the beginning of your application. So it needs to be stored somewhere. So it's stored in this section. By the way, it's it says it's read only section. So it's for constants only. So this is like a initialize data, initialize global variables. Another section holds the relocations just as to refresh your memory. Last time I talked about pieces of code places in your code which which needs to be adjusted when the code is either linked together or when it's loaded to the memory. Because these pieces of code these these places in code are dependent on where exactly in the memory the code is being loaded. Like for instance addresses of of functions where the the function actually ends in the memory. This address we need to remember and put it on every piece of code where this function is actually called. So this pieces of code which needs to be adjusted are called relocations and you can list in this section, you can list all the pieces, all the places in your code that needs to be fixed. Fortunately, it's a very simple code. So there are only two relocations and these are all both are 32 bit relocations. And these are indices in the code where we can, where we can find them. This is just some notion. One of them is the address of the message itself and the second one is the address of the show function which is being called. So they are both addresses obviously and we can put this in in in connect in connection with the code. So let me just dump the disassembled code which is stored in the text in the main text section. I'm not going into the details what's what's each instruction doing, but obviously this is the important stuff we need to consider. This is the call to the show function which we were which we were waiting for. And as you can see this holds like the the call is E8 prefix. This byte says it's a call instruction and the following 4 bytes hold the address of the function. We are in a 32 bit environment, so we need only four bytes for the address of the function being called, and incidentally this address is 0. That's obviously wrong, probably due to bad contrast you don't see right now. These 4 zeros are emphasized in red so you can spot them more easily. However, the Sun is working against us today, so sorry about that. But these 4 zeros are obviously not correct. However, they are listed on the previous slide. Index F points to these four. Actually we are indexing the end, I'm not sure why. So this is the. This is the number that needs to be fixed by the loader. Once the program is loaded into the memory, then this relocation needs to be fixed and the loader, the program that's responsible for initializing and loading and starting this application, is responsible to putting the correct 4 bytes in here where the actual address of the function will be. OK, so this is how how this will work. The same goes for the message. There are another 4 bytes over here because these bytes hold the address of the message where in the memory it ends. So when the program is loaded and the address of the R data segment is is clear, then the loader will fill here the right address where the where the message will be stored. This can be done in a more complex fashion. I'm not going into the details, just to remember there are pieces of code that need to be fixed so the code will run properly after the exact address where the program was loaded in the memory is clear. By the way, a trick question. It is possible some architectures has it. It is possible that some of the core instructions or some of the smooth instructions might be shorter. We have reserved 4 bytes over here and four bytes over here. But it's possible. Sometimes not, maybe not not in here, but in some cases it's possible to replace a call or a jump function, a jump instruction with a long address with a shorter address, because the offset in the memory is not that far. So we can use for instance A relative address which is much shorter. What shall we do with the extra bytes over here? Because we we need to fill them somehow. We cannot just shrink the program, because if we shrink the program we will create additional possible additional problems along the way because we we might disrupt whatever addresses are already pre computed there. So what can we do in case we need to somehow use only part of this and and replace these bits, these bytes with something harmless? Fortunately for us, there is an instruction that does exactly nothing. It's called NOP and it has only one byte prefix, A1 byte op code, so we can place it anywhere. Every time we need to pad some space in our code, we can always place specific byte which says NOP. It's a shorthand for no operation. And this instruction is just ignored basically by the by the CPU. It just skipped by the CPU. OK, so good call. These are relocations. And enough about compiling and linking and let's move to the runtime. Support the application. Any application in any language needs some additional help from the environment because it would be very tedious. If you need to write your application in the way it will directly attached itself to the hardware or to the operating system. That would be very, very complicated. So for many things in our applications we take for granted there is some piece of code somewhere that takes care of it. For instance in CC Plus Plus we have a plenty of this support. Well it begins with the compiler. Some some part is already done in the compiler. And also we have the standard library like all other languages C or in C++ have standard library. So it has some standard set of functions and other stuff that is at your disposal. And this also starts with the compilation because you need the you need the inter interface for these particular piece of code for these for these libraries. However, there is much more going on. We also need some runtime support. So your application is isn't running alone, it always runs within some runtime within the ecosystem of the libraries which are there to support it. The most important stuff that's being cared of in the runtime is the memory control. So the organization of the storage, organization of your memory, heap control, heap allocation and this stuff. Of course, at the beginning this needs to be properly initialized. So before the execution, before your main code is, your main function literally is being executed. All the memory which needs to be initialized before that, like all the global variables are somehow loaded or filled with zeros or something. And all the constructors of the global objects needs to be called before your main function. So there's plenty of stuff that needs to be taken care of. And once the runtime is done with that, it's, it executes your main application and it's still there. It still supports you because you can still call some functions from the standard library and stuff like heap control, heap management, It's still there because you need to. Sometime you need to allocate or de allocate some objects dynamically throughout your application. And there is another thing besides the libraries, which are pretty obvious. By libraries, I mean stuff like opening files or writing to consoles or reading the system time, stuff like that, which is pretty obvious. But actually it, we take it for granted, but you need to remember somebody had to roll that. And it's kind of a complicated, it's a, it's a huge stuff, a huge pile of code. And finally, there is one important topic we haven't covered that and we need to dig in right now and that's calling convention. So how functions are being called? Actually this is much more important than you might realize at the first, because calling convention is needs to be observed by all the parties. It needs to be observed by your code, by the compiler, by all the libraries involved, by the operating system, at least the libraries of the operating system, etcetera. Because all the parties needs to use the same protocol, same convention, so that each party understand the other party. You need to lay down properly all the rules, how functions will be named, how the attributes will be encoded or how the parameters, how the arguments will be encoded on the stack, etcetera, etcetera. So the callee or the caller will do their jobs properly. Let's start with what's actually being on the stack when you're performing function call. So when you're calling something or something is being called, depending whether you're the caller or the colleague, this stuff needs to go on the stack. So first of all, there is a some room for return value. Actually, this might be a, this might be a empty space that there might be no place allocate allocated for return value in some not that rare cases, in some I I dare, I don't dare say special cases in maybe more regular cases when the return value is so small, it can be passed down via registers. But in the cases the return value is large or whether the core convention does not specify how to return stuff through registers, then we need to allocate appropriate amount of bytes on the on the stack for the return value. Actually, yeah, return value should be something like the last thing we're, we are concerned about because we step, we're just starting the call, but return value, allocating place for the return value is the first thing we need to do. And right away with that, we need to somehow stack all the calling parameters or the function call parameters on the stack as well. By the way, again, some of these parameters might be sometimes placed into registers. If you remember, if you recall the specific the ABI specification of MIPS, then first four arguments are always in registers if they fit the registers, if they are small enough to fit the registers. So if you have more than 4, then they need to be stored in here. This is by the way, prepared by the caller before the call takes place. So this is like a preparation for the call. When the call is done, this stuff needs to be stored at the return address. That's obvious. We need to somehow store where we return from the function. This is very important information and it's it needs to be keep safe somewhere in MIPS. You probably remember that we get the return address in some specific register. There is a designated register where the return address will appear and so that we don't lost it. So, so that we don't lose it, so that it don't don't get lost, we need to store it somewhere. So we save it here. The runtime is responsible for that. And another thing that needs to be saved and is the address of the previous record. This whole record, by the way, I forget to mention, but you probably read that this is activation record. For some reason it's it's called activation record or it's called a stack frame. So this frame needs to point to the previous frame because there is always a previous function on the stack. In the very least there is the main function. Your main function has been called at the beginning, so there should be activation record of your main function. So the callers activation record needs to be saved. So we need this control ring to point to the previous activation record of the core so it's not lost. We know how to return, we know how to de allocate it from the stack and any machine status. And by machine status I usually mean registers that needs to be saved so it's not accidentally changed or something. It's also saved here. Some architectures do do this automatically or semi automatically, like they have instructions for that which just dump something onto the stack. Some architectures require you to do this manually actually, not you the compiler with the runtime. So this is not your job as a programmer. This is taking place transparently in in the background, but nevertheless it has to be done. And finally, once all this stuff is saved, the rest of the stack is available to us for locally and temporary variables. So basically local variables and parameters and intermediate values of your expressions if they don't fit registers again. So everything that needs to be somehow stored in the memory because it doesn't fit registers is then stored after this record on the stack. So these things need to be stored there. And this whole thing is called the activation record. By the way. That's the reason we don't need only the stack pointer, because the stack pointer points over here and sometimes it it can fluctuate, it can move on the stack as we visit separate parts of the function code. For instance, if we visit a for loop which has declared internal variable, then the for loop and for some reason the compiler will will decide not to place this variable into the register. That might be awkward, but it might happen. So if you have something like so a new variable comes to life over here. Or maybe more variables are in the body of the loop and these variables just need to be allocated somewhere. Yes, sometimes they can fit the registers, that's the easy part. But if they don't, they need to be allocated on the stack. So the stack pointer need to move further and allocate room for them that that might happen. However, we still need somehow to keep the pointer to this record. So usually we use a separate pointer which is called frame pointer or sometimes also base pointer which usually points over here. So it points at the bottom of this part which is which is fairly thick, which is firmly fixed because you can calculate the size of this of this data structure from the description of the function. You know what the return value is, what you know what, what the actual parameters is, etcetera, etcetera. So you can, you can calculate what's the size, the size of this. At least the caller can do that. So we just put the pointer over here so we remember where the actual record ends and where the local data starts. By the way, this pointer is then used by the compiler as a base plate, as a base pointer for all the references to all the local variables. So if we need to address a local variable, load or store a local variable, this is the base address and relative offset which might point to the actual parameters or to the local data is used. And this relative offset is compiled, is computed by the compiler. Another, another thing that needs to be taken care of is calling convention. By the way, the convention as I told you this is like like a protocol that specifies all the details, namely the naming or something which is called name mangling. And it's a bit tricky. I get to it in the next slide, the sequence of the call and return housekeeping and everything else. So who is responsible for placing or cleaning up which part of the stack? Who is responsible for cleaning up the core parameters for instance, which parameters are passed via registers, which parameters are passed via stack, etcetera. And also the role of independent registers. So some of the registers needs to be preserved, some of the registers may be, may be disposed of. So this all this is specified in the calling convention. I mentioned the name mangling, it's a very specific thing. I need to explain why we are doing that. The linker which we are using to put everything together needs to somehow identify the functions being interconnected. You know in your code there was a function called show and in some library there is a function called show and the linker needs to identify I that that's the same function. The guy in the code is calling this particular function and the way how the linker does it is via the name. So the name is the only thing we can use for identification. Unfortunately this works in old C because in old C we have only regular functions and function cannot be overloaded. So once you have something like a function Foo, there may be no other Foo anywhere in your code, anywhere in your libraries. So the Foo is kind of a unique name in C This may not be true because in C, first we have namespaces. So you may have multiple fools in multiple namespaces. Second, you have you can have multiple fools. Let's say one fool processes X and another fool takes X&Y. And that's fine because the compiler can make the distinction. The compiler knows whether you're calling Foo with one or two arguments. So the compiler can say we are calling this Foo. Now that's fine. However, the linker can't. The linker can see only the name Foo and that's it. So the linker would be confused. Third, we have methods. We have functions within classes, and that's completeness. So to make the linker compatible with a more advanced computing languages, with more advanced programming languages like C++, we need name mangling, which somehow transfers not just the name, but it also somehow put in information about the namespace and the class and the and the calling arguments, etcetera, in the functional name for the proper identification within the linker. And these are a few examples, by the way. Let's start with the C examples, OK? These are various architectures, various compilers, and these, by the way, these are different protocols, different calling standards. So these are just shorthands, some identifications for various ways how you can call functions. And by the way, if you're writing your software, you should which should be compatible on the function level with other libraries or with other software. You need to specify which course are you using with your functions. So let's start with C this is C this and this is C And we have function F1 with some parameters. So in C it's easy, it's F1F1 with something F1F1. It's easy sometimes, yeah, sometimes the convention says, OK, let's prefix it with_I don't know why actually, but that that's still fine. You can, you can understand it easily, but you can see it's always F1. Sometimes it's F1 with some stuff, but but basically it's always the same. It's easy in C it's not, because in C we need to distinguish F1 not only by name, but also by the arguments after it. So we need somehow encode. It takes 1 integer, one constant character pointer and one structure S Somehow we need to place that into the name so we can properly identify it. And yeah, I cannot decode this, sorry, from the top of my head. Don't ask me how this is done. But this is almost readable if you, if you just carefully enough, examine it very carefully. The F1 is over here. So that's the name. Yeah, so far so good. The Z2, I don't know why. Then don't ask. But this is the return value. I stands for integer, PK means pointer constant character and P1S is somehow the structure. Maybe I didn't read it completely accurately, so don't just split hairs with me. It may not be entirely accurate, but somehow it's encoded over there. If I misread it slightly, it's possible, but generally it's there. So this mangling is also a very important part of the naming convention, because if you have two compilers or two pieces of software which would perform different naming, then you're screwed because the linker wouldn't identify the names of the functions that needs to be called. OK, and finally one thing I mentioned but I haven't fully expressed is who's responsible for what. So let's say this is maybe better visualization because now we have 2 activation records on the display and this is the this is the activation record of the caller of the function which is just calling another function, OK? This might might be your main function for instance and this is the show function from the example. So the main function already has its has its activation record and it's calling the callee function show. The show function needs to remember the previous address. So basically the frame pointer which points over here also stores the link to the previous activation record. So basically the link over here is then used to adjust FP when we are returning from the call. That's necessary because otherwise we cannot fix. We cannot fix the frame pointer. By the way, this is the responsibility. The responsibility refers to who's responsible of filling this data and then cleaning them up from the stack. So note that the parameters and the return value are the responsibility of the caller, not the callee. So the caller needs to properly allocate both the calling parameters and the return value on the stack, and once the call is done, it needs to remove them. For the return value, that's obvious, because the caller needs to read the return value before it's discarded from the stack, so that that should be obvious. For the parameters that might not be that obvious because it might make make sense that the callee should clean up the parameters. In the end the parameters, the arguments are basically pre initialized local variables and local variables are indeed the responsibility of the core E. Each function is responsible for its own local variables. So basically why the parameters are not responsibility of the core E but the caller? Well, the answer lies in very adic functions. Anyone heard about variadic function? Those are functions which can be called with arbitrary number of arguments usually. Well, in C it's very very tricky. Or in C In Python you probably already used variadic functions. You may not even realize that. Good example would be function like print, not some specific print. I'm just making this up. But hey, I can actually use the real function print F from C This function takes as a first argument a string with a template with some string to be printed out. But this string may hold some specific placeholders which are replaced with parameters that come after the string. So for instance I can say print F and this says I'm going to print 1 decimal number, so it will expect one additional value and this needs to be int or something integer like. So this can be converted printed out as a number. But I can have multiple such placeholders in this string, so I need multiple values afterwards. So this printf function can be called with a different number of parameters and only the caller knows how many actual parameters were put there. The callee doesn't know, the callee has to find out somehow. OK, so This is why the caller is responsible for cleaning up the parameters. OK, so this is how this works. Just the refresher since all the data stored on the stack are basically copied. So this also says that all arguments are made by value. So if you if you if you're calling a function, all the arguments or the parameters you are giving to the function are copied are given as a value. If you need to somehow pass down something which is mutable, like you need an input output parameter or output only parameter, then you need to use a pointer in C or reference in C, But it's the same thing, the reference is hidden pointer. It's just a more convenient way how to deal with pointers. So in any case it will be copied to the to the stack. But if you use reference it's it's copying the address, not the value. But in any case it will end up on the stack like this. So basically if you have a function with one parameter as a regular integer and another parameter as a reference to an integer, the value parameter both are values, but the the first one will be copied as the value. So if the value was 5, sorry, yeah, the a is 5 S the value was 5, so it's copied. The number 5 is copied here and here. Since this is reference, then the pointer of B is taken. B lies on the address 3456, so the 3456 is copied in here instead of the value of B. But it's still the value. It's still some data stored on the on the stack. This is just a refresher. We already covered this in the C startup lectures. So basically this is just to connect, to get your information connected with something you learn at the beginning of the semester. OK, a few more words about variables. So now on. You should get us some notion where each variable lives. The static data and the global data are in static segment which is initialized at the beginning before your code starts and it's placed on in one location in your memory space and it's fixed. It doesn't change size during time and it's always there till the end of your application. Steak serves as a temporary lodging for local variables and for function arguments, which are basically the same from our perspective. And 3rd heap. Also your recovered heap is for dynamic memory allocation. So if you need to allocate some data and you don't know a priori the size of the data because you need to 1st list it or get the input from the user or something, then you can allocate it as you go throughout your your runtime. But the price we pay for that is the heap management. You already seen the heap allocation algorithms. There are some problems like fragmentation, etcetera. Yeah, those cannot be avoided. That that's the price we pay for dynamic allocation. This view, this notion of variables is, let's say, specific to compiled static languages like CC. But this will also almost except for the heap. The heap is, but not entirely. It will also hold for languages like C or Java. However, there are also dynamic languages like Python. You have already seen Python. Some of you may also see PHP or JavaScript. Those are the other two most popular languages I can come up with. These languages actually don't use this type of distinction. They also have global variables. They also have local variables as a model, so the programmer will feel at home. It's a good idea to have global variables and local variables separate, but the variables doesn't mark specific place in memory. If you have a variable, like if you see X as a variable somewhere was int X in your C code, I can tell you exactly where this X is in the memory. Well, maybe not exactly. I need to choose. I need some information like where the application was loaded, what was the offset I need, I need to examine the object files or the executable file so I but I can find out where in the memory the X lies. I can point my finger to it. I know it will be always here in this piece of of memory, either on the stack or in the static data segment. However, in languages like Python, X doesn't mean that X is just a name in some huge dictionary. So imagine in Python you don't have this type of memory allocation. All the variables are in some huge dictionary where the names are translated to some arbitrary values. So basically you have like a huge cloud of names and every time you can ask this cloud, hey, I would like the value of X and it just gives you some value. So it's a different approach because it's much more costlier. You need to look up all the variable accesses first and 2nd. In these languages, X doesn't have specific type. Here I always need to define that X is integer. And we know what integer means on this particular architecture. So we need. For instance, it takes 32 bits in PHP or Python. You ask for X and you get something, and it might be integer. By the way, integer in Python doesn't take 32 bits. It might, might not. It usually doesn't because it has arbitrary precision. So you can, you can have an integer that grows indefinitely. So that's the price we pay. But the benefit is that you don't have to get too much concerned about the variables. I'm telling you this just that you get the idea that there are some types of languages that takes variables and memory management very seriously and other types of languages, usually script languages or dynamic languages, which kind of a don't care about that too much. On the other hand, if you're writing something in Python And you're really concerned about the memory consumption or speed, then you're doing something wrong. You should have chosen a different language. On the other hand, if you're writing something very quickly and you need to rapidly prototype your 20 line code, then Python might be a good idea. A few words about heap. The allocation was pretty much covered in the part where we talk about memory. So I'm not going into more details about just to remember. Yeah, we can use heap to dynamically allocate memory, that's fine. De allocation is the topic we haven't covered thoroughly, at least not from the perspective of programmer. If you want to release your variable, if you want to release the memory you previously allocated in, I wouldn't say Nah, I'm looking for the right word. In serious languages, which take memory management very seriously, you need to do this explicitly. So you need to say, hey, I'm no longer using this memory, get rid of it. This is like CC. You need to say to the runtime this memory should be freed. We no longer need it. And I'm declaring that if I ever use this memory again, it should be considered a bad idea, it should be considered a fault and should blow up in my face. On the other hand, this is tricky because usually it leads into many errors. The simplest error you can do is that you do nothing regarding your memory D allocation. And if you don't care about your memory D allocation then sooner or later you end up with too much memory and you cannot allocate more. This may not be a problem if you have like a one time script that just runs and then it ends. But if you have a mission critical system or if you have at least a server that runs for days or weeks or months, it, it might be a problem if it even if it slowly grows in memory. So even if you forget to release tiny small bits of memory every now and then, the accumulative error over several months or over years may be disastrous. So it it's very error prone. That's why that's why some languages said, hey, programmers make mistakes all the time. So screw it. We, we don't let the programmers de allocate the memory on their own. We will take care of it for you, for them, because we know better. We won't, we won't fall through, we won't forget. We will do this all the time. So they implement mechanisms that de allocate the memory automatically once it's, it's no longer used. So every time some piece of memory is kind of stopped being used, then the system will automatically get rid of this memory. Well, obviously, as you gathered, there is a catch how to find out that some piece of memory is no longer used. It will no longer be used in the future if the programmer hasn't told us. Well, in C, the programmer will tell us, hey, I'm no longer using this piece of this object I've allocated previously. So since then afterwards, it's obvious no one will ever use this object afterwards. Fine, in C you just allocate an object and then sometime later it gets disposed off. How do the compiler, how did the runtime, how did the garbage collector that takes off, that takes care of the things? No, it can be removed. Well, there are several approaches to that. One approach is tracing. This is actually what C# or Java does. So they have some set of let's say active or hot variables. This include all the global static variables which can hold references and all currently active local variables and transitively it scans all the objects. So if you have like a let's say global variable that's a reference to an object then it transitively scans all the member variables of the object and look for other references. So this this needs to be a very depth wide search so all the references are traversed and and mapped properly mapped and that takes time. So C# doesn't do this all the time. Every now and then, usually when it starts running out of memory, it needs to stop all your execution. It freezes. Your program scans all this references to the depth mark all the used objects and get rid of everything else. So everything that wasn't marked during the sweep is removed from the memory. Yeah, it works. The price that we pay is execution stall every now and then. Your execution is frozen. And while it's frozen, the garbage collector can do it's stuff can can remove remove unused objects. The trouble is that for the most part, you don't have a direct control over this behavior. You cannot say, hey now I really need to run. Sometimes it just interrupts your program in the very inconvenient moment. So it it's get embarrassed, embarrassing because the user just clicked on a button and the the the click just triggered an algorithm that requires more memory and there is no more memory. So the garbage collector kicks in and the button just freezes because the the program is performing garbage collection instead of the the interactive feature that was implemented. So the user thinks that the program is broken. Yeah, that might happen. Worse, you use C# or Java for something like self driving car and you're approaching very speedily towards a crossroad and a pedestrian walks in and the system starts for the detection of pedestrians. Needs more memory. So your car is still riding, but the self driving system is waiting for the garbage collector. That also might happen. Obviously not. These systems need some fail safe so that won't get stored by garbage collectors. But just to get the idea, it might get troublesome in some cases. Another possibility would be reference counting. So let's say that every object that can be allocated, every piece of memory that can be allocated is associated with a counter. And every time you copy a reference, this counter gets incremented. And every time a reference is removed, the variable is removed from stack or or transitive reference from another object is removed or something, this counter is decremented. Once the counter reaches 0, the object can be discarded, right? This is very efficient and very easy way. It's much easier to implement than the previous one than tracing. The tracing is quite complex and and you need to see all the operations of your application. This this can be implemented with a with atomic variables. For instance can be implemented without disrupting the the runtime of the rest of your application. However, there is this problem. Let's say that we have object P which has internal variable that points to Q, and the object Q has an internal variable that points to P These two bodies will support themselves indefinitely because thanks to this reference, this counter for this for this object will never decrease below 1. And thanks to this reference, this counter for this object will never decrease below 1. So these two objects will never be discarded, even though there there will be no external references to either of them. They will live there forever. So yeah, this is a imperfect solution for imperfect situations. However, these are still being used. For instance, PHP uses such a such a the garbage collecting system. Because PHP was originally designed to handle short term scripts and scripts that run only as a response to a particular HTTP request. So it's expected that such a script is running for a very short time. So for a very short time, this should be a good idea. This should be a good garbage collector because even for a short time, you might need some garbage collection doing. But then again, if such thing happens like this cyclic reference, then what the heck, we will just clean it up, clean it up at the end when the script terminates. OK, so this is about garbage collection by the way C++ isn't today entirely without any support. But the approach languages like C++ or probably Rust are taking now is we are doing the garbage collection or let's say the memory disposal semi automatically. So it's not OK to rely 100% on the on the programmer. But what we are doing in in CC plus plus for instance what we are usually doing is that we are creating references like unique pointers. So instead of raw pointers which can be faulty, we are using these specific unique pointers which are in fact classes which are like smart pointers. And once a unique pointer cease to exist, it automatically dispose of the memory it's pointing to. So basically the programmer can't forget about disposing the memory. This smart pointer will take care of it. But the price you need to pay for that is that you are always using smart pointers instead of regular pointers. If you are always using smart pointers then you're probably fine and it's better alternative to the garbage collection from the perspective of performance. From the perspective of programmer convenience, a garbage collection is always better. OK, a few more additional words regarding the code compilation and program design. About portability, you've probably already written a few lines of code in Python, right? I hope so, because otherwise I don't know how you pass the first semester. So if you wrote some some pieces of code in Python, just out of curiosity, how many of you have coded in Python in more than one platform? Have you tried multiple platforms? Have you tried to run your code on at least Windows or Linux or Mac and Linux or something? A few people? Have you run into any troubles regarding running your Python on 2 platforms? I would guess none whatsoever, right? That that would be a fair guess, and I hope so, because Python you only. The only thing you need for Python is that you have a compatible runtime with a decent version. So if you're writing something which is compatible with Python 3.11, then your runtime is compatible with 3.11 or newer or something and then you're fine, no problem. If you're writing in C or C++, it's much more difficult. First level, you already witnessed in your Arduino classes. Different CPU architectures may involve your code on the lowest way. For instance, you have different type sizes, different in size. You all witnessed that you have int, which takes 16 bits on Arduino, but it takes 32 bits in a regular PC. Yeah, that's what happens. By the way, this can be easily fixed because some languages with a similar problem just said, hey, we are embedding the size of the type into the specification of the language. What the heck? Integer is always 32 bits. That's what C# or Java does. If you write in C#, your integer is always 32 bits wide, regardless of the architecture. So that that might not be a bad idea. On the other hand, in C++ there are ways how to do that as well. You can for instance use a specific type specific type aliases like U, int 32 sorry 32 T it's in the STD namespace. This is in the standard library. You can use it so there are specific type aliases you can use. And this type alias is fixed. It's somehow defined so that it's always 32 bit wide regardless of the architecture. You mean this one? This is the namespace separator. So this is the namespace and this is I'm accessing the namespace STD and this is the identifier within this namespace. So these two, these four dots are used to separate namespaces and identifiers or class names and identifiers. It's just a token. It's not that it has no special meaning. It's just a concatenation. So yeah, we have ways how to fix this in C++ as well, but you need to take extra care. Another level is the compiler. Unlike Python, which has, maybe that's not entirely true. Well, for PHP that's true. So unlike PHP, which has one interpreter, C++ has many compilers. Just from the top of my head, you have the GCC which is probably the most popular GNU Linux compiler. Microsoft has its own compiler. There are another compilers like Clank which is very popular nowadays. Intel has its own compiler. Many other players created their own C or C compilers, so you can use them all. And theoretically, since C has specification, has a very well defined norm for how to how to write things, how to write compilers, etcetera. They should be compatible, but they are not. So if you want to compile something for multiple of these compilers and usually you need to. For instance, I I used to write a lot of code that that needs to be compiled with the MCSV so we we can run it on Windows easily and also with GCC so we can run it easily on Linux nowadays. Nowadays, fortunately it's not that necessary because you can easily compile with GCC for Windows etcetera. But sometimes it's a good idea to support multiple compilers and it's pain in the *** because each compiler has its own flavor. Each compiler use slightly different approach. For instance, if you write in Visual Studio and you make yourself compatible with Visual Studio C and it works and everything's fine, and then you just move it to GCC. I tried to compile it for Linux and it just blows up into your face because Microsoft guys were very, let's say, more user friendly in the terms that they want to support less fortunate programmers. And they try to make the compiler very friendly in the way it forgives you if you just don't follow the standard rigidly and make some tiny mistakes. It says, hey, it's obvious what you mean by this code, so I'm not going to badger you about you. Forget this specific keyword that needs to be there because it's obvious you meant what you meant, so it's fine. But then you compile it with GCC and it blows up in your face because you didn't follow the specification of C++ rigidly. And he just said, hey, I'm expecting this keyword no matter what, so you need to place this superfluous keyword that need that sometimes needs to be there, sometimes not, but I want it there all the time. So sometimes it's difficult to make something compilable by multiple compilers. By the way, if you start with GCC, you probably will be fine, because I think GCC is one of the most pedantic compilers in the world. Then again, beware that some of the compilers like GCC have their own flavors, so it has some additional features and you should avoid using these additional features. And third level is obviously the operating system. I mentioned here that I wrote code that was expected to run on Windows and Linux. Sometimes it's easy because if you're doing something only using the standard library, like opening files, accessing the the console, etcetera, it's easy because it's the standard library takes care of all the nuances or all the differences. If you're not. If you need to do something more low level or something specific, then you need to write it in two ways. You need to write one piece of code for Linux and one piece of code for Windows, and then probably write some abstraction that covers both versions so you can use at least the rest of your application. You write the rest of your application only once for these. Sorry, I don't have libraries over there. So for these purposes it's a good idea to look for existing libraries like Boost for instance, which already take care of multi of portability of multi OS support. So this tedious work when you need to write your code multiple times for different operating systems was already done by someone else and you only need to use the library which already abstracts something away for you. Again, this is just a message. A portability is much more difficult in lower level languages than in Python. In Python you just, you're fine, you just need the runtime. In C it might be much more much more complicated by the way, for the binary compatibility. So this is not another level. On the previous slide I was talking about source code compatibilities or source code portability. So how to compile your source code for multiple operating systems, devices, etc. Sometimes you also need the portability of already compiled code. You're not always distributing your code as a source code by the way. Most people doesn't, unless you're writing in JavaScript or something. But usually you want to distribute your code as a binary code, as something that's not that transparent cannot be the the ideas cannot be stolen away just by copying in the code. So for that we need binary portability. For C, binary portability is very difficult to achieve. Yeah, we can. We can do that with some virtualization or with containers, but but it's difficult. But other languages like Java or C# already thought ahead and they embedded binary portability in their design. And here the binary portability is achieved by using by the usage of intermediate languages. Instead of compiling the source code of Java or C# directly into the instructions of the target CPU, they are compiled into the intermediate language which is called byte code in Java and cell in C You may already have heard about this. So this is almost like instructions. This is like a very close to assembly language. But this code actually it's, it's like an assembly language but for a virtual machine, not for actual particular CPU, but for a virtual non existing CPU which is called Java runtime or C runtime. And these environments, so Java or C# actually only implement a very tiny piece of software which is called the runtime which interprets. So it like executes virtually these instructions, it reads the byte code or it reads the cell and it tries to interpret each instruction in this byte code as it goes. So it's like a virtual CPU. And the only thing they, the authors need to port for different hardwares or different architectures is this runtime. The rest of the software, the rest of your software needs not to be ported because it's interpreted on all the machines in the same way. However, this is quite inefficient, right? Because if you really need to interpret every instruction, you need at least several instructions to perform the task. So it would be several times slower and nobody would want that. So there are techniques how to speed things up and these are just in time and ahead of time compilation. So they realized, hey, that that won't work. Just pure pure simulation, pure virtualization wouldn't work. So what they actually do is that they try to, they start running, sometimes they start running, especially in JIT your code in this virtualized manner. And then then they realize, hey, this code is run many times. So I just will compile, I will take the byte code and recompile the byte code into the code for the specific architecture I'm running on right now. So the compilation part of the compilation is differed from the compilation process of which programmer does during the development to the runtime process when the actual program is started. So nowadays, usually when you're starting an application application like in C# or Java, part of or sometimes entire code is compiled once more just when it starts. The head of time means it's compiled altogether. When the application is loaded. Just in time means that the small portions of code are being compiled on demand. But basically it's the same technique. It differs only in the timing when the actual compilation takes place. OK, that's about it. I think we covered all the topics we have prepared for the language and compilation session. Are there any questions regarding compilers, linkers, runtimes? OK, no questions so far. Fine, so let's move to another topic. Just a second. And another topic is the operating systems. Operating systems will be a slightly larger topic. We have scheduled 4 lectures. I hope we can squeeze it a little because we are running out of time as I mentioned at the beginning. So operating systems actually before I start operating systems is a very huge topic. So this is like an introduction. There is another another subject which which covered the this topic in more detail and where we can actually write piece of operating system kernel. Oh yeah, this was just a landmine I prepared for my colleague Jacob. So let's move on. Operating system is basically a piece of software that has two major roles. The first role is abstraction. Abstraction is very important because you don't want to design your software to be compatible with whatever possible hardware is out there. I will give you a let's say slightly nostalgic story that that's related to this. When I was young gamer I used to play games which first introduced more modern sounds. Nowadays you are very used to high defined sounds in your applications and and games and everything. And if the sound is in 3D and played on on 7 different boxes, that plays the sound or whatever. It's considered low tech, so yeah. But back in my days, the speaker of the of the PC was much like the speaker in your Arduino Shields. So it was just beeping and the very new piece of hardware that was emerging back then was a sound card. I'm not sure whether you know what sound card is, because nowadays sound cards are integrated parts of your motherboard or your laptop. So you don't even know they are there. It's it's like they are automatically always there. But, but back in these days, you need to, to buy a very expensive piece of hardware that needs to be added to your PC. And this specific piece of hardware was capable of playing sound in let's say normal way. So it was capable of playing music or doing realistic side effects, not just beeping. And the first games that start using these sound cards were like, you just open the game, you need to open the setup in the game. And then you you have like 20 possible sound cards or sound card systems and you need to select the right one. Usually they didn't have the one you had but you need to select the compatible one. And every game just re implemented its own driver for 20 different possible sound cards. And if they didn't, you were out of luck. You couldn't play the sound in your game. So it was very tedious for the game developers. It was also very tedious for game users because sometimes they didn't do it right or sometimes we have slightly different different cards and suddenly the sound wasn't working. Or worse, it was working but in a bad way. So it was making noise instead of sound. So that's like a world in which I was growing up. Fortunately for today, we use much more, better abstractions. And one of the roles of the operating systems is that no application, no game needs to implement its own drivers for sound cards, for video cards, for keyboards, etc. They are just using the abstraction provided by the operating system, which is well defined. The same for all your keyboards. You can imagine there is the same abstraction for all the audio cards. For all the sound cards there are, there is the same abstraction. The application just needs to ask the operating system to play some sound and the operating system do the difficult stuff like loading the right driver, communicating with the hardware in an appropriate way, etc. So this is the first important role. The second role is the resource management, because there are many resources that needs to be shared among applications in your system, in your PC. Again, back in my days, it was like you were always running one application. In MS-DOS, you just run one game and that's it. Everything else was suspended, everything else wasn't running. Only this one game was running and and that's it. In your regular Windows or micos or whatever system you're using, you're probably running at least 10 application simultaneously. Sometimes you don't even know about it. So even if you open multiple windows with browser or multiple tabs within one browser, that's already actually using multiple applications. And all these applications need to somehow share the resources they all have but one CPU. They all have but one main operating memory. They all have but one hard drive usually. So these resources needs to be somehow shared by these applications. And basically there are two main ways how to do that allocation. This works well with memory. Yeah, I have one application that needs 1 GB of memory over here, Another application needs another GB. So we just place that GB over there and we can manage to handle these two gigabytes separately, right? Other things cannot be shared this easily, like CPU. Yeah, technically you might have multiple CPU cores. So technically you can allocate 1 core for one application, another core for another application. That would work in some situations, some scenarios. But if you just open your regular Windows task manager or your or just list all your processes on your Linux or whatever, then you you can find out that there are 10s or hundreds of applications or processes or services running simultaneously. And it's not possible. It's not feasible that you would allocate one CPU core for each. That's just not possible. So we need time sharing. You just borrowing the CPU or CPU cores to individual processes, to individual threads actually for short periods of time. So they can compute something, they can do their stuff, but afterwards the CPU is taken away from them and it's given to another process. And this, if you do this fast enough, you will create an illusion that they are like sharing the CPU in a in a very fair in a in an every very efficient way. And also some parts needs to be abstracted. For instance hard drive or also network needs to be abstracted in a way that you are never using hard drive directly. No application. Well, there may be exceptions like if you have a system application for defragmenting the hard drive or something or fixing the errors on your file system, then yeah, these applications might use the hard drive almost directly. But usually normal applications doesn't access the hard drive itself. They are using the file system abstraction. So they are operating at the file level. They are reading files and writing files, reading directories, writing directories, but they are never accessing the hard drive. Actually, your application doesn't even know whether the file it's actually reading is on the hard drive or is being transferred via network to you, or it's being currently stored in the main operating memory and it's just making an appearance. It's a hard drive. All things are possible nowadays. So there's a specific abstraction and the application doesn't even take, it doesn't even bother itself with the details like where the actual file is being stored. So this is about operating system introduction. And before we get started, let's start from the CPU. And for that, we need one piece of information I've neglected to mention when I talked about CPU architectures. Well, actually, I didn't neglect to mention it. I didn't tell you this information deliberately, so you wouldn't be bothered with that. Before we actually need it, but now we really need it, so let's start with that. CPUs have at least two modes of operation, and by modes I mean privileges privileged levels. One level is for everyday regular applications, so it's usually called the user mode. So normal applications are running in like a sandboxed or a restricted regime of the CPU. They cannot do everything. They can do only the regular instructions. They can compute, they can access memory, they can do the regular stuff, but they cannot, for instance, access the system registers that tweak how the CPU behaves. For instance, they cannot handle things like interrupts. We get to interrupts later. So they cannot communicate directly with the hardware, from the CPU, with the external hardware. And besides that, there is this other mode which is called kernel mode or system mode or privileged mode. And in this mode, usually the CPU can do everything, it can communicate with hardware, it can execute all the instructions, no instructions are restricted, it can access even the system registers etc. And the tricky part is how to ensure a secure and efficient transition between these two modes. Actually the transition from the kernel mode to the user mode is quite easy because if you are in the kernel mode, your omnipotent, you can do everything you can you can. So if you can do everything then you can just say hey I'm restricting myself to the regular mode no problem. But the tricky part is how to securely enable transition from user mode to the kernel mode. Because if the application just can say, hey, I want to switch to kernel mode, then there will be no security at all. If every application can just switch to the kernel mode every time it wants and just continue executing its own instructions in privilege mode, then there would be there wouldn't be any security, there wouldn't be any reason for this. So what what's there is there is a trap. There is a specific instruction which is called syscall. It behaves almost like a regular call instructions or regular jump instructions. Actually not not jump call. We need to return from this call. So it's it's like a call instruction. But embedded in this instruction is the increase of the privilege level. So within this call is also the transition of the CPU mode. So when you perform this call, it also jumps. It also switches to the kernel mode and the the catch is the tricky part is that you cannot call arbitrary function. You cannot call arbitrary piece of code. Actually you are not even specifying what you are calling. You are just performing this call. And the kernel was responsible for filling the right address of the function of the system function which is being called into some system register which cannot be altered from the user mode. Let me explain an example. Let's just perform something very simple. We are calling printf hello. So this is very straightforward piece of code, The most easy code I could probably write, the easiest code I could write in C this will translate into something like this. The call itself actually, well, this is not entirely true because this is actually call to a Lib C function which is somewhere in the runtime of the C, But somewhere in the print F embedded deep inside will be the system call. Because yeah, we need to perform at least one sys call so that we can take this data and put it out to the operating system. So the operating system will use it and write it to the stream, to the standard output stream. And here is how this sys call will look like. Before that, there will be some initialization. We place all the important stuff into registers and our operating system provides us with the guidelines, with the manual which operations of the system has which op code. So this Rix and RBX, actually RBX 1 means we are writing to the standard output, but the RAX is the right operation. So somewhere somehow in some manual, somebody wrote that if I want to print something out into a stream, I need to put 4 into RAX. So yeah, that's it. Don't ask. It's it's copied from the documentation. And these these additional parameters are the number of the stream to which we are writing, the address of the message we are writing, the pointer to our memory which we want to dump, and the length of the data that will be dumped into the written into the stream. OK, so we are putting the core arguments into the registers. And by the way, all the call arguments must be in the registers. And then we perform syscall without any explicit argument because the syscall will actually take this register. You haven't seen this one. This is a system register. It will take this register and take the address in this register and jump to this address whilst switching to ring 0. This is Intel terminology for the kernel mode. So they call. Ring 0 is the kernel mode, ring 3 is the user mode. They have a specific register where this number 0123 is stored, and in the past they have a. They thought it might be useful to have multiple levels, not just the kernel mode and the user mode. Sorry about some levels in between. So they they said OK the ring zero that the main level this is like a root level would be would be the kernel mode and the user level will be ring 3 and there will be ring one and two in between. But later they find found out that no operating system wanted this distinction. They either want to run application software or they want to run the kernel. So in the end we are no longer using the intermediate rings. I'm not sure even if they are supported in the hardware right now, but the terminology remained. So in here we switch to the kernel and call the kernel function. The kernel is responsible for handling whatever we put into the registers and performing the task we ask it to do. And then someplace in the kernel code it calls sysreturn which will actually jump back into our application while switching back to the system system mode user mode. Sorry, is this clear? So it's not that difficult from the perspective of the programmer, but internally in the CPU there is plenty stuff that needs to be done. And also this would be very tedious for us to write. So we are usually not writing this. This is written. This is embedded somewhere in the libraries of the operating system where. Well, what do you mean by security reliability? Could you could you like elaborate on that? Could you present some vector of attack that that could be exploited via this? Like surely if any random like what is something in state for example, like what it would be interpreted as characters that we just printed out the The thing is that this register holds the address in memory where we are jumping. This address in memory isn't accessible by the by the memory of the operating of the application which is running. So the application cannot override this address nor the code which is at the address. That's the whole point. Yeah, I see where you're taking this. So there is another mechanism which protects the memory, and we haven't talked about it yet. We started with the CPU, so obviously we need memory protection as well. The application cannot just write everywhere on the memory. The application has its own memory and it cannot write anywhere else. That's the whole point. So the kernel memory itself, it's protected. It cannot be accessed. So yeah, if it can be accessed, then yeah, we can inject something into the kernel memory and then perform the jump. That would be that would be disastrous. But thanks to the memory protection, this is not possible. Yeah, I think this would be a good example to to cover for the end. So just to give you some notions, what actually are and aren't system calls, let's just take this piece of code and examine this from the perspective of what syscalls will be performed when we are doing this code. By the way, just to explain for those who are not familiar with streams, we are reading this file, this text file, we are reading it into a string and then we just print it out to the standard output. This is just an elaborate way how to read a file to a text file into a string. Yeah, it's slightly more difficult in C than it is in Python to read the whole text file in a stream in a string, but yeah, this code will take care of it. So I think I will need the example, hopefully. Here we go. So this is again the code. I think I can scroll it out. This is the contents of the text file. This is actually if you want to replicate, replicate this. This is what we did. I just compiled the code using standard GCC G because it's C code and I run S trace. S trace is a Linux tool that runs an application. I wrote it as a prefix of the application itself. I run S trace app and it runs the application and it monitors all the system calls that they are performed. And I just save this S trace, the output of this S trace into the file and this is the dump of this file. So let's take a quick look on what's going on here. So first operation is some exec and yeah, sorry, this is a confusion. I somehow I renamed it somewhere. So I I wrote yeah, it's it's main, but I wrote up here, but it was main. My mistake. Just imagine that there was main over here. Sorry, I I just mixed up when I was fixing the the stuff for the slide, but it's not that important. So this is basically we are loading entire main executable into our memory space and we are starting executing it. So this is how every application actually begins with loading its own code. Yeah, and we need to access you. You may notice that we are accessing several things like, ETC. Refers to files that are that holds the configuration. So we need to find out where are the loading library path is etcetera. And we need to load several libraries. This is the the runtime for standard C++ DLL actually. SO dynamic, dynamic library. And there are several more. All this stuff we are just loading. We are just loading libraries. By the way, do you know what MMM map is? Anybody knows? This is memory map and it's used to take part of a file and map it into memory, like load a huge block of file in a memory. It's faster to map this stuff into memory than to use standard read and write operations which are much slower. And we are finally approaching the last part. So once all the initialization and take a note that there was plenty sys calls just to initialize, just to start up the application and we are finally get to the stuff which is of our particular interests. So let's start over here. This is open at. Yeah, this is just some note, some shorthand for open operation with some specific attributes. So we are opening the atxt file in the read only mode and we are getting #3 as a file handle. Why the heck we are getting 3? This is the first file we are opening. Actually, that's not true. We have opened many files as libraries before. But you can see that the first file we are opening over here also yields 3. Why the heck the first file, the first handler we are getting is 3? Why not 0 Or OK, 0 might be reserved for for failures or something. So why not one or two for some for for? Well actually they are not reserved, they they just already exist. How that's possible that we just started an application and there are also already file handlers that exist? There is standard input and output. Actually there are three input, standard output and error output 01 and two. So the three is the first available file handler there is. So This is why every open gives me 3. And by the way, if you follow up, every open has its corresponding close. So every time we open something, it's closed so this file handle becomes available again and the next open will also get 3. That's why we are getting 3 here. When we are finally opening our own text file, right then we perform read 33 is the file handler. This is the result of the. This is the buffer which will be filled. This is the maximum amount of the buffer. The read will read up to 8000 bytes, 8 kilobytes and it yields 21 which says hey we only find 21 bytes in this in this particular file. So hey, I would like to satisfy your L8 KB, but I don't have more stuff to fill you. So this is the read which we get. Then there are some stuff which is not that important and right, we are writing back the contents of this buffer with the appropriate size now, because we already know how many bytes we read and that's it. Why the hell we are writing to 1? 1 is standard output, 0 is standard input, 1 is standard output, 2 is standard error. You probably know that from Linux classes already, right? Have you have you already redirected your outputs to standard output or, or error output? So so you already know that one is standard and then 2 is error output. So that's it. This is the yeah, this is the example of a strace. And I've just run out of time today and we agreed that I won't be, at least I will be trying not to prolong the, the lectures too much. So as I announced at the beginning, some, some people weren't there weren't here at the beginning. So let me let me repeat that due to my absence last week, we created, I created an unfortunate situation because we were already very tight in our schedule. We were already behind our schedule. Unfortunately this year we have only 12 lectures and the stuff the slides are to 12 1/2 lecture. So we are one lecture and a half about behind. So we agreed that most people would like to have an extra lecture instead of prolonging all the lectures every week. So I will write to you and with some poll where would be the best time having this extra lecture. I would guess there are two options. One option is that we somehow fit it in the the vacant space in the either 1st or 8th May because the those weeks are the the weeks where the holidays are. Or we will do it some sometime at the end, because in the end, we will know how much time we, we actually need to, to finish everything. And it will be probably either the one week after the semester. So one week, the first week of the, of the exam. Or sometime at the end of the semester. I will investigate possible options and I will send you a poll which option is the best for you so that most people could come. So thank you for your attention and hopefully nothing will disrupt our flow next week. So thank you and and see you next week.