-memory2-lang1-en I've ended on this slide, so we didn't do anything. I haven't told you anything about caches, so let's pick up here. Caches are perhaps the most important piece of hardware and you will hear about them all the time, especially if you browse into operating systems or into high performance applications. Because every time you need to do something that has something to do with with memory and you either need to do it in a more secure or more sensitive way or you need to do it in a high performance way. You're really concerned about the performance of your application? Then memory caches are the first culprit, are the 1st place to start to look for optimizations, etcetera. So what are caches? First of all, cache. The term cache is a general term. So there are various types of caches. For instance, your browsers have caches because if you open the same web page and I wouldn't venture a guess what that page might be in in your browser, that it's a good idea that the browser keeps the code of the page cached inside inside the browser or on on your local hard drive. So it doesn't have to be loaded every time you open that particular web page. Same goes with the memory. So some parts of memory can be cached much closer to the CPU. So every time you need to access this particular memory, the CPU doesn't have to make the long request to the main memory, but it can access the data much faster. So basically cache is in some structure, or in this case a hardware that holds some data. And the important part is that, sorry, yeah, the cache is used only when it speeds the things up. So cache is somewhat a transparent mechanism which is only there to speed the things up. So assume that the loading of the data or sometimes it might be you need to compute the data, you need to compute the result and the cache is there just to memorise your results for your common answers. So every time you need to do something which is slow or expensive, then cache is there to help you with that to give you the answer more quickly. However, the cache can be removed anytime and the system should work the same. The performance may be hit severely, but the the semantics of the system or the application or the hardware in this case would remain the same. So caches are somewhat transparent to the application. The generic cache operation is that you make a request for some data, so there is some identification of the data. In case of memory, it's the address, in case of the browser example it would be the URL, right? So it's just sometimes unique identification of the data you need to request and you just request the data and the cache. If the data are placed in the cache then they are returned. Otherwise you you get nothing. So you need to make the long trip to the memory. You need to load the the website from the Internet. You need to compute the data. Whatever you need to do to get the data, the expensive the long way in the CPU. The caches are mainly there to hide memory latency. The latency is the delay between the memory requests and the answer when the the operand or the instruction is filled with the data from the memory. This whole concept is based on something which is called locality of reference. That's just a term that describes the assumption why we are placing. Well, not we are. The vendors are placing the caches on the CPU. The notion is that when your application runs, it very often access the same small area in the memory over and over again. Yeah, this assumption doesn't hold for every application. Obviously it can't. However, for many situation it does. Just assume how large is your program in the memory. The program isn't usually that large, at least not the part that's currently running. So the the currently running, but the hot part of your code usually fits several kilobytes or 10s of kilobytes. If you're doing some intensive computations, then your hot data, the data you are currently processing also usually fits a few kilobytes or 10s of kilobyte sometime, sometimes it's megabytes. And of course you get stupid algorithms that just wander around the memory in a random fashion. And yeah, you cannot do anything about those. However, most of the algorithms are tend to access the data which are close together in the memory. And if that is so, then caches will help us sincerely severely, because the the caches will localize will give you the localized memory you are accessing. The small portion of the memory you are accessing will bring it closer to the CPU so you can access it in a much faster fashion. So the CPU cache operation is basically embedded into the load operation of the OR the load instruction which is in the CPU. So when the every time when there is a request to the memory. And remember, if there is a load store architecture, the only access is made by load operation and possibly stores. Of course if you have a X86 architecture for instance, some of these memory operations are embedded in other instructions. Usually you are using move instruction to move data from the memory to the register or vice versa. But there are some other instructions that can accept reference to the memory as an operand. So all these instructions that access memory trigger this request of the data and then we then the cache just search itself whether the data are in the cache. By the way, in CPUs there are multiple levels. You already noticed that when I was talking about the CPU design. Usually we have like 3 levels in contemporary CPUs and the each level is like the L1. The first level is the closest one to the CPU which is smallest and fastest. L 3 is usually shared among all the cores, it is largest and slowest. So it's like there are three levels to make this more convenient. Because it's very difficult to create a cache which is both fast and large. That's why we have very small caches, very close and very fast, and larger caches slightly further, slightly slower, but much larger. So yeah, this request is repeated on all levels. So first you are going to the first level to L1 cache. If the data are there, you get the data, if not, you are going to the second level. You asking the second level of of the cache for the data. If they are there, you get them, if they're not, you are going to the last level, to the third level. If if the data are there, you get them and if not, you need to make the long trip to the main memory. OK, so this is basically how caches work. One way how to remember this is if you don't remember what caches are. Well, caches are something you already used during your high school, right? So why waste the long trip from your bed into the into your wardrobe if you need to fetch a new fetch? I wouldn't say a fresh set of clothes. I said less dirty set of clothes or something like that. So in this case, you don't want to waste the long trip to the wardrobe, right? So you just access the pile of the laundry just next to your bed, which every bachelor can access in constant time, right? Because we have some special abilities. Well, I'm married, so I'm not no longer bachelor. But when I was a bachelor, I, I could use this, this unique ability to just search my pile of, of clothes and access any clothes which it was there in constant time. So this is this is a cache, this is the memory, and this is the CPU in our sense. Few things specifically related to the caches in the CPU. So first of all, what? What are we actually caching? Because it would be very inconvenient to cache every single bit or every single byte. That wouldn't be easy because yeah, we can technically address every single byte of memory we can, but it would be very impractical to cache them individually because the overhead would be enormous. And you probably already remember that we are accessing the data not by bytes, but more usually by words like integers. So normal CPUs are incapable of accessing anything smaller than 32 bits. Normal CPUs like in your laptops not. Not like Arduino, right? So basically it will be very impractical to make this too fine grained. So what we define is that there are cache lines. Cache line is a small block of data and we are always caching the whole cache line. So we divide the memory into smaller box. For instance, on most architectures like X86 architecture, I believe all your laptops you have here has cache line of of the size 64 bytes. It's very likely. Yeah, some architectures have 32, some architectures might have 128. But roughly if you say something around 64, you're basically never wrong, right? And it's not not that important whether it's 32 or 64, it's the block about this size. So again like in the memory allocation, this is the block which is not too large. So it's somewhat fine grained, but not too small not to produce too large overhead. And also the size is selected so that the sketch line can be easily fetched from the physical memory. So usually the physical memory is operating somewhere close to this range. So fetching 64 bytes from the main memory would be like on the similar speed as fetching anything slower. So basically thanks to the parallel buses when you're serving data from main memory, you you probably remember that from last week. What was the size of the bus of one dim dim chip I I demonstrated on the slide. Does anybody remember? Can I give you a hint? There were three bits dedicated for the bus address for the byte address. So if we have 3 bits for the byte address, that's that's a byte. So from each chip you can get 8 bytes. If you have double channel, you can get 16 bytes. If you have more channels, you can get more bytes. Plus, thanks to some specialized circuitry as as you should remember, if you just if you just accessing the data which are near to near together in the column order not in on the different rows, then you can get them much faster. So reading consecutive data from the memory is much faster than just wandering around the memory. So if you want to write read consecutive 64 bytes from the memory, it's it's very fast. OK, so This is why they choose a size which is somewhat close to what's good for the main memory buses. And of course it's aligned obviously because you want to load the data, you want to address the data in the fashion that you can just drop the lowest bits of the address and the upper bits keep the address of the whole cache line. By the way, how many bits can we drop from the address if we are, if we want to address 64 byte blocks, how many bits on the in the address would be 0 in the end of the address if we have aligned address to the 64 bytes, that's right, 64 two to the power of 6 is 64 exactly. So by the way, so if we are addressing cache lines, we can forget 6 bits because we know they're they're zero. That's a trick that can be used in in in caching. Cache hit is a terminology when you searching the cache and the data are there. So you've got to hit the data are there. You can't get the data fast. Cache miss is the opposite. If you if the data are not there, you you miss the opportunity for speed up. So that's cache miss. And in that in that case you need to search the higher level of cache hierarchy or you need to access the main, main memory. By the way, the cache miss is somewhat more complicated because every time cache miss occurs, we would like to make sure it it doesn't hit miss again. Because usually when you access some data, it's very likely you are going to access the same data or the next byte. So it's a good idea to load the whole cache line anyway very soon. So basically every time a cache miss occurs, what you want to do is to populate the cache to save the data in the cache. So next request for the same address or at least next request for the same cache line wouldn't miss, right? However, for that there is one one cache. We need to select a victim. Either there is a free cache line so we can just put it somewhere, but usually sorry sorry sorry, it's probably on the next line. But usually we have a problem because the cache is full, so we need to select a victim. We need to select someone who will be removed from the cache and the data will be loaded instead of the of the removed record. By the way, this is even more tricky because you can access memory from multiple CPU cores. That's a completely normal thing to do nowadays because you have multicore CPUs. All of you here sitting here probably have multicore CPUs. It it would be very unlikely that someone here has a single core CPU. So there is a another protocol that needs to be put in place to make sure that all the CPUs are accessing the caches in some coherent manner so that the CPUs cannot break anything. For instance, you cannot make 2 CPUs right into the same cache line at the very same moment. That's something you need to prevent because that would be disastrous. On the other hand, when multiple CPU cores are reading the same cache line, there is no problem there because reading is fine. Yeah, you can, you can share the cache line and and you can read. For that there is a specialized message protocol. I'm not going to into the details because it's it's quite technical, but just be assured that there is some specialized hardware protocol that makes sure that the caches distributed among multiple CPU cores will stay coherent. Basically the CPUs can communicate over the over the memory buses and or they are snooping on the buses and they can alter the state of their internal cache lines if another cache, if another CPU core cache is gets populated by a particular cache line, for instance. So there there is a protocol for that. OK, by the way, yeah, I forget to read this one line when when we are selecting the victim for the for the new cache line, don't remember that we cannot just sacrifice the old cache line and just drop it. Sometimes the stored cache line can also contain modified data. So the caches are not only for reads, they are also for writes. If you're writing data, if you're writing some variable, some, some data in the memory which are already in the cache, the modifications take place place in the cache and sometime later they will be propagated to the main memory. So there there is this small detail over here which cannot be overlooked. Just remember that the rights needs to be also handled properly properly in the caches. By the way for normal application. This is some some general statistics that was made by some scientists. If you just take overview of contemporary pieces of software that are usually running your CPUs, they usually have a very high success rate, very high hit rate for the caches. So this tells us that the caches are doing their job properly. It's a good idea to have these caches. This code, all this code that that succeeds on the cache hits would run much, much slower if the caches wouldn't be, wouldn't be there. OK, this is just the the to remind you about the levels of caches. So first of all, the first level of cache, which is smallest and closest. By the way, this level is divided into two parts. I didn't emphasize that when I was talking about CPU. So level 1 cache is actually divided into instruction cache and data cache. The instruction cache is used only for loading instructions, for loading the executable code and the data cache is for all other memory operations. L2 cache is shared for both memory for all the memory, so for both for the code and for your data. And L3 cache is also shared not only for instructions and data, but but also it's usually shared among the cores. Usually the L3 is chopped into some directories into some fragments and each core somehow manages its own fragment. But other cores can access a fragment of an neighboring or or any other core. So it's much more difficult than that. But you don't need to know these details, it's just important thing you know that usually there are like 3 levels of caches. By the way, L1 has roughly today something like 32 kilobytes, so it's not that large. On the other hand, 32 kilobytes of instruction case should be enough to keep all your hot code. You usually don't use more than 32 kilobytes of your contemporary running code for the data. It's much smaller because 32 kilobytes is not that much. But again, then again, it's very fast. It's like it can be accessed with within units of cycles, so it's not as fast as registers, but only a few times slower than registers. So it's very fast. L2 cache is somewhat slower, like again several times to L1 cache, so like 10 times slower perhaps than the registers roughly. I'm not giving you the exact numbers, but again, then again, L2 is much, much larger. On current servers we have like 256 kilobytes of L2, but contemporary CPUs can easily have megabytes of L2 cache and L3 cache again is much, much larger. It has today I think in the the maximum is hundreds of MB. I'm not sure if maybe even more. Sorry, what? This is just a general, this is general depictation of execution unit. So this is just a very small block which which will contain the arithmetic logical units, the the float floating point units, everything that computes something. So all the executive stuff is here. Yeah, it's not, this is more important from the perspective of cashiers. This is just actually, this is much larger that this much more detailed. You, you remember the the diagram with the pipes. So there are many types of units and they are all in here like just concatenated though. No, no, no, it's not there. Well, it should be in blue if it's UU plus it has to have some stars around. So no, no, this is not a European unit. OK, so how the caches are implemented? This is quite important because you need to understand the reason why the caches are so small when they need to be fast. Well the reason is they using something special called associative memory. You know the regular memory is address by address, right? So by an index by an offset. So you have a number that tells you which memory block which which byte or which sequence of bytes depends on how wide is the word you're accessing the memory. So the address will tell you where to look, which row to activate, which column to activate, where you read the data. So it's very efficient because you don't have to activate the whole memory, you activate only very small portion of the memory and the portion is deduced from the address. So This is why the memory is organized this way. The associated memory isn't address at all. You have a key value store. So basically it's something like a dictionary in Python, just not. Implemented the same way just to give you an idea notion. So basically you have something which is called key, an identifier and the value the data, in this case a cache line or actually this will hold for the browser example as well. The key would be the URL and the value would be a HTML document for instance. OK, so this is the same, same idea. However, in memory, the key is actually the address. So yeah, it's it's tricky because it's like a copy of the address. Actually, it's not the whole address. We don't have to copy the whole address because I already told you that exactly the last six bits are always 0 because it's address of a cache line. So we don't have to put all 32 or or 64 bits. We can, we can put some bits away and also we are usually cropping some bits from the top because we are not making the cache fully associated, but that's another story. But basically you store the address as the key and the value here is the cache line plus some additional information. For instance, you need to keep the state of the cache line for the massive protocol, for the coherence protocol. So some additional bits needs to be placed there for for management reasons. OK, the trick is that to search, sorry to search this memory, it's very expensive because here I know the address. So I'm if I say OK, my data are on the address, let's say 4096. So I know I need to activate only this part of the memory so I can access I can power only this memory cell. Technically I'm I'm lying to a bit because DRM doesn't work that way. But basically I'm I don't have to power the whole memory the full I need to access only one memory cell here. I need to activate them all at once and broadcast to them the address and ask them all at once. Hey guys, does anyone here has the number 4096? Hey, does does anybody have it's the same here? Because if I give you everyone one piece of my memory, I can just broadcast the request. Hey, does anybody has this piece of memory? And only the person who has that will answer me. Yeah, I got it. And if nobody answers, I know it's not there, but I need to activate you all at once here. I need to activate only oneself. That's why this memory is so expensive, because it's more much more power expensive to perform such such a request. That's why we need to keep the caches much smaller. And there are other tricks, for instance this. This is something which is called fully associated cache, which means any any address can be placed on any of these positions. That's that's the best cache we can have and also the most expensive one in the terms of implementation. So they usually using some which something we just call N way associative cache, which means the address cannot be on any arbitrary place in the cache. But we already know that the cache is like divided into smaller groups and we know that some addresses using some hash functions, some some straightforward hash function, some addresses can be placed only to some small portion of the cache. So we don't have to search the, the whole cache, we are searching only a small part of it. So there are some tricks, hardware tricks, but they are not important from this perspective. What's important is we have some clever circuitry that does that for us. And it's, it's addressed by keys, which are basically cropped addresses and the values, remember this is the cache line, OK, this is how caches basically work. And finally, final part, final piece of the puzzle to the memory is the NUMA systems. I already told you that the multiprocessors are somewhat tricky because of the caches. So we need to have a protocol that that keeps the caches coherent among amongst multiple cores. But that that's not it. We have more complex systems on on our hands. If you're in your, if you have a single CPU core in your laptop, that's pretty straightforward because this is usually it has some single internal bus. Remember the, the circular bus inside the, the Coffee Lake example I, I showed you last week or the week before. So there is someone system internal bus and it's attached to the all CPU cores and it's also attached to the system agent which holds the memory controllers. So basically all the CPU cores are accessing the RAM via single bus. Yeah, there, there might be some redundancy, but that's not important right now. What's problematic is that if you have too many of these cores, then this bus gets overloaded. That's why they come up, come up came up with tricks like a circular buses or in even more more modern CPUs which have much more CPU cores, they are using a grid bus. So basically the cores are organized in a grid and there are row buses and column buses is much more complicated, but technically the bus can get overwhelmed with the requests to the memory. So it doesn't scale well, scale well. What can we do? One one option what we can do is to divide our system into multiple CPU, not cores, but the dies, the sockets, which means we have multiple CPU units, multiple CPU chips technically, which are communicating over some external buses, right. And the benefit is that each of these CPUs, these might these were denoting in my previous speech CPU cores. These are really CPU chips, so technically inside might be multiple CPU cores, but that's not important right now. What's important is that they communicating via external buses and each CPU, each CPU die, each CPU socket has its own memory. The benefit is that when the CPU is accessing its own memory, it's much faster because it it doesn't get clocked too much with the other CPUs. However, it would be very impractical for the programmers if every CPU, and I mean CPU die would be would could access only its own RAM. It would be very difficult to design A software that would run on such a system. So these NUMA systems actually make the whole RAM visible to all the CPU cores. So from the perspective of your operating system, from the perspective of your application, you have 4 pieces of RAM which are usually concatenated one after another, or they may be interleaf or that that's not that important that they are somehow combined together. So if you have like 4 times the the size of RAM or the combined size of the RAM of all the CPUs and each CPU core can access each other's RAM, which means there has to be additional protocol that allows the CPUs to communicate one with another requesting memory. Technically, it's just an extension of the massive protocol. Basically, they're using the same principle. So if this CPU needs to access this RAM, it sends a request for the RAM and this CPU and the other CPUs, remember that this cache line, this piece of RAM is stored, is temporarily cached on this CPU. So technically it's just an extension of the same protocol, but basically it's much more slower because in this case, in this previous example I was giving you, the system buses were internal. They are much shorter, much smaller, but even when they are smaller, they can communicate with a short latency. By the way, as a as a voluntary home example, a home assignment, you can just compute how fast the the light travels. If you have two circuitries, 22 chips 30 centimeters apart, for instance. So just imagine that you have two CPUs or CPU and A and a associated device or something 30 centimeters apart. And imagine that they are communicating using some optics or something which is slightly faster than than electric wires. So they can communicate at the speed of light. So compute the latency. So I, I believe everybody can compute it within a minute. So I, I'm not giving you bother to, I won't bother you too much with that anymore. It's just to give you notion, sorry. If the bus, if the bus is internal, it might be slightly faster than if these buses are external. Nevertheless, these systems are normally used widely deployed and usually we are using in our servers NUMA servers which has two or four CPUs. That's pretty normal. OK, this is pretty much it. This will conclude the talk about the memory. Are there any questions regarding memory, memory organization, memory access, caches, whatever. OK, let's move on to let's move on with another topic. And the follow up topic is programming languages one second. So programming languages, yeah, obviously this is a broad topic, so maybe this should be called something like an introduction to compilers and runtimes. So we are not going to talk about various programming languages. We are going to talk about the support various programming languages has from the compilation part, from the runtimes and from other stuff. So let's start with the compiler or the compilation process. Basically, you already know, or should know at least this interpretation of a compiler. So basically compiler is responsible for turning your software, your written pieces of code into something that's that can be absorbed by the CPU. So into the machine code, into the instructions encoded in binary so they can be intermediately interpreted by the CPU. Yeah, here the compiler is depicted as a black box because many people perceive it that way. And technically, if you're just a user, it's a correct, correct view because you basically take your source code, put it into GCCMVC, whatever compiler you prefer. And then you either get the executable, which you can immediately deploy on your operating system. So it's given to the CPU to execute. Or sometimes, of course, if you make a mistake in your source code, if ever, if you typed it incorrectly, if cannot, it cannot be translated into the instructions. You will get a set of error messages. So you can, based on them, you can refine or fix your code and then recompile it. That's it. By the way, in the first semester, you weren't doing this, right? Because you just have the Python script and you just run Python, your Python script, and then that was it. So there was no compiler, right? Wrong, actually. The compiler was there, you just haven't seen it first hand. The compiler was embedded in the Python runtime. Every time you run your script, there was compiler present who actually chew up on your code, spill out the instructions, and the instructions were immediately executed so the runtime did the compilation for you without actually telling you. So it's like a one lie to the baby switch starting with a with a simple languages. No offense man, it's just like a the C guys are making fun of Python guys, that's all. That's just this funny stuff. So basically it's just something that's hidden from the regular user so the regular user doesn't have to be bothered with with intermediate steps like compilations. Now, since you've grown up, you're given C or C and in C or C, and actually the same goes for for Java or C#, because normally most modern applications, modern languages use this compilation step which takes place a priori before the application is executed. By the way, the benefit of having the compilation, the compiler producing and by the way, in in Java or in, in C#, it doesn't have to produce immediately the output of the of the binary of the of the instructions for the CPU. It can produce some intermediate result which is then later turned to the actual binary code, but that's not that important. OK, So what what I'm saying is, if the compiler needs to be used at the design time, at the coding time, at the development time, it's always better because you can get some feedback. Some mistakes you made can be intercepted by the compiler and given back to you so you can fix your code. If your code is distributed to the end user and then it fails because you have a typo in your code, that's obviously not a good approach. This, this is somewhat embarrassing for the program. So how this works, don't get scared. Yeah, this don't panic. Maybe should be May be on the top of this slide, so don't get too excited about this. Just to give you some glimpses of what compiler is formally. Basically just assume that we have something which is called a language, an input language. Technically, this is like a language, your programming language you are currently using, and each language has something which is called a grammar. Grammar is like a formal description of that language. For instance, in Python it tells you what are the keywords that for or if is a keyword. So it has to be used in some sense, in some manner in some context, right? In C++, the grammar tells you that again the keywords and and that the block is wrapped into curly braces and stuff like that. And then you have output language, which is basically the instruction set of your CPU, which also can have a formal grammar which which is basically derived from ISA. If you have AISA which specifies what instruction set your CPU can use, then you have a grammar for your for your CPU. Basically, you just adjust minor details, but let's not dwell on the details at the moment. And we can say that this language is accepted by some automaton. The automaton in this context is just a formal specification of the CPU. Yeah, I'm not going to explain in detail what automaton can be. If you heard about Turing machines or stuff like that, there is a it's one of type of the automatons, which is one way how to abstract the CPUs. If you want to be a theoretician, again, this is not that important right now. What's important, what's the more important part is that when we formalize it like that, then compiler is basically just a mapping. Compiler discovers and creates a mapping between your input language into the output language. By the way, the language, the programming language isn't the language. The programming language is the grammar. The language in this context is your code written in that language. So the language is the string or the strings that represent your code. That's what's in this context called a language. Don't ask me, I'm not, I'm not this. I'm not expert in this area. I didn't know why they use this slightly confusing terminology. So they create a mapping between your code in your selected programming language into the output grammar. So it generates a binary code which should be accepted by the CPU. By the way, sorry yo. If there is for all words in the in the language in the input language exist an output word in the output language, then the the mapping exists. If there it doesn't, the mapping doesn't exist and the compiler is responsible for telling you which words in the input language were and there. So that's just a formalization of this process. Sorry about that. I'm always getting confused. I'm not the expert on this. So again, don't worry about it. If you want to get formal, there are plenty of opportunities. First of all, you will get the mandatory automata and grammars course and it's actually essential to get the notion about all these terminologies and all these details. So again, this this will be one of the courses you will pick up in the second, second year. So you will get the hang of it once you once you get the basics. And then there is a course called Compiler principles, if anybody is, it's elective actually. So you don't have, you can skip it if you'd like, but it's a good, good course because it will give you a good idea about how the compiler really works. And you will, you will create your own compiler for simplified C language. So you really write your own compiler. It's a cool thing. So basically this is it. Just to give you another glimpse what the grammar is. The grammar is specified using some rules and one of these rules can look something like this, an iteration statement. This is a like a variable, like it's called non terminal. It's like a placeholder or variable or something. So it's not actually actual letters or something. It's just a placeholder and these bold things are actual letters or actual tokens from the language. So basically iteration statement in C is either while do or for and this describes how these statements will look like. And the the thing in italics which is not bold is again non terminal. So it's again a variable. So basically while it's always when you have a parenthesis and in parenthesis there is something which is expression. And again, expression has its own definition and it's followed by a statement. A statement may be a expression ended by; Or it can be a block of statements in curly braces. Yeah, and and some some other things. So this is like a prescription of what can be written in the language. And this grammar can be used to derive, to parse your language and verify whether it's correctly written. And then this grammar is used to translate it into the, into the second language, into the output language, into the binary. So this is how it works on the theoretical level. Let's go on to the practical level, which is much more fun. On the practical level, compiler is actually composed of multiple components or parts. These are the most important ones actually. Again, this is just a simplified depictation. It's just a simplified schema. There are many internal. For instance, a compiler or assembler are quite complex parts and there are many internal steps inside. So I'm not going into all the details here, but basically these parts are important because if you'd like, you can force your GCC or whatever compiler you are using to give you the intermediate products after each of these parts. So if you like you can you can take a look how your code would look like if you just run the preprocessor and nothing else. You can you can just run the preprocessor and compiler and you will get get your code assembly language so you can examine the instructions there. Or you can run it through through here and here you will get something which is called an object file. And this is almost the binary, almost the binary which can be executed if it holds some parts of the binary, but it also holds other information like what memory should be allocated where, how the functions are named. So usually you have multiple object files because some are produced from your source code, some are produced by libraries and you need to put them all together to produce the ultimate final executable code. OK, let's start with this part. The preprocessor basically performs a text level tasks only. So things like includes I will use C or C++ as a reference. So if we are using C++, includes or macros are resolved at this particular level. So basically after that you get a huge C++ file because all the includes and everything are put into one file. So you get a huge C++ file which is still AC plus plus file. It's somewhat awkward if you want to read it, but you would recognize most most of the code because most of the code isn't malformed yet. After the compiler. This is the most difficult step from the user perspective because it really transforms your code into something else. But then again, it's it's pretty straightforward. If you if you take the compilers course, you will learn this part how to transform NAC code into into the some intermediate assembly language or intermediate instructions. The assembly language is very close to the machine code, so as it's represents your application, it represents your program almost at the instruction level. But some pieces are still missing. First of all, some optimizations are not performed. Some optimizations are performed afterwards. And 2nd, this also holds some additional stuff. For instance, you still don't have exact addresses. If you have some jump, conditional jump for instance, you don't have the address filled in yet, you have still the label. So you have some mnemonics inside the code. Still, this is resolved after the assembler, because over here all this stuff is finally resolved and here you really get the binary or something very close to binary, some intermediate binary. For instance, in C++, most of the object tile will be really the binary. By the way, these three things usually are bundled together in one executable. So if you write like something like GCC and you give it your source code, all these steps will be automatically invoked and automatically performed. You don't have to do it manually. So it's like usually all in one bundle. The linker is sometimes also invoked immediately by the GCC. But if you perform one CPP file, if you perform pair CPP file compilation, so each CPP is compiled individually, then each CPP will produce the one object file by the compiler. And at the end you need to call the GCC one more time using all the object files together and then to link them. Why is it so? The answer is to simplify recompilation, because if you're writing your code and you need to recompile something, then if you if you cache your object files, you don't have to recompile all of them. Sometimes you do, because if you if you change something which is inadvertently included in audio files, then audio files will get recompiled. Yeah, what can you do? But if you just modify some, some small piece in some CPP file, then only that CPP file, so only that object file needs to be recompiled. So this steps doesn't have to be called for all the other object files. So just to speed things up, the linker can be called separately, especially if you're producing multiple object files from your source code. OK, by the way, libraries, and we will get to that in more detail, have two parts. Usually in C++, 1 part is the interface specification, so things like function names or classes or or stuff like that. So the compiler will immediately know what functions it should expect in the long run. For instance, if there is a some library which promises you, you can have a function that prints something out to the console, then the interface here of that library should told should tell the compiler that there is such a function. Because otherwise compiler would say, hey, I'm trying to find this this print function somewhere print F For instance, you already know print F from your home assignments, right? So I, I I'm reading this print F, but I really don't know whether this print F is correct because I don't know how it how this should look like. Whether it takes one argument or two arguments, should it be integer or a string or something else. So the compiler has to know so it can match the function calls with the appropriate arguments to the what what it thinks about the function declaration. And in the end the linker needs to be provided library objects where the actual implementation is. So the implementation of the print function doesn't have to be here. Here is only the interface specification, how the function will look like what what arguments should it accept. But here the the function should be implemented. So in the end, the linker needs to find all the functions that were previously declared in the interfaces or used from the interfaces of these libraries. To give you more concrete example, they'll just say that this is a compilation of this this CPP code which includes this display H header over here. So basically, yeah, basically this display H will be included here where include is like a copy paste it over here. And then these macros needs to be resolved. So let me give you a glimpse what will happen if we go step by step through the compilation process. So first part, if you want to try it on your own, this is an example from from MSVC. So from the Microsoft compiler, if you're using AGCC or something else, you need to find out the appropriate switches for that. But they are there. I just don't know. I just don't know them from top of my head, but you can find them quite easily. So if we just run the preprocessor here, now let's see what the result would be. OK, just zoom in so everybody has a good view. I think this should should be enough. So yeah, this is the same thing you you've seen on the slide. So there is the main code which includes the display and the disk, sorry. And the display which basically defines one function. You can see it only defines the function, it doesn't implement it. It's just the sorry, declares the function. It doesn't define the body of the function, sorry. So it just tells you that the function will be named show it takes one string, one constant character pointer, and it returns void, so it doesn't return anything. That's it. And what does the preprocessor do is it combines these things together. So basically it copy pasted this show over here. The end if and the if death these parts have disappeared. So do you know what these these things do? And maybe we didn't explain this to into detail. So does anybody know what these lines do? OK so my fault my fault. Let me explain. So the first line says if not defined and this is the name of the macro. So this if this back row is not defined then we are going in. So this is what we want. Then we defined the macro and this and if goes to this if not def. So if this macro is defined, then everything in between will be skipped, will be discarded by the preprocessor. Why would we do that in in header? Well, actually we have to do that to prevent accidental multiple include. Let me give you an example. Let's just say you have a header AH and BH and BH holds include AH, right? So basically B includes A and then we have a main CPP which includes both. So technically it will include A. So the preprocessor will copy everything in A into the into the main, then it will include B and in B it will include A again. So it will copy the A two times in in the CPP. Normally if you do, if you do nothing, this will actually happen if you don't place any additional code there. So what we need to do, we need to prevent that. And this actual code prevents that because if the A is included the first time, this macro is not defined. So it's defined when it. When it goes into this code for the first time, it defines this macro and all this code is placed inside. It's not discarded, it's used. So when A is included in main, it will be properly included. And second time when B is being included and transitively A is is trying the, the preprocessor tries to include A. What happens is that A over here already has this macro defined, so it skips this whole block. And presumingly this whole block covers the whole file, so the A isn't in the a isn't included twice. If it were included twice and it holds something like this, it would be bad because the compiler would say, hey, there are two declarations of the same function. Show that's that's an error. You cannot have two declarations of the same function that that's a bad so that that's a mistake. So it would actually told you told you we cannot compile this. It's wrong. But even though it's correct because it would be the same function, by the way, there is a shorthand for that. It's called pragma ones. Just if you, if you want to use it, just look it up. It does the same thing. It's just shorthand for this basically. And another thing I would like to point out is what's over here. Why do you think there are some such awkward things that appear over here? What's this stuff? Would anyone venture a guess? Why would compiler, sorry preprocessor put this stuff over here? What purpose does it serve? OK, error reporting every time you need In the subsequent steps when an error occurs, you need to properly report this error to the user. But the proper reporting is to tell the user where the error actually is in the source code. And if you just put all the source code on one pile, you need somehow distinguish which part of the of the code. So this tells you, hey this is where this is where line 1 of display H actually starts in the whole bundled CPP code. OK, so this part was copied since line one from display H. And here we ended on line 8 of display H and we starting since line 2 of hello CPP. So every time there is an error somewhere in here, thanks to these markers, the compiler will know how to properly point out where the error was in the original code. Is this clear? So this is this is like a hex you need to do if you need to keep track of where your code comes from. So this is preprocessor. This is the simplest part. Second part is slightly more tricky because we are now transferring from CPP, which is pretty understandable by humans, into assembly language, which is far, far worse. So let's try it. OK, the same example, the same piece of code, and this will be the the result. Yeah, this is much more incomprehensible, but let me just point out few things that we should recognize. First of all, there is a some string. Yeah, this string will be here. The DB is like a sequence of bytes. This is a sequence of bytes ended by zero. AH, with 0AH I I will reveal this syntax to you, the H at the end. This is a alt assembly syntax in in CPP. In C and CPP, we are using 0X for hex numbers. In assembly, they're using the hex number followed by large HI. Don't know why, but this is just a convention. So this is a hex number. What is 0? A hex In hex 10, which is the #10 Yeah, but but the hello world should give you the hint. Right now it's obvious, so this Has to go somewhere. So this zero a is a character hexa representation of the character What else is important or interesting here? Let me scroll a slightly more to the bottom. Is this part. Yeah, this is the most important part, the most intriguing part. And you probably recognize this one because you should at least slightly recognize some of these some of these lines. This this is obvious. What's this set EAX? Yeah, that's XOR, which sets the EAX to to zero. Yeah, that's the shorthand for writing zero into EAX. But you are right, that's XOR operation. And similarly, this is call to the show function. This is move of RCX which save the pointer of course to the message. And the message is the segment which was previously marked over here. Yeah, this is the message segment which holds this data. So basically this is how the pointer is acquired and then how the how the function is called. But by the way, why do you think the EAX is zeroed? Yeah, I know this is tricky question. OK, So what we quick refresher, Quick refresher we are just performing show message. So we are just calling a function show giving it the pointer to our message and then we are doing return 0. That's right. So we are returning the value into the EAX register. That's what you can deduce from this from this piece of code. The EAX should hold the return value of a function in this case. Yeah, this is just some stuff that's required to do to be done. This is just altering the the stack pointer so we get to that in a short order. How stack pointer works and return 0 means a return to the address 0 and the 0 here doesn't actually hold the actual address, it's just some intermediate value that will be filled in later. OK, so this is how the assembler works. Yeah, the the full compilation you already know that will produce some executable file like MZ exec on on Windows or L file on Windows. So I'm not going to show you that. And the object file we we get to that in a short order. Before we do, let me explain a few more terms. Now we need to think about how all the stuff is linked together. So in the end the compilation process will produce object files. OK, so we get the object files and now we need to put the object files together and we need to put the libraries into the object files. Library is basically a piece of code which was pre compiled by someone else in this context which was pre compiled by someone else and given to you either as a binary or as a part of the operating systems. Most of the libraries are in the operating systems or some libraries are given to you in the runtime of your of your application. So there are many ways how to get get the library and there are two ways how to link the library, static linking and dynamic linking. I will explain in detail on a next door few slides from now. So static linking use this type of libraries. This is notation for Windows and this is notation for Linux. OK, I will use Windows notations for various reasons. So on Windows you will get a Lib file and you can get the Lib file to statically link it into your application. For dynamic linking you use DLLS in Windows or SOS shareable objects on. On Linux the linking process is the bundling process where the object files are put together with the libraries and everything is probably rearranged and re associated so it can be placed into one executable file. There are several steps that that take that takes place there. And then the important part is it needs to handle something which is called relocation because you need to place your code somewhere in in your address space. Sometimes your code can be generated in a position independent way. So it doesn't matter on which address the code runs. Let me give you an example. You have something like a for loop for instance. I just write it schematically. So basically somewhere at the beginning or at the end or maybe both, there should be some jump. For instance, there is some conditional jump. I just symbolically write it as jump conditional, which basic based on this result will take a jump at the end of the loop. Usually it's the other way around. At the end of the loop we take a jump back, but it's not that important. So there is some jump and the jump needs to know the address where to jump, right? So if you have only absolute jumps, so the address has to be absolute, then it's tricky because this address needs to be adjusted when the program is loaded into the memory, because before that you don't know where the program will be loaded if the jump is relative. So you just write something like jump C + 16, which tells the compiler, which tells the CPU jump 16 bytes ahead. It's just like a add function for the program counter for the instruction pointer. OK, so if it's written in this way, then we're fine because you can place this for loop anywhere in your memory and it will work the same. Because if we get to this jump and we need to jump, it will tell the compiler, hey, just jump 16 bytes ahead and everything will be fine. And yeah, of course you need to have the proper management that these instructions in the loop plus and everything else over here will take exactly 16 bytes. So the jump will jump after the the last instruction here. So you need to the compiler actually needs to do appropriate calculation here. But in the end, this code is ideal for us because it can be placed anywhere in the memory and it will work the same. So that's what we call the position independent code. Other code needs to be relocated, so places where an absolute address will be. So if there is an instruction that jumps to an absolute address or an instruction that loads or stores data from an absolute address, it needs to be recalculated during the linking and then again during the program deployment 2 times. That's why there is the third part, the third player in this game, and that's called Loader. You cannot just put your application in the memory and start executing it from the top to the bottom. It's much more to it than this. So Loader is a part of the operating system which understands the format of the executables in your operating system. By the way, an egg note, last week I had an incident. I wouldn't call it an incident. I have a story with one of the students from the second or third year and the student just compiled an executable exa file on on his Windows machine on his laptop. And then he uploaded this exa file to the our Linux server and he was very surprised that it wasn't running there. Why is that so? Well, it's obvious because Windows is an operating system and Linux as an operating system are using a different formats of executable files, so they have different loaders. This part of the operating system is different, slightly differently implemented in both. They're doing basically the same thing, but they recognize different executable formats and they perform slightly different functions. The purpose is the same, but they're implemented in slightly different ways. So you cannot compile Mac application and just deploy it on Linux without any additional support it it wouldn't work because you need to deploy it on the operating system for which it was designed because the loader is expecting some some format of the executable. By the way, as I just told you few moments ago, Windows are using MZ executables while there is another word for it, I just forget the proper name. But Linux is using L files for that. So there's a different file format for binaries. And the job of the loader, regardless of the operating system is to yeah, load the executable to the memory. That's the, that's the easy part. And then perform first dynamic linking. So it needs to dynamically attached the libraries which are provided by the operating system and which are this is, This is why it's called dynamic linking. So which are linked at when the application is executed at the runtime and then it needs to perform relocations again. So this is like a first stage when the relocations are just like a pre computed when the stuff is integrated together and then the relocations need to be recalculated again based on where in the memory the application was actually placed. OK, so this is a tricky part. I will I will show you this in more detail on an example. But first, let's start with the with the linking process. So what I mean by linking is that let me let me explain this example. We have a library called. My library doesn't matter and it provides this this stuff, one variable, one global variable which is accessible from everywhere, and two functions. OK, so this stuff is in my library H and you can see these are just declarations. So we are just telling you we are telling ahead the compiler what type of functions the compiler can expect what what is available on the palette. And this is placed in the H file so it can be included in the any application we want to we want to use which ones to use this library. And then this application can actually call this functions open or it can call this function read. The compiler is fine with that because it was told a priori ahead that these functions will exist, should exist. And what are the the arguments? This one takes a pointer to the constant character. This one takes an integer and integer and a buffer. OK, so we the compiler knows that when I'm calling these functions like this, it's fine because yeah, I'm providing a string which is constant pointer to a constant character. This function accepts constant character pointer. Yeah, that's fine. It's matching. This function accepts integer and integer and a buffer. And I'm I'm giving that an integer and an integer and a buffer. Yeah, that's fine, no problem there. Actually there is slight problem because this is a pointer to character. Actually it's a, it's a character array. But it's it's the same as a pointer to character. This is a void pointer, but anything, any pointer can be casted to void pointer without a warning. It's fine to cast anything to void pointer. So this is fine for the compiler. Compiler just performs the cast and and is very happy with with this result. OK, but in the end we need to make sure that the function actually exists. The the the appropriate function will be actually called and that's the job for the linker and the loader which are responsible for finding out the right implementation which should be in my library C and put these things together. So making sure that this and this function will be matched somehow via addresses in the memory. By the way, the linker and the loader work on the strings, so the name of the function is the important part for matching. So in old C this string open and this string open is the thing that will be matched by the by the compiler. The name must match. So the sorry the linker will perform this this matching on the names of the functions. OK, knowing this, there are several ways how to perform the the linking. The easiest part, most straightforward part is if you have the source code, because if you have the source code of this library, then you can just compile everything together. That's the easiest part. You just take the library and take your application and the compiler will first compile the library into object file. Then your application will be or Application C will be compiled into object file and then the linker will put it all together. So here is where the preprocessor needs to know needs to put in the header files. So the compiler will know what functions to expect, how to match the parameters and here the linker will find out call to the open function here and that there is an open function exported from here and it will perform the matching. So it will adjust the address where this core is performed to this function, how it lies and it places the the the right function into the memory somewhere. This is the easiest part because if you have the source code, you you can do whatever you want. This is this is also what happens if you just separate your application into multiple C files or CPB files. And that's by the way, a good idea to do if you're writing a slightly larger application, if it has several 1000 lines of code, then yeah, it's a really good idea to separate it into multiple files and the same will go. You can, you can name your files part partial library or something. So you will know this is like a slight library file. This is the main, main, main file that where the main function is etcetera. Another option is that the the provider of the library doesn't want to give you the source code. Yeah, nowadays everything is given as a source code, right? Because we have GitHub. Well, this might be true for JavaScript or Python, but it's still not very true for C or C++ and usual not not true for for C# or Java. By the way, there are still many software companies which believe that keeping some intellectual property as the source code would be a good idea to not to give it to everybody, because then everybody can just use it. Yeah, today maybe we are slightly shifting from the source codes to memory model to large models and AI models and etcetera. So today we aren't protecting the code anymore, we are protecting the data, but it's basically the same thing. So if somebody wants to protect their source code because there is some hidden intellectual property and it would be bad if someone can just read our ingenious algorithms, then we can hide the CPP file and provide only. Yeah, we still need to provide H file. Everything which is in the header still needs to be provided because otherwise no one could compile against our library. The H files needs to be distributed always, but the CC file can be converted into the object file and only the object file. Actually not the object file but the derivate of the object file which is the library file for static linking can be provided. So instead of instead of giving you the source code, the the author of this library will is giving you the library derivate that the compiled library if you will. And in this case as a Lib file. So the linker will embed the Lib file. This is just just more fancy elaborate way how to combine multiple object files into one file. So basically this look at this as a collection of object files. That's one way how to look at it. So basically you are linking together object files and collections of object files together via linker. So basically it's the same thing, we just skip the compilation of the object file. This is performed in hidden in private by the author of the library and this is performed by you when you're compiling your code. That second option, trouble with that is some of the some of the libraries are used in every other application. For instance, if you're on Windows and you are using some libraries for operating the GUI of the windows, so creating forms and creating buttons on these forms and stuff like this, then you need some basic library which is always present in the Windows system. And it would be very impractical if this library has to be embedded, copied over and over and over again into every executable that it wants to use it. That would be very impractical, right? So what this operating system does is that it just creates a shared library, a DLL or multiple DLLS for that matter, which are actually distributed alongside with the operating system. Or sometimes they are not there implicitly, you have to install them, but they are shared among all the applications that use this particular environment. So basically the linker. By the way, the linker needs to know the DLL a priori, so the DLL needs to be present during the compilation time. But the difference is that the DLL is not embedded into the executable code. The DLL is used only to verify that all the functions that should be there are there. So the linker is satisfied that the DLL will provide all the functions that are promised via via the interface. And then it somehow puts in some stubs which are later fixed by the loader. So it just puts there some some markers that say, hey, loader, when you're loading this application, we really need this DLL and this DLL needs to supervise these functions. So make sure they are properly relocated once the executable code is loaded into the memory. So there is external stuff that seems to be performed after the application is loaded to the memory by the runner, by the loader. But the the price, the the benefit of this approach is that this DLR is not embedded into the executable. It's in the memory only once, it's on the system only once and it's loaded only when needed, on demand, if you will, by this, by this executable code. By the way, second benefit is, and that's just a tricky stuff. We get to that when we talk about operating system. If we have a multiple applications that are using this DLL code simultaneously, then actually some memory can be saved because the DLL can be present in the memory only once. But that's another story, just another way how to save some additional stuff like like the memory. So these are three ways how to compile applications through the source code, which is the simplest and then static and dynamic linking. OK, we have 15 minutes left so we can still continue. The most difficult part when combining object files and when actually placing the the executable into the memory is how this executable or actually its memory space should be organized. So let's start from beginning. This is a schema. You probably, you probably already seen this how data are organized within application memory space. Right now. Let's not dwell on details about that. Multiple applications needs their own multiple virtual spaces. So let's not let's not go into the details of memory translation. So let's just assume that every application has its own memory space, either an own memory block or somehow it's, it's assured that this this particular memory is only it, it belongs only to this, to this program. So at the top, usually there is a code. So there's something called called code segment. And this holds all the instructions. Everything that needs to be at some point could be at some point executed. After that there is the static data. So this is the part we know at the beginning that will be there, like global variables, for instance. And then we need two additional memories, stack and heap. Stack is where all the local variables and function arguments will go during function calls and heap. We covered heap last week. Heap is the dynamic memory from which we allocate dynamic variables like with malloc or new or other mechanisms that are provided via your your application, your own time. It's slightly more difficult than that because for instance, the static debt data can be divided into three parts. First of all, there are constants, some literals, and they are somewhat. They lie somewhere in between code and and the data. Because they are not code, they cannot be executed obviously, but also there cannot be modified. So this is like a static data but read only static data. And then there are two types of static data, initialized and initialized. Initialized means that you provide you provided in the code the initial values for this data like an initializer for your global variable or if you have a modifiable string, if you have a not constant character pointer but character pointer so it can be modified some some time later. And initialize static data is like a huge array for instance, which is not filled by anything or which is, which is initialized not with the data as a data embedded in the code which were embedded in your source code, but initialized with something implicit like zeros. If you create a vector, huge vector of of data. But that's a bad idea because that's a bad term because vector actually is STL container. So if you create a huge array, this array is zeroed at the beginning, but there is no implicit part of the data which is present in the executable. This data are initialized via data stored in the executable file on the hard drive. So this is basically like copying the data from the file into here this is just we need to allocate the memory for them and perhaps 0 them or something implicit. So this is these data are not present, they have not their own copies in the executable file. Second thing is that the stack needs to be allocated for each thread separately. So if we have multiple threads running our application, then each thread has its own stack. We will talk about thread scheduling and stuff like that in the operating systems part. So right now, just let let me assure you that it's not that important for the rest of the part. And then again, stack is allocated dynamically. So it's not present in the executable file. It's it's allocated by the runner when the file when the application is being started. But just this, this hiccup is there because if we have only one stack, it's easy because steak grows one way and he grows the other way. So if you just place one on the on the bottom of the statically allocated piece of memory, and the size of this is obvious from as long as you load, as soon as you load the file, you know how large this part is, How much memory for this. So you can place the stack over here and heap on the other side of your memory space and they can just grow one against the other. Yeah, obviously they can overlap. And once they overlap, you're in big trouble because at that point your memory run out and your application will be terminated because there is nothing else we can do. But yeah, or we can we can deny the heap growth or stack growth. Well, if we deny stack growth, your application re terminate it anyway because that means you cannot call a function. And once you cannot call a function, there is nothing else you can do. You just need to terminate your code. But if you cannot grow heap, there are some mechanisms that can be put in place like the allocation can fail for instance. And there there are maybe ways how to deal with failed allocation so your application doesn't have to end. But if you place multiple stacks one after another, you are somewhat deny this opportunity because one stack can run into the other stack. Yeah, that that's a tricky part. So basically that we are putting some traps at the top of each stack. So if one stack tries to overrun another stack, it's detected by operating system. It's just that there is. This is much more tricky when you placing the stacks one after another. But then again, stack should be relatively small. You shouldn't place two large structures on your stack. You shouldn't create two large local variables if you are creating a local variable. And this local variable should take megabytes or more, then you are not doing your job right. So local variables can be small buffers, integers, pointers, etcetera. That's fine. So it usually takes a few kilobytes. If your stack grows unexpectedly, you are doing something bad. Either you have two large data structures and you should use heap for those. For larger data structures you should use heap or sometimes you can use global memory. And secondly, second option, second possibility is that the stack is growing out of control because you have unchecked recursion, right? If you, if you call a recursive function and you do it wrong, like you don't have the right final case, you probably run into that. But did anybody write something recursive? I I believe you do you have to do that in the first year, right? So so you know what a recursion is. So if you do it wrong, then your recursion routes runs out of control and your function is being called indefinitely. So at some point it will run out of stack. In Python you wouldn't know because Python has a very sophisticated mechanism how you detect that there is a too deep, that the stack gets too deep, you have too many calls performed. So it it kills you before it runs out of memory. But technically in C++, you can run out of stack memory if you do something wrong. Yeah, second, second thing. By the way, in X86 architecture, this is this is the other way around. Stack is on the bottom and it's growing up, and heap is on the top and it's growing down. But that's just a technical detail. The hardware has specified that when the stack is growing, the address is decreasing. So we need to place stack on the bottom. Just a technical technicality. It's not that important. OK, Yeah, I think we can cover the linking part and linking part not linking park and then we can we can call it a day for today. So what's the linking process? We already know that that any C file will produce an object file and every object file will actually hold the first part of this memory space. So remember this, the code constants and initialize static data needs to be put into the executable. The rest will be allocated dynamically or created dynamically by the runner. So these three blocks, these three segments needs to be placed into the object files and then subsequently into the executable file. So this object file will hold code A, constants A and static data A. The same goes for the B. That's right. So each each object file will have these three segments somewhere. And there are there is a library. And as I told you, library is basically like a package of multiple object files. So it contains object file P&Q. So it has all three parts of P and all three parts of Q. And the job of the linker is put all these segments together, but in the correct order, because we need to make these segments compact. Yeah, so we put it all into linker and the linker will concatenate first the code, then the constants and then the static data is enough, right? Well, it's not that easy. Let's just assume that we have piece of code over here. Actually not. There is a function over here which is being called somewhere from the code B. So the address of this function here is calculated assuming this is the address 0. So the offset of this function is calculated from this offset. But if we move the code B segment over here, this address is no longer valid. We need to move it by the size of code A, right? This is what is called a relocation. So every piece of code in B or somewhere else which refers to this line of code, either a jump or a function call or anything else that absolutely refers to this address needs to be adjusted, needs to be relocated, recomputed. So the correct value is over here. So the correct, correct value for this address is all over the code. And that's the tricky part because you you can't, you're not just adjusting this line, you are adjusting every other line of code which refers to this line. So you need to map your code, you need to save this relocation information in your object files so the linker can perform these adjustments when the code is linked. The same goes for constants, the same goes for the data. OK, so this is the main objective of the linker to combine the things properly and then to perform the relocations to the adjustment of addresses because some of the pieces, some of the segments can move. By the way, where did P? Where did, where did P go? The code that P is not not there. Actually, yeah, linker is smart. So linker finds out that that any, no, none of the functions in code P was called from either of these other functions or other segments. So it says, OK, hey, I can drop it no problem. So it just compacts your code. Only the necessary code, only the reference code is put there. The same goes for constants and everything else. So we have like a few more minutes. So I will just start this. I will just tell you a few words about it, but we get into the examples and into the details next week. So the object file actually has a very specific format for Windows we are using cough yeah, I forget what what that means. Compact FF is file format, Compact object file format, something like that. And this is the format of the file where all the segments are placed. So it needs to cover the format of code, constant static data and few other things like the relocation. So basically it's a collection of segments. This is a list of segments. If you just take a object file, this is the object file I compiled from the hello world example you saw before. And this is if I just print out the summary. By the way you can try it your yourself. There is dump bin executable on windows objective dump on on Linux or dump object. I'm not sure there is an equivalent tool on Linux. So you can you can try this yourself. And if you just ask for summary, then this is what what will goes out. So it will list all the all the segments which are present in the object file. By the way, text, yeah, over here text is the code. Yeah, I know. But it's it's it's not called code for some reason. It's called text. Our data, our data, our data over here. This is the read only data or raw data. This is where where the constant will be. Yeah, X data, P data, it's very, very difficult debugging symbols. It's very important. You can, you can either generate a version with debugging symbols and then all additional stuff is placed there so it's easier to debug your code or it's dropped if you're, if you're compiling for production, this is the initialized data, P data and X data. Also use it's awkward stuff for for exceptions and directives. Yeah, there are some some formal declarations. So this is like a list of sections what can be found in the object file. And since I'm running out of time, I will I will just cut it over here and we will take a look next week about how some of these important segments look like. OK, are there any questions, immediate questions before we conclude for the day? So thank you for your attention and hopefully see you next week.