So last week, if I'm not mistaken, we ended up here and we talked about devices and interrupts and we explained how interrupt works to notify the CPU that something has happened in the device, that the data transfer is complete, that somebody, somebody pressed a key on a keyboard or whatever the device needs to report to the operating system, to the CPU. So let's end the part regarding devices. Yeah, that's very important and also very bulky part. Unfortunately, we didn't have time for more details and let's switch for processing. That's another very important topic in the operating systems before we started a few terms just to clarify the details. So first of all you already have some intuitive notion about what programme is, so let let me formalize it a bit. A program is basically a collection of instructions and data, usually written encoded into one file or a bunch of files depending on the operating system. On most operating systems like Linux or Windows it usually is and can be 1 file. Some other systems like Mac OS prefer to use a bunch of files, so they they basically say that one application is 1 directory. But that's just a technical detail. It's always encoded in some in in file or multiple files on the on the hard drive. And usually this is a production of some compiler and linker. So you you write your program in AC or whatever language you prefer, then you compile it and the compiler yields one or multiple executable files. The program just sits there. So the program does nothing until you start it. And when you start a program and you execute a program, it becomes a process. So basically process is something like a running program. Technically process is much more because process is an entity of operating system. So it has some additional attributes, it has some, it has some additional resources. Most importantly, it has its own address space. It has its own memory where the data and the instructions from the file are loaded before you can execute them. Obviously, because you cannot execute code on the hard drive, you need to load the code into the main memory so the CPU can execute the instructions. So basically the program becomes live when you create a process out of it and it becomes live by an activity. And by this activity, of course mean that the CPU starts executing the instructions which were previously stored in the file. Yeah, this step isn't that easy as it sounds, because there are many tiny steps. For instance, if you have dynamically linked libraries, remember dynamically linked libraries from two or three weeks later before. So yeah, you need to make sure of the operating system, the operating system loader needs to make make sure that all the libraries or the dynamically linked libraries are present in memory as well, so they can be interlinked with this executable. There are some relocations, there are some tiny fixes in the code that needs to be done after it's loaded into the memory. So yeah, this needs to be handled before the program can be executed. But these details were covered in the runtimes, at least the most important details. Within a process there should be at least one thread. And the thread is like a the stream of the stream of instructions being executed. That's the activity which is being performed by the CPU. The thread is also the basic unit of kernel scheduling. So the kernel, when it when it handles multiple processes, multiple programs running simultaneously within the same operating system on the same host, it schedules, not processes, but it usually schedules threads. Obviously there may be multiple threats within one process. That's why we didn't make this distinction. Yeah, normally or usually, there is at least one threat within within a process. If you'd have no threat within a process, then this no long, this is no longer valid. Nothing is running in the process, then the process isn't alive. So it's not a process. And sometimes we can make a even finer distinction about what can be scheduled, what can be executed. We can talk about fibers or possibly tasks. Fiber is like a lightweight threat, but this is usually not implemented within the operating system. Fibers are usually implemented in in libraries and they usually use some a form of cooperative scheduling. So usually fibers are like a lightweight abstraction above threats. But some some libraries or some operating systems don't even recognize fibres. So this is just another term that you might need to remember just to get you the full picture. But we won't get into the fibres in the terms of operating systems because usually fibres are implemented outside operating systems in libraries. What's the most important part is remember how the data are laid out in memory when the process is started. So basically the code and the static data are loaded or initialized when the process is start. This is the part that is loaded from the file when the process is executed. Some part of this might be also loaded from the file. The other part is just zeroed or initialized. And then we have the stack and heap, which also needs to be initialized when the process starts. And the important part, you, you should remember right away that each thread has its own stack. So if we have multiple threads, there need to be multiple stacks present. And the tricky part is, yeah, if we have a like a huge address space, we can place the code and the static data at the beginning and heap at the bottom and we just say, OK, let let the heap grow upwards and let the stack grow downwards. It's just a convention. We can do it the other way. So in this case, if we have one, one stack and one heap, it's fine because they are growing one against another. And if, if you have like a 64 bit address space, then it's fine because they will never intercept these, these two barriers will never, never touch. Well, yeah, they technically might. But if you have a 64 bit address space, it would take a real effort to to do that. And, and also it would mean that you have some enormous machine with, with memory that is unimaginable because 64 bits is a lot. So normally it's, it's, it's not what happens. But if you have multiple stacks, what might happen is that one stack grows and it hits the other stack. That might happen if these stacks are too close together. So first of all, stacks are usually designed to store small amounts of data. If you, for whatever reasons, place a large amount of data on the stack, so you create a huge local variables, you're not programming very well. That's the rule #1 So usually stacks should be small. Second, if we have a 64 bit address space, we can still place these stacks quite far apart because we have enough room for that. So there might be terabytes in between. So usually this shouldn't happen either. And also it's a good idea to place some cushion some some specific area just right above the the second stack. And this area is like a special marked. So when anybody access this area, it means that the first stack has overflown and the operating system usually shuts down the application. So it's like a memory that shouldn't be touched. Yeah, we can do that. I will explain it later when we talk about virtual memory, but there should be some safeguards which prevents 1 stake hitting the other. So that's that's one thing to remember. Let me take a closer look on the distinction between the process and the threat. So they are they're both important entities of the operating system and the the important things to remember process is mainly the code loaded in memory. So the one process is 1 code. All threads share the same code. Yeah, usually they are executing different parts of this code, but for instance, two different threads can execute the same function within the same code. OK, so they share everything. They also share the global memory. So they have one memory space and all the threads one process, All the threads of the process share the same memory space, which is a bit tricky because if these two threads or multiple threads are running simultaneously, they can also simultaneously update or overwrite the same memory, which is potentially dangerous. You need to make some steps. You need to make some effort to do this properly so there are no mistakes. There are incidental mistakes. And also any other system resources. This, this, this includes mainly the file handlers, also network sockets or whatever network connections your application makes, any operating system primitives for UI synchronization primitives, etcetera. On the other hand, threat is just a position in the code, right? But it's not just the position. The position together with the programme counter of course also includes the CPU state, because if there is a code to be executed, it always alters the CPU state. So the entire CPU state is part of the threat context. If threat is switched to another CPU core, then the CPU state must be saved and then restored on another CPU core whenever threat is rescheduled. And by the CPU state that this mainly concerns the the CPU registers but some other stuff and of as you already know the stack is also property of a thread. So if we have multiple threads, we have multiple stacks. Technically if you just remember where each variable resides, then you can deduce right now easily that each thread executes its code on its own local variables. So within local variables, within function arguments, each thread has its own copy. It's like recursion, right? So basically each thread has its own stack, so all the local variables are unique to that particular thread. On the other hand, global variables or heap variables can be shared, are shared in fact, among all the threads. So we need to make some synchronization efforts if we need to access them. OK, How to create a process? That's the question, right? So it's actually quite easy. I'm not going into the details of the technical stuff. So basically there are operating system calls, There are some procedures, some functions you can call and create a process in Windows. It's not that difficult. It's only tedious because you need to initialize many individual structures. These structures hold some additional properties, some some options, how the process should be created and what the process can do and what, what are the privileges of the process, etcetera, etcetera. But the bottom line is there is one system. This is a function which actually it translates to a system call or even multiple system calls, and this creates the new process. This needs to be executed somewhere. So always one process is creating another process. That's important thing to remember. Actually the same goes in any other system. Always one process creates another process. So basically technically you need one process or multiple processes which are responsible for executing applications. For instance, in Windows you usually have this desktop right at the front face of the windows or the which also implements the the the sidebar or or the bottom bar where the start menu is and you somehow select an icon or something and and double click it. At that very moment, the process that displays this desktop is responsible for finding the appropriate application that corresponds to that icon and execute it using this system call. OK, but it's the process that creates another process. What's also important is that each process yields some sort of a handle. It has some identification. We can shorthand it to PID, like a process ID, but there is some handle of the process and we can use it later, for instance, to wait until the process finishes. So when you execute and one process executes another process, it gets an handle. And this handle can be used for various things. And the one of the most important thing is wait for the process to die. Because usually if you, if you like, execute a script or execute a batch application, you want to know when the computation finishes, right? Yeah, sometimes you execute an application which tends to run for a long time. For instance, you execute a Firefox or Chrome or whatever is your favorite browser. In that case it usually stays on because everybody right uses. You usually have your browser open all the time with 27 different tabs, usually with forgotten stuff, etcetera. So yeah, it's possible that the the process may run for a long time, but normally, normally more usually to let's say our computation perspective or programming perspective, you execute something like solution to your homework. And the solution to your homework tends to load some data, load some input, perform some computation, yield the output to a file or to the standard output or whatever, and then terminate. So it's a good idea for the parent process to wait for the child process to terminate. In Linux, it's very much the same. So you can see there is a similar function, similar procedure to create another process. And also this procedure somehow yields PID. I'm, I'm going into the details later. So you get the PID of the new process. So in the end, you can wait for the new, for the newly created process. That's it. OK, that's the common that are the common part. What's slightly different is that here you are creating a new process as a new entity. The old process is completely intact. It gets the handle in here somewhere. Yeah, this structure is filled with some information about the newly created process and you can get the handle out of the structure, but that's it. This, this is the only communication that happens after the creation in Linux. It's somewhat different or actually this is not a specific of Linux. This is how all Unix systems work, right? So there is a special functions of special operation of the operating system called fork. And fork tends to duplicate processes. So you have one process, the parent process that wants to execute another. So first step of this is that the parent process fork itself, it duplicates itself. Yeah, it's awkward, but it's like how this how this works. It's a similar to let's say cellular division in the Organism. One process is split in two. And the tricky part or the the funny part of fork is the new 2 processes share everything including the memory. So technically this is like a creating a new threat. It looks like creating a new thread, but it really creates a new process. But both processes are running now and they share everything, including the memory, including the stack, including the program counter. So those two processes after the fork just happen to end up here on this line of code. And the only distinction or the only difference is then one of these processes is given the parent process is given the ID of the child process. OK, so basically this value, the P value, which is PID, yeah, if the fork fails it, it returns some error code like -1 or something. And then you need to then the fork didn't work and you're still in one process and you need to handle the error. Unfortunately, if fork fails, then you're doomed, because if the if the operating system cannot perform fork, you're done. You can just reboot the machine or something, because you're screwed everything. If you type a command in your shell and you want to execute some trivial command in your shell, it creates a new process, it calls fork. So if you cannot write a simple program in your shell, the root cannot log in. You cannot fix anything in that machine. The only course of action is to reboot the system. OK, so let's skip the error handling. It's not important. So these two actually these two if else branches are for the two newly created processes. If P is 0 then I know this is the newly created process because zero is a special value. No I no process in the system has has PID zero. The first process created during the bootstrap the init process has PID 1 so no process ever has PID 0. So if I'm if this 4 grits on zero I know it's not valid PID. So this is the, this is the branch of the newly created process. And the exec is a special command that say hey, throw everything, everything away, discard all the memory of me, disclose or close or open files or sockets, everything and glowed a completely new process in my set in my memory space. So this is like an execution, but within existing process. That's the only distinction from the from the windows. If you didn't get an, the else part goes for the original process, so the original process gets a valid PID. So P is not zero. So here it can do something with that valid PID, like wait for the newly created process for instance. If you didn't get the details regarding fork, it's not that important. It's just like a nuance that some operating systems take a different approaches to this, to this creation of new processes. But in the end, the same part is there is some function that basically creates new process. Yeah, I forget to mention one important thing. New process is always tied to a program and program is executable file somewhere in the file system. So yeah, the path, the executable file and possibly some command line arguments or the command line together. This is like these two strings together. Basically is given to the create process. So somehow I need to give the create process or the exec the path to the newly to the program that should be executed. So this is this is the connection between process and program. Yeah, I, I talked about processes, creating processes. So usually we the original process is denoted parent and the newly created process is denoted child. Because we like analogies from real world, right? And the child, at least in the nooks, holds also identifier of its parent. It's a good idea that a child know who his parent is. That's also an analogy from real world, right? A child notifies its parent when he dies. That's a handy tool. As a parent myself, I would very much appreciate this feature in real world. Unfortunately it doesn't work that way. But in the end, what can we do, at least in our world of computer science of of computers, at least in Linux, a child sends a signal to the parent when it dies. Technically speaking, the operating system sends the signal to the to the parent, but when the child dies. But, but that's no difference. And another thing is that a dead child becomes a zombie until somebody asks how it died, until somebody asks for its exit code. The exit code holds the the value, the reason of death usually. So this is like a benefit we have in in Linux that we can wait for the for the termination. We can we get notification when a process dies and we can also ask its exit code, even, even if it's long debt. And then again, it's a good idea to ask your dead children how they died because otherwise you can, can end up the zombies can, can bottle up in the system. And and yeah, they, they take resources. So it's usually good idea to quickly ask your dead child how he died. So, so the resources of the zombie process are released. Killing a process doesn't kill children. That's again an analogy from the real world, because usually when you kill parents, the children live on. So this same goes in in operating system, which also means that killing a huge sub tree of processes. Because if you have a complex application, the application itself might execute multiple processes you have like the main process which corresponds to the loaded program, but then again it executes additional processes that run on background for instance. So usually if you want to kill the whole application then you need to kill multiple processes. You need to kill the process itself and all spawn children. And that's not possible easily because you can kill the parent, but when you kill the parent then the children's are orphaned and readapted by the in it process. And the in it process in Linux is always there. That's the first process started at the beginning. It has PID 1. So all the the children are somewhat rehashed, regrouped under the under this in it process. It's like an orphanage for all the processes. By the way, this also, this is also how demonization works. If you you want to create a demon, do you know what the demon is in the in the Linux terminology? It's like a system service or it's like a service that tends to run in the background. OK, So what do you do if you want to create a demon? You have like some initialization exec process for the demon. The exec process spawns the demon and then the demon. The child process kills its own parent when the parent is dead. The demon process. The child process is is adapted by the, by the by the init process and it it can do all the stuff it needs to do in the background without any direct connection to its parent. OK, so this is how demonisation works in the in Linux and another another. The implication is that if you want to kill the whole subtree of the processes, you need to pay. You need to put in some extra effort. And it's a bit tricky because technically a process can be spawning other processes. Why are you killing its children? So you need to be quick about it or you need to suspend the whole tree before you start killing. Yeah, it's always better. It's always easier to kill someone when when someone is asleep than than to kill someone who's running, right? That's like one of the basic rules of, of this kind of business, right? So technically you need to do some extra effort to to kill the whole subtree of processes if you want to. In Windows the situation is similar, but a bit more complicated. The trouble is that originally the basic structure which is called Win 32 process entry doesn't does not contain the PPID, does not contain the parent process ID, which makes it slightly more difficult to track who's the parent of which process. There is no actually there is no special relationship between the parent and the child. So yeah, you can get this information from some extended system information structure. So yeah, there is a way how to get to that, at least on modern windows. And it's a bit just a bit tedious to do that, but you can do it. But then again, killing a process doesn't kill all its sub trees. If you if you kill a process and the whole subtree is killed in the in the process manager, for instance, yeah, you might do that often your Firefox or whatever browser you use just go go crazy, do some during some update or something just goes wacky and you need to kill it completely. So you just find that the the browser in your process manager and then you, you, you press the button, kill the process or end task or whatever, whatever its name and the whole subtree of all the processes is killed. But that's the job of the task manager. Again, there is special effort needs to be done and this special effort is implemented in this process manager. And this process manager is an application like any other. It just has very powerful privileges so it can kill other processes. So this is about parent child relations. Let's move on and let's just take a look about on how threads are created. About threads that that's much more easier because threads are normal part of parallel programming. So you you need to create threads all all the time. Actually usually you don't even create threads on your own. Creating threads is usually job for some parallel library you using. It's a very low level stuff, even for programmers, but even programmers sometimes need to create a thread. All operate, all common programming languages have direct adaptations, direct abstractions regarding the threats in C + + C#, Java, all these languages. Thread is basically a class. So you're basically creating a new instance of a new class, new object. And the only thing that you need to remember is that this new object is given a piece of code to execute, which is usually a function. Yeah, it might be something else like Lambda or delegate or whatever programming language you're using. But technically it's bottom line is it's some sort of a function or method that's being executed. Plus optionally it may get some additional parameters and this it's like a function call. Technically the difference is this new function is called, is executed in the newly created thread. But then again it's very similar to function call. So over here once the thread is created, it immediately executes. So here in this part of code, the main thread which executes the main function and this newly created thread T are running possibly concurrently. I'm saying carefully possibly because there is no guarantee that these two threads will ever run concurrently. It depends on your system resources. If I have a very old or some very low tech PC, then this code might still run sequentially. In this case T and main thread might actually run somewhat semi concurrently. So they will like share the CPU and for a while they will be running the first thread, for a while they will be running the second thread, and then we will be alternating on the CPU. So that's also possible. But if I have a normal modern CPU with multiple cores, it's very likely that these two threads will be really executed in parallel until I call T join, which is like wait for death. It's very similar to the to wait for a single object or wait PID for the processes. So basically here I'm waiting for T to terminate. This will block the main thread. The main thread will be suspended until thread T finishes it's job, finishes this code and terminates. So this is how threats are being created and terminated. And the most important part, the most important job the operating system does is scheduling. So it needs to perform, it needs to handle all the processes and make sure that they share the CPU in some, in some manner. The scheduling algorithms, yeah, are quite complex to explain the best scheduling algorithms would take much more than this course. Actually, it would take like a dissertation if you want, if you're into this stuff. So it's quite complicated, especially now when we have multiple CPU cores. By the way, just to clarify a few things, Multitasking term stands for executing multiple processes within the same operating system. Multitasking is something which is quite old. Actually it's not that old because I still remember an operating system that didn't have multitasking. Would you venture a guess what that system would be? I was using an operating system that didn't have multitasking. Yeah, actually all the windows had multitask. All the windows I knew. I'm not sure about all the windows. All the windows I knew had multitasking. Yeah, MSDOS. That was the only system that was available when I was young and that didn't have multitasking, not at least not in the traditional sense, because in MS-DOS there was only one application running at any given time. Yeah, you can like a suspend application, keep it resident in memory and execute another application, but it's not real multitasking. So multitasking is concurrent execution of multiple processes, real concurrent. And it can be done in single on a single CPU core, it can be done. You just need to make sure that all these processes are somehow sharing the CPU core evenly in a time multiplexing. Which means each time for time slice, for a short period of time, the CPU is allocated for one process only. But this time is kind of short. It's for few millisecond or few, few tenths of milliseconds, so that it creates an illusion that these processes are really running currently, but in fact they share the CPU in a very fast, orderly fashion. Multi processing on the other hand is a utilization of multiple CPU cores, which is the job for current operating systems because usually you have multiple CPU cores. I'm not sure if you can even buy less than two or maybe 4 cores nowadays if you're buying modern and mainstream CPUs. Of course, I'm not talking about specialized CPUs like Arduinos or CPUs for your washing machines, but normal CPUs for your normal laptops. I'm not even sure if you can even buy CPU with less than two or less than 4 cores. So this is a normal thing that all the operating systems needs to be doing and actually they need to be doing these two things simultaneously. Usually you have many, many more processes in an operating system. By the way, if you have access to some Linux server, you can easily check how many processes are running on that Linux server. It would be thousands usually. Normally, yeah, it depends on what survey it is, etcetera, how many users are using this server. But technically it's no, no problem if there are 1000 processes. So yeah, you usually don't have 1000 cores. So you need to balance all these processes among all the available cores. And that's quite a juggling act. You, you need to do this properly. Yeah, sometimes you need also to set affinity. Sometimes you want to prefer that some processes are running, running on some cores only. Sometimes you have a very specific processes that needs to run all the time or that needs to consume a lot of resources or they need to respond in real time to something. So you can allocate CPU cores forever for these, for these processes by setting affinity that specific process runs on a specific core or set of cores. Yeah, and all these things are very, very, very challenging. Another thing you need to understand before we get to the scheduling algorithms themselves is what's involved in the multitasking. When multitasking happens, what you need to do, what you need to be able to do, is to suspend one process, actually suspend one threat. I've previously mentioned that we are scheduling at the level of threats, not processes. So you need to take one thread away from the CPU core and put another threat in its place. What that involves. This involves something which is called context switch, changing the context. The context itself holds everything that's necessary for this, for the thread to run on the CPU core, most importantly the registers. That's the most part. If you remember at least the registers, that would be good, right? But well, if you remember that there are registers and maybe something else that that would be good because usually there is something else. For instance, if there are some additional units, let's say 87 Co processor, yeah, probably we don't have these anymore. Sometimes we do for some specialised CPUs. But if there are some additional units, you need to save the state of these additional units as well. By the way, if I'm talking about registers, I'm not talking only about the 8 or 16 registers. I've, I've shown you there are many more registers like vector registers or specialized CPU registers, system registers. So there are many more registers than you you've seen. And also it needs to adjust the virtual memory so the address space of each process is unique. I haven't talked about it, about this in detail. We will cover it in two weeks or so, or in three weeks, sorry, there will be vacation. But basically it needs to switch to the virtual memory of the new thread. If the threads are of the same process, this doesn't have to be changed because both threads have the same virtual memory space. But if these two threads are from different processes, this needs to be adjusted so that the new thread, the newly executed thread, will see the virtual address space of its own process, not of the old thread. Of course, that would be dangerous. By the way, this doesn't involve memory caches. Memory caches are transparent. So yeah, they can affect a performance. Because if you have one thread heavily crunching some data on a CPU core, and then you put another thread in its place and it starts to crunch a complete the different numbers, complete the different data, then yeah, it will have different data set in the caches. So this data set wouldn't be very useful to the new thread. So it it kind of needs to flush everything out of the cache and load new data to the cache. But this will happen automatically, transparently, it cannot access the data of another thread in the caches. Caches are transparent. They work on an on a physical memory, not on the virtual memory. So yeah, caches, switching threads on CPU cores may affect the caches, may affect performance, but they don't they don't pose a threat from the perspective of the operating system. So yeah, context, which basically means that I need to store everything listed in here somewhere in memory. Every CPU thread, every, every operating system thread entity has its own space somewhere in the operating system kernel where everything involving this can be stored somehow and from where it can be restored. So basically you need to save it for the old thread and load it for the new thread. And it's quite costly. This context switch cost hundreds or maybe thousands of instructions because just just you need to save the registers, right? So count how many registers you have and for each of these registers you need to you need to perform store or load instruction. So there are plenty. And now let's go to the scheduling small, let's say diversion before we get to the real scheduling, not real scheduling, poor choice of words. Before we get to the, let's say, scheduling in mainstream operating systems. I've got, let's say a bit of a teaser that scheduling can be quite complicated if you have quite specific requirements. This is a once just to put you any at ease. This is just one slide. I'm talk, I'm not going to talk about real time systems. That is a very specific domain of operating systems. But sometimes the scheduling can be very difficult, especially if you need to to maintain some guarantees in real time operating systems, in real time scheduling, you need to, let's say meet some demands that are made outside of the operating system from the external environment, especially if you're doing scheduling or handling processing in some sensitive operating system, in some sensitive machine that controls mission critical systems like self driving car or a nuclear nuclear power plant, which is, by the way, do you know where the nearest nuclear power plant is? Where the not, not the power plant where the the nearest fission react reactor is. Yeah, somewhere over there, a few meters next to us, a few 100 meters next to us. So yeah, if something goes terribly wrong, well, don't worry. It has a power outage of which can probably boil water in a kettle, but that's it. So it, it's not very powerful, but I've been there and, and we've overloaded it once and yeah, it makes a huge noise when the real time system decides to release the graphite rods that control the reaction and they just fall down and stop the reaction. So that's the stuff I'm talking about. If something bad happens, if some bad event, not not necessarily completely bad, but when some event happens, we need some guarantees like the deadline. We need to make sure that within some time something will really happen, right? For instance, if you overload the reactor and the operating system gets a notification that the power outage or the reaction monitor or whatever is out of scale, it needs to within a few milliseconds or a few 100 milliseconds, it needs to release the magnetic clamps that holds the rods and it has to be done. It's it's a deadline which which needs to be met if you have a self driving car and it's it's approaching an obstacle. On the road, it needs to deploy brakes right away or within some reasonable amount of time. Also that there might be additional constraints like release time, for instance. There is some time delay that needs to be, let's say, observed before the action is taken. A good example would be a an airbag deployment. I'm not sure if you've ever been in a car accident. I hope not, but if you if you at least observe it in some test or something, you may have noticed that the the airbag isn't deployed immediately. If a car crash is detected, the airbag needs to be deployed as I don't know 1050, I'm not expert, but with some delay so that the body of the driver or the the the next two driver, the the passengers are moved by the momentum into the right position, right? Actually there is some additional mechanisms involved. You know, if two cars crashed or if a car crashed into an obstacle, there are some deformation zones and the the engineers have calculated how much actually time it takes before the head of the driver gets to the right position above the the steering wheel. And at that very moment, the, the deployment of the Arabic must commence because if it if it's sooner, then it, it will blow up and then the head will move into the deflating airbag, which is which was which would be no help at all. So it these two forces need somehow to work right together. Furthermore, there are some additional techniques like your seatbelts. They have some specialized pressured gas valves and and and containers which are released so that your your seatbelts are loosened up just a bit so that the seat belt like gently releases you into the airbag. These two things need to be coordinated, coordinated together. So technically what I'm speaking about is that all these things needs to be put into a synergy and it must happen in this precise order. By the way, when we're talking about deadlines, there are some different types of deadlines. The the most basic distinction is hard and soft deadline. Hard deadline is a deadline that needs to be met. And if it isn't met, it doesn't matter anymore. For instance, if you don't release the, the graphite rods into the core of the of the nuclear power plant and you don't do it in time, then you you shouldn't bother anymore because there is no power plant to speak of, right? If you don't deploy brakes before the obstacle, then it doesn't matter because the, the CPU that computes that drives the car no longer exists because it's been demolished by the by the obstacle. So this is deadline that really needs to be meet. But many systems have lighter, lighter concerns like soft deadlines. For instance, if you play a movie or if you send an SMS. Do you guys know what SMS is still OK? Just kidding, I hope so. So if you send an SMS for instance, there is a soft deadline. How long does it should it take for the for the SMS to be delivered? It's usually within seconds, but it's a soft deadline. So if this doesn't happen within the deadline, it it bothers the the authors of the of the software, it bothers the, the vendors that bothers the company that runs your cellular network. But in, in, if it doesn't happen too often, the people wouldn't complain. So it's like, yeah, you can sometimes if the system is really overloaded, you can sometimes relax these deadlines, but you shouldn't do that very often. If you're playing a movie, if you have AVCL or whatever player you prefer, so you're playing a movie which you just downloaded from a torrent or whatever, then you can, the player itself needs to deliver the images, the frames of the of the movie in some timely fashion. So it needs to meet a soft deadline for each of the frame to decode it, to properly render it, to place it into the right buffer for the in the video card, etcetera. Yeah, if sometimes this deadline isn't meet you, you can see it in in your player as a hiccup as a as a small distortion in the in the movie. But if it happens once or twice during the movie, you will probably won't complain, right? If it happens every second, it would be problem you you wouldn't you did you wouldn't like to watch that movie. So this is a very specific things that needs to be sometimes observed. But fortunately for us, let's focus on normal systems when these hard constraints or more difficult constraints aren't required. First of all, we need to perceive how each threat or scheduling unit is maintained within the system. So basically there are few states a scheduling unit may be in. When a thread or process is created, there is some initialisation state, yeah, all the data structures needs to be filled, etcetera, etcetera. That's boring. And afterwards it gets into the ready state. In ready state, it's ready to be executed immediately. So it has everything it needs, it has data loaded in the memory, it's basically ready to run immediately, right? But usually most of the processes, most of the threads are in the ready state. Because usually you have much more threads, much more processes then you have CPU cores. When it happens that CPU core becomes available, it switched to the running state. So the running state is the instruction of that particular thread are actually being executed by one of the cores. And this might end by two possibilities. One possibility is that the threat is has run out of time. This running state wouldn't be there, couldn't be there forever. So each thread is given a slight time slice, a short period period of time for how long it can run. And once this time is depleted, it can either be switched back to the ready state, or it's also possible that the thread performs some operation, that is, that requires some additional resources from the operating system. Let's say loading data from a file. That's a costly operation because loading data from a hard drive is like 6 orders of magnitude slower than executing instructions. So it's it's, it would be a waste if, if this threat will be running just waiting for the data, if it just wastes millions and millions of instructions or millions and millions of cycles of the CPU while it just wait. So it's turned into a boxed state and it's somewhat temporarily, temporarily removed from the scheduling. So it's not planned for running for for some time. Once the data are loaded, when the conditions for blocking are resolved, then it's turned from the block state back into the ready state and it can be scheduled. It can be executed again. Once all the instructions of the thread are finished are are concluded are the thread terminates. It's actually, this is for a I'm not distinguishing between threats and processes too much right now. Yeah, if threat terminates, it can be joined. If process terminates it, it becomes a zombie and it can be asked for for exit code. So but the the principle is same for both. So it's turned into some special terminated state. And in in this state, it waits until somebody asks how the process or how the threat has has terminated. So there are multiple ways how to implement the scheduling. The simplest way is cooperative scheduling. By cooperative it means that the threat, when it's switched to the running state, it must voluntarily yield the control. So the threat itself is responsible for monitoring how much time it's spent on the CPU. And if the threat deems it has enough, it doesn't have anything more, any work that's very important right now to do or it has deemed it, it it took too long to compute something. So then it can decide to yield the CPU to another threat. Trouble with that is that if you have a malfunctioning threats, malfunctioning programs or malicious programs or just programs written by lazy programmers who don't want to yield the CPU, that's, that's probably the most often scenario, then the operating system has no means, no additional means how to take the threat from the CPU. So this is a very, let's say, naive way how to look at the at the multitasking. This is usually something which is employed in parallel libraries, not in a, not in normal operating systems, but in special specialized libraries. Well, you assume that all the threads, all the units are cooperating somehow in the same effort. They just divided the work. So they can run the work, They can compute the work on multiple cores, but they are computing together, They are all in this together. So they want to yield willingly, because they need to share the data with with their fellow threads, their fellow computing units. But normally in operating systems, we usually tend to use pre emptive scheduling. By the way, I know only one operating system. I've used only one operating system that used cooperative scheduling and that was Windows 3.1. It has multitasking, but cooperative multitasking. So yeah, processes programs in in running in Windows 3.1 needed to yield explicitly if they wanted to allow other programs to run. So usually we are doing pre emptive scheduling which means each thread is given only a time slice that that's what I was talking about previously. This is how this usually works. We we have a short time slice, short periods of time which are given to each to each scheduling unit. And the operating system has some hardware timer, uses some hardware timer that uses interrupt to monitor what is what is being, what is running on each CPU core and to change what is running on CPU core. So basically imagine, imagine it as this, there is a timer that can take, I'm not sure every millisecond or something like that, OK or every few milliseconds. And this timer every time it takes it executes because it's an uninterrupted. So it executes an interrupt handler written by the operating system which invokes the scheduler. And the scheduler takes a look at the the thread currently running on the CPU core and says OK, you can still run or it says no, you you've been running too long. And if it's if it has been running too long then it blocks. Sorry, it it consumed it whole time slides, so it blocks, it interrupts the thread and it makes a context switch to another thread. OK, so it can swap which threads are running on the CPU core. This is basically how the preemptive scheduling works. So remember what's the important part here? You need to remember that we need external help. There needs to be some external source of some external timer that helps us with the scheduling. Otherwise we wouldn't be able to interrupt the running process. Interrupt the running thread. Unfortunately, deciding how long the thread should run on the CPU and which thread should run on the CPU next is a difficult task. The mainly it's difficult because we have many objectives we want to fulfill and these objectives are not entirely compatible. For instance, we we obviously need to optimize the usage of CPU. We really want to use all the CPU cores and if we have some workload to do then we really want to compute. So we don't want leave any CPU core idle. If we have work to do, we need to allocate the CPU cores fairly. This is one of the most important parts. So you want to give equal share or as equal as possible share to each of the processes, each of the threads which are currently able to run. Otherwise some of the processes might feel like left behind or something. You need to maximize throughput, which means throughput is number of processes completed per time unit. So you want, you really want this to scale so that many processes can be processed together and finish all together within the operating system. And also you want to minimize the turn around time. So for each of these processes, you want to minimize the time that takes for the process to finish. Yeah, this these two kind of go somewhat together. And also you need to minimize waiting time, so or minimize response time. So for each application, for each process you want to minimize the time it spends outside of the CPU core, especially if these applications are interactive. Unfortunately, individual applications have individual requirements. If you have something that handles UI, such application can sleep most of the time or be blocked most of the time and only needs to wake up when something happens in the UI, when the user clicks a mouse or where the user presses a button. OK, so at this at this time at this point, the application needs to wake up and do some stuff and then it can sleep again. However, there are also different other types of applications which tend to compute a lot, tend to crunch numbers. These applications want to run as long as possible on on the CPU core without possibly any interruptions. So yeah, you can see that they have different requirements. And also these computing applications don't bother themselves that they don't care about response time because it doesn't matter whether they sleep for a millisecond or or one second, it doesn't concern them. So yeah, these are like a basic requirements and all these requirements, I wouldn't say they are all met, but the operating systems are trying to address all of these issues when designing scheduling algorithms, how to do this, how to, let's say try to bring some fairness and and some some additional stuff into the scheduling process. So first of all, let's talk about priority. Each process, sorry, each threat, each scheduling unit has, it's a priority. By priority, I mean something like a number. Imagine a number, and this number expresses the importance of the process or the importance of the threat. The more important process, the more important threat, the sooner or the more often it should be scheduled. Or it might be scheduled before lower priority processes. OK, so this is the notion of priority. Usually we divide priority into two parts, a static priority, which is like a part of this number which is assigned when the program starts, when the threat starts. Yeah, you can do that using operating system tools or or even command line tools. And you can just adjust this number to denote the importance of the process or or the threat within the process. Usually the process have has one number, threads has another number, and these numbers are somehow somehow combined together. And then there's a dynamic priority which is changing over time. And this dynamic priority is there to add fairness to the scheduling. So basically, the dynamic priority tends to grow when the process or the threat hasn't been scheduled for a long time and decreases when the process or the threat takes some computation time, spend some time on the CPU. One possibility how to look at it is that the priority, the dynamic priority is increased for all ready processes or for all ready units of scheduling every once in a while. So every once in a while, everyone who's waiting for the CPU is giving a slight nod, slight bump in the in the dynamic priority. And once a process or a threat is executed, is assigned to a CPU, this dynamic priority is reset to 0 or to some lower value. So it's decreased. And this usually works. The whole priority is usually combination of these two numbers. So it's like having 2 numbers which you, you can just add them up or something. And how use this numbers to actually plan for something? So there are many algorithms that can be implemented. The most most straightforward one, yeah, just to have something in comparison is first come first serve. So yeah, that's a boring algorithm. Just to have some basic baselines. You have a single queue where the processes are stored. Yeah, the processes are stored on one end and taken from the other. So let's say the processes are put in from the tail and and taken from the head and the head process is running on the CPU. Yeah, this in this implementation without anything, this was usually used in batch processing for large computers like 40 years ago. And the head process is running on the CPU until there is no it has anything else to do. And when it has anything else to do, it's just removed from the queue and that's it. Yeah, that's not very, not very interesting. You can also have instead of regular queue, you can have some other queues like priority queue where the processes are ordered somehow. For instance, you can take shortest job first. This minimizes throughput because the the processes will be the shortest processes will be executed first. So you can process more and more processes in a in a unit of time. Unfortunately this requires 2 things. First of all, this is not a good idea to do to do generally because long processes would starve. And 2nd, you usually don't know how long a process would take. That's an information which is like, yeah, you can, you can detect this information exposed, but not a priori. So usually you don't have this kind of information. Yeah, you can do this the other way around, but that's even worse. So what can you do in the terms of pre emptive scheduling? That's more interesting that those were just the baselines for non pre emptive scheduling. So the the easiest stuff you can do is a round Robin scheduling. Yeah, round Robin actually was originally designed as a method for signing partitions or or signing some documents which are inflammatory. So if you're signing some for something like a petition, then you don't want to be first on the list, right? Because the first one on the list should be persecuted. By the authorities, right? So they devise a method like a round tape where isn't clear who was the 1st on the on the list who signed it first. This was the way how to obfuscate this so everybody's on the list, but there is no clear first name. So the the round Robin technique is that you have multiple things, in this case the units of scheduling. You still have a queue and you assign the first process in the queue to the CPU, but only for the time slice. If it completes then it's easy, you just throw it away and take another. But if it gets blocked or if it runs out of time, you just take it and put it to the queue again. So you are rotating the units of scheduling possibly evenly on this, on the available CPU core. That's the easiest thing you can do. If all these units of scheduling have the same or similar requirements, it might work fine. Actually if you're implementing A trivial operating system, like if you're implementing a core kernel core for the operating systems course, this is what we did because you don't really need to do anything elaborate just to make it somehow work. This would be like the first implementation for your operating system. Unfortunately, if some of these, some of these units of scheduling have special requirements, especially for interactivity, for low, low latency, this wouldn't work very well for them. Because usually these UI processes or UI threats will get to the CPU. Then they will notice that nothing happened from the point of user interaction. So they will immediately terminate, immediately yield the CPU and will be put in the queue again. When they will, when they spend a long time before they get the CPU again, they don't use the CPU, they need to use the CPU when somebody, something happens in the UI. And if they're waiting in the queue and something happens in the UI, it will take them long time before they can respond to that. So yeah, it will work, but it will get these interactive processes somewhat sluggish. So what we need to do is somehow divide the priorities, divide the types of processes. This is 1, let's say, like a virtual algorithm. I'm not sure if this was ever implemented in a real world. A similar algorithm was used, or algorithm that somewhat emulates this was used in older versions of Windows. I'm not sure what exactly Windows are using for scheduling nowadays, but let's just imagine that we have multiple of these round Robin queues. And this is like a queue for the interactive processes and this would be like a queue for the heavily, heavily computing processes. And of course you might have multiple queues in between. So this design has some additional rules. Of course, the rule, the first rule is where should we pick our process? Because if we have multiple queues, we need some rules which queue to choose first. So we search them from top to bottom. If the first queue is non empty, then we take process from the first queue. If the first queue is empty, we take the second queue etcetera, etcetera. We take the first non empty queue from the top and we take the head process from that queue. So this is who will be scheduled next. OK, when the process is blocked so or yields its computation slice. So when the process ends prematurely, let's say, then the process is moved one queue upwards. If the process is blocked, it's taken temporarily out of the scheduling. And then when it returns, it's it's placed one, one position upwards, 11Q upwards. If the process yields its time slice, if it doesn't want to compute right now, it's also moved one slice upwards. And then the if the process takes all the time, if if it consumes its time slice, it's it is moved one one way down. OK, so the processes that tend to wait for interactions are moved upwards. the OR which are blocked are moved upwards so they can interact immediately when they get unblocked. The processes that tend to compute a lot that they take all the time, all the time slice to compute, they are moved downwards. And also it might be a good idea to change the time slice for these for these queues. For instance, let's say the time slice for the processes in the top queue would be like a millisecond or very few milliseconds. On the other hand, for the low lowest queue, it could be easily 50 or 100 milliseconds because these processes like to compute. So let them compute. Let's give them the CPU core for a longer period of time. These processes want just to check something, so they don't want to compute that much. OK, this is basically how this works. Yeah, it works much better than a regular one queue round Robin. But then again, having this many queues is somewhat tedious to manage. And also it doesn't always work, as I said, because for instance, if you have a process that just yields its own time slice, it can end up very high in this hierarchy. But if it doesn't block, if it just yields, it might end up in the top queue. And it's always executed from the top queue. So there needs to be some additional mending rules for that. So if a process is executed multiple times, it it it's given the rest or something. So yeah, it's not entire entire algorithm, but it's just an idea of how the scheduling may be augmented to give some some differentiation among the scheduling units to give you a glimpse about a real world, a real world scheduling algorithm. This is a simplified description of a completely fair scheduler, CFS, which is currently the default. Last time I checked, it was the default scheduler in the Linux kernel. So this is something which is really being used in a real world operating system. So the CFS works as follows. Each process has one important number, which is called virtual runtime. In a very rough sense, it's like how much time this process consumed, how much time this process was running on the CPU. Actually by process, yeah, I'm talking about processes, because just ignore that. It's the unit of scheduling in Linux, it's the unit of scheduling. It's process, but that doesn't matter. So each scheduling unit has this number which says how long it actually take the CPU, how long it actually run. And all these scheduling units are placed in one red black tree. So it doesn't matter, it's red black. The important is it's a balanced tree and we can very fast make a search for a process for the scheduling unit with for a unit of scheduling with the smallest virtual runtime. So normally what we do is we take the process that the unit of scheduling with the smallest virtual runtime and that's the one that is going to be executed. How long there is a maximum execution time which is repeatedly computed as there's the time slice and it's repeatedly computed as the total waiting time divided by current number of processes. So you kind of try to guess from the waiting time how long you should get, what's the even portion of the time so that every once in a while all the processes will get its fair timeshare. OK, so if the average waiting time is one second and there are 20 processes waiting for this particular core, then each process will be given 50 milliseconds, its maximum execution time. And yeah, this value is recalculated adaptively, continuously, because you need to adjust it based on how many processes are there, etcetera. So the longer each thread, each scheduling unit waits, the greater time slice it gets. It makes sense because if the system is overloaded, we want to increase the time slices so that we are lowering the overhead and making the processes really utilize the CPU. And the algorithm itself is quite simple. As I said, we take the left most node of the RB three, so the process with the smallest a virtual execution time, virtual runtime. And this is the one process that will be executed on the current core. If it completes the execution and it's easy, it's just removed from the arbitrary and and it's removed from the scheduling. The same goes if it's blocked, then it's temporarily removed from scheduling and if it reaches the maximum execution time it's stopped or interrupted or contact switch then it's it's re entering the tree with a new time key and the new time key is the old time key plus how much time actually the process spent on the CPU. So it's easy, you're just counting time. It's just time accounting, nothing else. As you can see this this makes sense when you want when you want to prioritize new processes. If a new process, new thread enters the scene, it has virtual time 0 because it's new, it hasn't run yet. So it's it's prioritized over all the old processes which are already there. And we just usually accumulated quite some time running on the CPU. So new processes, new new threats are given priority and they're running for some time before they they're enter the normal herd that that's been there for for some time. However, to make this let's say even because if this would be true, if this whole algorithm would be true as this, then threats that has computed a lot wouldn't get the CPU anymore, at least not if there are any other processes available to run. So to make this working, there is one additional condition that that fixes it. And it says that the virtual runtime decays is decreased over time. So every once in a while all the units of scheduling in this tree have their keys decreased by some constant. OK, that's it, no additional things. So as you can see, the rules are quite simple. And even with these simple rules, yeah, quite simple, there are some additional things, some some corner cases that needs to be covered, right. But basically with these two rules, how to compute the time slice and how to select easily the process that can be executed, that's enough to have a fair schedule. Yeah, of course, this is a like a dark magic because how exactly the virtual time the case I haven't I haven't explained and I do not even know. I can just say that how fast this decays is also affected by the threat or the process priority. So the priorities I talked about earlier are somehow incorporated in this. So processes with higher priorities have higher decay constants than processes with lower priorities. OK, so that's pretty much it. Regarding the scheduling, there are a few more things about processes. One of them is inter process communication. You already know that some processes need to communicate with other processes, right? Otherwise there wouldn't be, we couldn't have features like that. One process kills another process. Actually, you've seen the simplest interactions between multiple processes already. I'm assuming all of you are attending the Linux classes, am I right? So what happens when I write something like this? Yeah, I know this is a sorry about that. It's it's mind teaser. I used to do this all the time because you shouldn't call cat to with a grab. The grab can load its own file, but I I can't resist. This is like AI. Hope your Linux teachers see that and their head explode or something. But technically it doesn't matter. I'm just what I'm doing here. Do you remember the do you, have you seen these two common mind tools? You haven't seen a cat before? In the class. I've definitely seen it out. Well, this is an animal and this is a fruit, so no. So cat, what does cat do? What's the content of the file? So we just take the file and print it to the standard output, Right. And grab is a filter. It's given a pattern is given usually a regular expression pattern, and it filters out the lines of the file that meets this pattern. Basically, we don't need to go to the details. That's not important. But what what happens here from the perspective of operating systems and not really this is a process, this is an application. Actually this is a program which becomes a process to be completely accurate. Same for this. So these are two programs which are executed simultaneously and what what does this do? What is this? It's a pipe. That's right. And what does pipe do exactly? So this will create a process. This process has standard input, standard output and error output. This will create a process. So yeah it has like 1 input and two outputs right? The input is connected to the input of your terminal. The first process is this input connected to the input of your terminal and this pipe takes this. This is city out connects this to. This creates this magical interconnection. And yeah, this is still connected to your console. This is still connected to your console, and this is still connected to your console because there are no additional things. OK, these three things are merged together to the to our console. So basically this is the basic interaction between two programs we can imagine and and it uses standard input and output. So it's, it's kind of lame, but it's very useful if you're doing something on the command line because you just create enough individual applications, you you pipe them together and then you have like a processing pipelines for some textual loading or something. Actually pipes are more generic because pipe is a well actually this pipe is not this pipe. Pipe is also an entity of operating system and it it works in a similar way. Actually it's like 2 processes tend to share a pipe. So they create another link which looks like this. One process can write to it and another process can read to it, but it's separated from the standard input and output. So when 2 processes needs to communicate, it's it's easier if they just negotiate a separate pipe which will have the same intention, which actually you will use a very similar instruction, similar, very similar functions. One will just print to it and the other just will read from it. Yeah, but it will be a separated channel from from standard input and output. That's usually more practical. Actually it works very similarly with with TCP circuits. If you have ATCP circuit, it's also a pipe of some sort where you can write or from from which you can read. So it's it's very similar. By the way, having a pipe is almost identical to having a socket over the TCP loop back. And do you remember what the loop back is? Sorry, yeah, every, every computer has a like a virtual network device inside, inside the operating system, which actually all the processes of this operating system can connect to it and it creates appearance of an Internet connection, but it actually just resends the data back. So if two processes use loop back interface, they are just connecting within within the same computer. The packets are not going anywhere outside the computer. It's just but they they can use the same networking mechanisms as if they were communicating with outside world. So sometimes it's good for debugging or it's a good if you have a, let's say, more complex application that involves multiple processes, multiple programs. And this program is to communicate. And sometimes you want to run them on the same computer some months, sometimes you want to run them on multiple computers. Another way how to processes can can communicate. Yeah, but sorry, so this is what I'm talking about. This is the IPC, this is the communication between processes. Another way is having a shared memory. Shared memory is more tricky to establish. So yeah, I'm not explaining how to create shared memory. But when you do, you basically have like a 2 pointers in two processes and each process points to its own data buffer. But these two data buffers are somewhat overlapped. So if one process writes something into this buffer, it magically appears in another process. Technically they are using the same memory. And finally, final part is signals that that's something that's more typical for Linux or POSIX systems. So in in Linux kernel or Linux application a signal is kind of a call or an interrupt that one process can send to another process. In a sense the signals are somewhat an extension of CPU exceptions. And also some CPU exceptions translate into signals. So if something happens, something bad happens to your application and sorry, exception occurs, then some of these exceptions are translated into signals. The signal handler itself is a is a function. So you you can create a few functions and this and then say, hey, I'm attaching this function as a signal handler for this signal. And when this signal happens, this function will be called. But it will be called immediately like an interrupt outside of the normal flow of instructions of your main thread. It will be called within some service thread provided by the kernel. OK, so yeah, these functions then can be called at any time, at any moment during any piece of your normal regular code. Let me just give you a few examples. Yeah, I have a Just by the way, you may have noticed that there are some additional information sometimes hidden in the notes beneath the slides. If you haven't noticed that, check check it out, because there are some important things so I don't have to rewrite them to the to the whiteboard. By the way, there are some links, usually there are some additional links or some additional explanations that might be important. But the the this is a list of the most important POSIX signals. I'm not sure if you can read it. Hopefully now you can. And so one thing you should know about signals is that if you have no signal handler or you don't handle it properly, a signal will terminate your application. So unhandled signals terminate your application. Think of it this way, all the default handlers have something like exit or terminate in them. Sick term is actually sent by the operating system or by other application. If somebody wants to kill you politely, if somebody just asks you to die gently and quietly, they will send you sick term. So please just drop that and we'll be, we'll be fine. There is a more violent version of that which is called sick kill, and that's actually a signal you cannot stop. You cannot have a handler for that. So if somebody sends you sick kill, you are dead immediately. This is what what's used by the tools for killing the processes. If the processes start to misbehave or become deadlock or something, then you can send them sick kills to terminate misbehaving processes. Yeah, you need, of course, you need sufficient privileges. So usually a root user is sending sick kills to the misbehaving processes. Another very important signal which you might have encountered, and if not, I'm sure many of you will encounter it during your 6th home assignment, is 6F6F is shorthand for segmentation violation and basically this has happened if you perform invalid memory access. If you're touching memory, you shouldn't be touching 111. Quick example what you can do if you have a. You have a new pointer and you try to dereference it. This just blow your application away, throwing the 6F signal which will ultimately result in ending your application. Because this is not allowed operation, you have new pointer, which means this pointer is isn't pointing anywhere in particular and you're trying to access that value. That cannot be done. No can do. Sorry Sir, your application has been terminated. It also happens if you try, if you have a rogue pointer that wanders in a part of memory which isn't allocated for instance, then the operating system says, hey, this well operating system with cooperation with memory protection, which we will cover next time, hopefully will say, hey, this memory is protected. You cannot access this memory success. By the way, there are other things. Sick child is sent to parent process when child process terminates. I've already told you that. So this is one of the signals. And for instance sick int is sent to you if you press CTRL C on your on your keyboard. So CTRL C translates also into a signal, and if this signal isn't properly handled, it kills the application. So if you learn that control C sometimes terminates your misbehaving script or something. Yeah, it does, because it sends a signal. There are also user signals. These two signals are designed for user defined purposes. So if you want to communicate among processes, you have two signals which are completely harmless. Well, they also terminate the process if there are no handlers. But but they are harmless from the perspective they have no any additional meaning. You can use them for whatever you need to do within their inter process communication. Yeah, I think so. There are only, I'm not sure 1632 signals, I don't remember, but there are only very few signals. So you can have too many of them. Oh by the way, seek I'll, yeah, this is one example I skipped accidentally. Seek I'll is when an illegal, not I'll but illegal instruction is executed. This is actually direct translation of a CPU exception, because remember if you execute an invalid OP code, invalid instruction, it throws out an exception software interrupt and this software interrupt is translated by the kernel into a signal which is translated back and sent back to the process itself. OK, unfortunately I have exactly three 2 1/2 minutes, which is enough time to say you few more things. But I have a completely new topics to start and it would be very irresponsible of me to start a new topic right away and hurry up through the introduction within two minutes. So unfortunately I will have to let these two minutes go. Are there any questions before we conclude for today? No more questions, no questions at all. OK, so thank you for coming. I hope next time not next week, remember next week our vacation, the other week are also vacation. So we will see each other in three weeks. And next time I hope just that your ranks will be replenished again and we will meet in a greater numbers. Thank you and enjoy your vacation. And remember after three, after these three-week. We will have a longer session. Yeah, I, I assume I will make a short break somewhere in the middle of this session because I don't want to keep you here too long. So we will interrupt it in the middle somewhere. But we need to prolong it slightly to fit all the data, all the materials in. So thank you for coming and see you in three weeks.