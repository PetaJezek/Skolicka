Let's get started. Last week we ended about here. So I give you a brief introduction into the operating systems and then we end up in this example when I showed you that actually many functions of a very simple application are performed are realized via operating system calls, via sys calls. Today we move on to another sub topic, which would be the architecture of the kernels of operating systems. So basically an operating system is a software like any other. Well, maybe not like any other because it needs needs to be written more delicately and and has many functions. But in general, you need to, let's say, follow some regular software engineering design, software engineering engineering patterns. On the other hand, it's much more difficult because you have some limitations. For instance, the first one you should remember from last time, your entire code is running in special mode of the CPU in the kernel mode. So every mistake you make is much more profound, much more fatal mistake, since any error you put you do in your code. Any, any potential buck in your code might lead into the crash of the entire system, not just a single application. Bugs in applications are usually handled. Let's say fatal bugs in your in your applications are handled by the operating systems by killing the applications. So the operating system is just killing the malfunctioning application, and the system the rest of the applications may run on may continue. However, bugs in operating systems tend to crush the entire system, and that's if you're lucky. If you're unlucky, it might introduce subtle additional bugs into your software, it might compromise security of your applications, etcetera. So this code is a bit tricky. There are many approaches how to design the code well in the early days, and that goes for Linux as well as Microsoft Windows as well As for some other operating systems. The main approach was to create a monolithic architecture, which means everything is part of a one huge ball of software. Of course, even here you need to do some, let's say, leveling of your functionality. For instance, that the most fundamental thing you can do is that you divide your procedures, divide your functions into at least 2 levels or multiple levels, where on the lowest level are internal procedures, internal functions, and on the highest level are service procedures. So basically these functions, these procedures directly correspond to the system course. This is what the system is offering to the other applications via some libraries, via system calls. And then there is a bunch of internal internal procedures, internal functions that are called only from these upper functions. So even here you can do some layering. Even here you can do some basic code decomposition. You may follow some good practices, by the way, like the practices you are embracing right now during the labs. OK, well, maybe not entirely. We had some minor disagreements with Mr. Sobota, but but basically what you're doing there is is the similar thing. You try to decompose your code into multiple functions. The trouble, the main trouble with the monolithic architecture is that everything is in one place. So first, it's a good thing, because if you need to call something, it's there. If a new module, if a new part, if a new functionality needs to raise something, take something from the file system or need to allocate memory internally or something, everything is there, everything is available. The trouble is that there is no or very limited extensibility of the design, and extensibility is very important for operating system. Let me give you a very trivial example. When I plugged this into the operating system, the operating system needs to load a new driver for this USB dongle, which connects me to with this with this pointer, which connects the PC with this with this pointer. So basically a new piece of software, new lines of code. Well, not lines of code, but a new piece of binary code needs to be introduced, added into the kernel at runtime. Well, actually it doesn't. It may be. It may be that the that the code to handle this piece of hardware is already in the kernel. That may be. But if if that is so, it means that the the kernel of the operating system would need to encompass all the all the drivers for all conceivable devices that I can connect to this USB. And if if that was so, the kernel would be really huge. So usually we cannot take this approach anymore because there are so many different devices that can be attached dynamically at runtime to the, to the PC, to the, to the hardware. So we need some extensibility. So only the pieces of code, only the drivers, only the the modules that are really required during the runtime, during this particle when this particular PC is running should be in there. So in the case of monolithic architecture, introducing new modules is technically possible, but at first it's quite tedious because when you have a monolithic architecture, then adding something is always breaking the monolithic architecture. And 2nd, it's very error prone because most of this driver, most of these drivers, drivers for various USB devices are not written by the vendor of the operating system. Microsoft isn't designing all these, all these pieces of code because it would be impossible. There are so many vendors of hardware. So usually the vendor of the hardware is creating this piece of software. So we are basically allowing someone else, a third party, to introduce a code into our kernel. And this kernel, as I said before, is a very delicate piece of software. It's the most delicate piece of software you are running on your PC. So we need to keep this software safe. We need to keep it free of bugs. And then again, we need to keep it extensible. So we need to allow others to introduce pieces of software into our kernel. So that's these two, these two assumptions, these two requirements are somehow going one against the other. Yeah, so there were, there were ways and design attempts how to avoid this monolithic architecture, at least how to separate some some of these concerns because it's very, it's very there is no information hiding a a piece of software driver can potentially access any part of the operating systems, even the parts it doesn't require for this. For instance, this is emulating a keyboard. This tries to create an appearance. Is a keyboard actually keyboard with only two keys? Granted, but keyboard nonetheless. And keyboard definitely does not have to access the file system. For instance, it doesn't have to allocate memory, it only needs to report when a key is pressed and that's it. But technically, if I have a monolithic architecture, the software that drives that controls this stuff might technically access my hard drive or it might send something over the network. And that would be, let's say, unfortunate. If it's intentional, it would be a very, very serious security breach. So some other attempts were made. This is perhaps the oldest. The idea is OK, we know that we have some devices, some some pieces of code that doesn't need to run the very core of the kernel. The most important, actually this is the most important part. The part that controls the CPU, the part that schedules the which code is currently running, the part that handles low level hardware stuff like interrupt. We will get to them. So this is the most important part. And theoretically, this part can be and should be written only by the the vendors of the operating system preferably. So let's keep this part separated from the others and build everything else on top of this part because this is like an essential part which everybody requires. The next step would be to introduce memory management because when we can manage code, when we can manage the CPU, then we can manage, then we need to manage the memory, the allocation of memory, etcetera. Actually, there is more than meets the eye here because you don't, some of you perhaps don't know yet about the memory protection. So there is another mechanism called paging and memory protection and we will get to that next week or the other week, I'm not sure. So we will get to that. But this is much more difficult than it appears. This doesn't only involve the allocation algorithms, it involves many, many more things. If we have the memory management, then the next step would be to create another layer which would handle the input and output. So handle things like hard drives, handle things like keyboards, mice, etcetera. And on top of that, we need some more user friend interfaces for things, namely for the files for the hard drive. You don't want to access your hard drive in a way that you would say, hey, I want to read that block or I want to write that block. That would be very tedious, that would be impossible to manage. So we need another abstraction, namely for the file system and namely for the hard drive. So the hard drive isn't operated in the terms of blocks, but it's operated in the terms of files. So the file system introduced the the abstraction of files and directories, etcetera and connects this to the IO and to the memory management. So the files can be properly loaded and stored from and two hard drives, they can be cached in, in memory, etcetera. And on top of that there is another layer, the final layer, which forms the user interface. And by user interface I don't mean only the interface like buttons and windows etc. Mainly I mean interface for the programmers. So this is like an application interface interface for other pieces that doesn't run in the kernel, that runs outside of the kernel external applications. By the way, if you run Windows or Linux, most of the applications you consider part of the operating systems aren't part of the operating systems for the management of Windows. In Windows, yeah, I know that is the name of the operating system and term from the UI. So these two things are the graphical things in Windows aren't embedded entirely in kernel, they are separated libraries in user space that you can use to make your life more convenient. OK, so the idea was to create one layer up on the next level and theoretically this could work. And this approach is somehow now incorporated in current operating systems. Not entirely. The original idea would be that each of these layers would run with a separate privilege mode of the CPU. So the CPU would have multiple privilege modes, not not only the kernel mode and the user mode, but multiple levels and and each of these levels would need a special special switch. So for instance, the kernel could kill any of the parts of the any of the modules that would handle memory or input or cetera, because the kernel would would run in a more privileged mode than the others. And this this idea still has some footprints in the real hardware. Do you remember last week I told you about the rings in the Intel CPU design that there are ring zero and ring 3? Ring 0 is the kernel mode and ring 3 is the user mode, right? So these rings were inspired by this idea. The original idea would be that the ring 0 would be for the kernel, ring one would be for the memory management or IO management, Ring 2 would be for external drivers for 3rd party drivers, and ring 3 would be for for user applications. So each level would have its own hardware level of protection. In the end it didn't work out because when the when Intel designed this, the monolithic architecture was dominating architecture. And basically it still is. If you take a look at Linux kernel, yeah, they're trying to do their best to redesign some parts and to to fix this problem, but it's still basically monolithic architecture. So this is one way to go. Another way, another idea was completely different. Actually it came from somewhat from networking, because in networking you usually have the smart network, the Internet for instance, and then you have servers and you have applications, you have services and clients, you have web applications and browsers, etcetera. You have a mail server and mail client, right? So this idea was adopted into the operating systems. The basically it basically says let's keep the kernel as small as possible. Only the most essential parts will be really in the kernel, meaning they will really run in the kernel mode by the CPU and everything that doesn't have to be there will be exported as a service. So basically we will have services which are part of the operating system, but they will run in user mode as the regular applications. So in the end, there will be a very little difference between system services and applications. By the way, if you if you list all your processes in your Linux or Windows or whatever you're using, you probably notice that there are much more. There is much more. There are much more applications, much more procedures and much more programs. Sorry, OK, I cannot do that here. I just want to show you the hey, go away, I want to show you the process list on this PC, but I don't have the administrative rights right now. So you can you can imagine that there are many services running your windows, for instance, as well. So the idea is, is viable and it's, it's still being adopted. But technically the, the services that are running on your windows are not the the system services. They are like additional management services that you also require for smoothly running all your software to give you more comfort, but they are not required. Most of them are not required for the kernel itself, but these services would really perform the kernel job like memory allocation. Memory allocation wouldn't be part of the kernel. There will be a service and you can ask the service for another block of memory for instance, and it will give it to you if some memory is available. And the kernel is mainly responsible for handling the communication. So there is a mechanism where application can communicate with services. All the services can communicate one with the other using some very quick and very efficient protocol. The trouble is that if you defer too much into these services, this communication overhead would be very tedious, very bulky, because technically for every operation between two services or between application and a service needs to be handled via communication. Normally in regular application when application needs something, it calls the kernel. So this core is inevitable when application requires something. But normally in in monolithic org, not only monolithic, but in current kernel. So you usually stay in the kernel mode until the the request from the application is serviced until it's done. But here you just emerge back from the kernel mode passing the message to a service. When this first service realized it cannot handle this request alone. Imagine that this is a request for reading a block from a file, reading some data from a file. So yeah, first service say OK, I'm handling the files, yeah that's right. But I cannot handle it right now because I have no no more memory to to for buffer to buffer the data. So first I need to ask the memory service to give me some memory so I can properly buffer some data. And each of these system calls needs to re emerge back into the user space, trigger another application. So it might cause many system calls because each communication is 1 system call which might produce a undesired overhead not mentioning the communication. Sometimes communicating via small messages may be very efficient if you need to just trigger something or do something very simple. Sometimes it might be very tedious especially if you want to load huge amount of data. And these communication channels only allow to efficiently pass small, small messages. But then again, sorry, this architecture was very popular 20-30 years ago, but it was never really used in wide in widely adopted operating system. It was used in experimental system that there was a like a ton of experimental operating systems written 20 or 30 years ago on this architecture. By the way, if you heard about, I'm not sure if we have it. No, I don't have the name here. An OS which was designed at our faculty by a colleague of mine. That's also an experimental operating system which runs which is built on this idea of micro kernel architecture. Then again, some ideas were adopted to to real world applications, real world operating systems for instance. The internal kernel design follows this paradigm. It may be shocking to some of you that Windows is here is in this terms a bit way ahead when compared to Linux for instance, but that is so. Then again, Linux is trying to adopt this idea by offloading some parts of the kernel. The best example would be the file system in user space. So basically the file system is one of the most actually the least important things from the terms, from the perspective of protection. It really doesn't have to run in kernel mode. So what they do is they they defer all the functions that are necessary to handle file system operations like decoding the internal file system structures, allocating the proper blocks, formatting the data format, formatting the files, meaning properly adjusting the data from the files into the into some blocks, etcetera. So these operation operations are very simple and most of them does not require any hardware support, does not require any interactions with other services. So basically they they deferred this file system to the special application which runs in in user space like a regular application. And every time some other application requires to access the file system, it actually asks the service for. Loading the data or a file or something. So in some cases this is feasible, like in file systems. For some other things like allocating the memory or scheduling processes, it might not be completely feasible. Or it might be feasible but not very practical. The main problem of these systems were that they were quite slow when compared to regular monolithic architectures or so. To give you glimpses of real software, real design, this is actually a map of Linux kernel. Yeah, it's a mess, but then again, all the operating systems are that complex. But you can see that they did some pretty good job in trying to separate some concerns. Even though this is a monolithic architecture, they at least try to divide it into some modules into some blocks that are not entirely independent unfortunately. But at least they are somewhat independent. The layout is 2 dimensional, so the vertical parts refer to different areas of concern. The low level system, the processes, memory management, storage management, network management, human interface, that's pretty obvious. So this is somewhat similar to the well actually it's not. I would be lying to the the vertical is slightly more similar to the to the to the nested approach. And on the vertical scale, the bottom is formed with the hardware interfaces. So this is the most low level stuff you can write. This is the really the the handling of IO, handling of the CPU, handling of the memory allocation, etcetera. And as you go up, upwards, it creates a better and better layers of abstraction. This is like the most important level because it bridges the low level abstractions and the high level virtual abstractions. Technically speaking, communicating between these modules, each of these squares can be perceived as a module almost. So communicating vertically is fine. So if you need a memory that there is some top level memory abstraction at the user space interface, so it communicates downwards to allocate the memory and then report the results backwards upwards, that's fine. If you communicate this way, that's perfect. Communicating horizontally isn't that perfect because each of these domains should be as separate as possible. So every time you see a cross line or a vertical line, sorry horizontal line, that's a not very good line. It's like not entirely a flaw in the design. Sometimes these interconnections are inevitable. However, every time there is a cross section crossing line on the horizontal axis, it indicates a potential problem. Are listed in physical memory operations are die. I'm sorry I don't follow. It's possible because the, the indicators here, you can, if you can read them, actually are function names. So yeah, some, some of the functions needs to be buried very low because they need to communicate with the CPU directly or something. They they basically trigger a specific instruction or an exception or something. So yeah, don't be scared by by awkward naming and awkward positions of these functions. It's a very complicated architecture. No, no, no. It's not like a stuff it does. It's like a functions that it offers. So if something goes wrong, you need to function to die or something. Yeah, but it doesn't mean that you're there. That doesn't mean that this function needs to be always executed or something. OK By the way, some of these square blocks, some of these modules cannot be even fit properly. For instance, there is a page cache and swap which is responsible for page management and swapping, so offloading the pages from memory into the hard drive and vice versa. And as you can see, this rectangle is stretched over memory management and storage management because it needs to perform both. It's very tight, it's very tightly coupled module which needs to step into both domains and it cannot be avoided easily. Actually, yeah, it can be split into two modules like a swap memory management and swap storage management. But then again, the communication between these two modules would be so intensive. It would it could be impracticals or it they just didn't know how to split it properly because of the spaghetti design of this code. Both both are possible. For comparison, this is a structure or diagram of Windows kernel. You may, yeah. It is based somewhat on the micro architecture. So yeah, at the very bottom there is something we just call hardware abstraction layer. So Windows internally performs some level of abstraction over the hardware, which is very important. This is actually embedded in the Linux as well. But in the Linux it's divided into all the sub areas and there is a micro kernel. So there is some communication level which is the most important part, one of the most important parts of this of this kernel because it handles communications of the all other executive units of the of the of the kernel like I/O, IPC, scheduling, memory management, etcetera. And by the way, everything you see in Windows, like the like the Windows that appear, the buttons, the file system itself. Some people complain that that Windows still uses a case insensitive file system naming in some cases and uses things like dot exa to recognise whether a file is executable file or not. Yeah, that's so. But actually that's not the the part of the kernel. The kernel itself is quite cool. This is the. The trouble lies in the Win 32 library, which handles everything else. So the system itself doesn't know anything about file naming. The the operating system can run also POSIX, it can run any Linux without a problem. It can run OS2 without a problem. The kernel itself. The trouble is with the Win 32 library, which takes many, many burdens along the way from the older systems like Windows 1233.13.11, ETC. By the way, this is the kernel which was used in Windows NT and then later in Windows 2000 and Windows XP and ever since. It is an old kernel, but it was well designed 25 years ago, or maybe more so even though it's old. Yeah, of course it has made some progress over the years, but technically it was quite well designed from the beginning. I'm not counting Windows 95 or Windows 98. They they were built on a different kernel. They were basically an extensions of MS-DOS. They they weren't full blown operating system. The kernel were completely different. So this was just to give you some ideas about the architecture inside. Yeah, the architecture itself is very complicated. So in the first year, it would be, I would say, irresponsible of us if we just taught you everything we know about kernels. That's, that's impossible. But then again, there will be, there is another course called operating systems. So if you're interested in that, you can visit that course. And actually you will try to write your very small kernel for a very specific hardware. So it's, it's fun. Actually, I've done it and it's not that bad if you learn CN and start start coding at that level. So another topic now we we would like to cover, I would like to cover this, this lecture would be devices. Devices is one of the essential parts. Device handling is one of the essential parts of operating system and operating system responsibilities. So let's start with a, with a terminology. By device, I mean any peripheral, anything that has a specific purpose like a keyboard or a mouse or a or a monitor or a hard drive. So all these are devices attached to a single system. Yeah, technically there are many devices embedded in the system. For instance, if you just take the motherboard, it looks like 1 device. But actually motherboard is like besides it's a hub which interconnects the the CPU with the memories and with the with extension cards. It also has some integrated devices on it. Usually it has integrated network adapter, which is also a device. It has a sound card integrated which is also a device. So yeah, sometimes appearance appearances can be deceiving, but I will try to keep the example simple. So from now on a device would be usually something you can plug into a computer via things like USB or via SATA cable or something. Each device needs something which is called device controller. So it needs a counterpart which connects the device on the hardware level. For instance, this device requires a USB hub or USB controller that interconnects it on the wire level, on the electrical level with the computer itself. If you connect a hard drive, you need a SATA controller or M2 controller that will communicate with the hard drive on the hardware level, on the electrical level and send some signals on the wires, etcetera. And on the other hand, on the other side, the device controller provides us with a uniform, uniform interface for all the devices of the same kind. So for instance, if I have a SATA controller, SATA device controller, it can connect any SATA device. I don't mind whether this is an SSD device, whether it's a it's a magnetic hard drive. I don't mind whether it was made by Seagate or or Western Digital or whoever now builds hard drives that that that beyond the point I know that whatever hard drive is plugged there I can access via this device controller using some form of some uniform protocol, some some well defined abstraction. And this abstraction itself isn't enough because even among device controllers, I may have multiple device controls. I may have a SATA driver from Intel, I may have a SATA driver from other vendors that cooperate with AAMDI, may have driver made by MSI or whoever now builds the motherboards. So yeah, there may be multiple vendors of the devices and each of them might have slightly different abstraction layer. Yeah, they usually have the same the same common parts. Like when I'm communicating with hard drives, I would usually have things like I need to list all the drive drives attached to this controller. I need to send a block or read a block from this particular drive. So yeah, they will be very similar, but then again, they might be there might be minor differences. Sometimes I may have a SATA drive, a SATA controller that supports rate grouping the disks together virtually, and some SATA controllers doesn't support that. So I need to know how to communicate properly with this particular SATA controller, and that's where the device driver steps in. Device driver is a piece of software, a module, a piece of code that allows us to interconnect this particular device controller on the software level so that the operating system knows how to communicate with this device controller. And again, it's another bridge on the layers of abstraction. It's a software bridge, so it creates the necessary adaptation from a universally well known layer for all the hard drives. So each operating system specifies whoever writes, whoever creates this device. Controls for hard drives need to implement this set of functions. OK, if you want to sell, if you want to attach your hard drives to our windows, you need to implement this set of functions. That's it. And the device driver is responsible for implementing all these functions and then send appropriate signals, translate these functions calls into the appropriate signals into the appropriate communication protocol with this particular device controller. So usually the vendor of the device controller is the one who is responsible for writing the device driver. Yeah, on some levels, some devices are so easy, keyboards, mice, that you don't need an explicit driver from your vendor. Yeah, that that may not be true. If you, if you have a regular keyboard with regular, I don't know, 102 keys, whatever, then you probably don't because it's very generic, generic piece of hardware. It's connected via generic USB. So you don't need a specific driver for this particular keyboard. If you just use a generic driver which expects some generic things from the keyboard, it will work fine for this particular keyboard. However, if you buy a very specific gaming mouse that have not two or three, but 10 buttons or whatever, and these buttons are configurable or, and you can, you can set which color the the mouse glows or whether it vibrates when you get hit or something. I'm just making this stuff up because I'm not that, that much of A gamer. So if you have a very specific device, then yeah, you need to have a specific driver for that that particular device. Having devices creates modular devices and device drivers actually creates the problem of chicken and egg, right? Because let me give you an example. You have a very specific device controller to handle hard drives. So we have a device controller that allows you to communicate with your hard drive. So you need to load the device driver for this controller. Guess where this device driver should be stored? When your PC is off, right? That's right, on the hard drive. So how the hell I will load from the hard drive the code that runs the hard drive? Yeah, that's typical, right? So for that purpose there are some generic interfaces like BIOS or modern UFI and these are like well known interfaces that allows you to some basic communication with the most fundamental hardware like the hard drives. So this is used usually during the boot because that's where this problem arises the most. So you during the boot you can communicate or the operating system can communicate with this UFI to identify all the devices to to get some basic inter operations with these devices. So to get like a vendor string or some identification from the devices with the hard drive, it can perform some low level, albeit slow communication. So it can at least load some data from the hard drive, it can load the boot loader from the hard drive, it can load the most fundamental part of the kernel and it can load the driver for the hard drive. When it initialize all this, then it can drop the UFI and then it can communicate with the driver directly using the device driver that was just loaded. OK, so there is a mechanism how to avoid this chicken and egg problem. So we can slowly and very clumsily load data from hard drives using BIOS or UFI so that we get to the code that will finally run the the device efficiently and and in a much, much elaborate manner. By the way, the interaction between the device controllers and devices might be quite difficult or elaborate. Normally, or what would you probably expect is that the device controller has a direct link to all the devices attached to it. Well, in many cases there is one device controller and one device attached to it. But then again, if you have a more elaborate PC rig, then you probably have multiple hard drives, right? I have two hard drives in my laptop, but that's because I take my old hard drive from my previous laptop, so now I get 2. But then again, if you have like a big case, you can have easily 3 or 4 hard drives in your case. So there is one SATA controller which is connected to using multiple wires, small wires to all the hard drives you have in your rig. That's the easy part. In all days when we have we didn't use SATA or M2. Have you heard of Pata? Pata it it stands for parallel ATA. The ATA is for communicating with the hard drive. So this was parallel ATA. This is serial ATA by the way, just a small the diversion SATA was devised because communicating via serial cable is faster than communicating via parallel cable, because if you have a serial cable, you need only to synchronize 2 wires. If you have parallel cable where there are 40 or 80 wires running in parallel, it's much more complex for synchronization and it much easier. Creates some noise on the lines. I need to flue that up exactly. I don't know, don't remember from from the top of my head, but you can Google it up easy, easy enough. So the parallel part was used to create a very bulky piece of cable. The cable was thin and wide because you need to place 40 or 80 lines in parallel. And it was also easily damaged because you just wrap it a few times and then you can throw it away because one of these 80 lines was broken or or interrupted or something. And that was very tedious. So This is why SATA is better than than Pata. But then again, once you got this cable, it was very difficult to have multiple cables in your case because you cannot possibly have like 8 Pata cables in your case. There was no room for that because these cables are very bulky. So what they did and also the design of device controllers was very expensive back then because each device controller was basically a separate chip. So what they did is they allowed you to connect 2. Two disks on one cable. So the cable basically had three connectors which were all connected to all 40 or 80 pins. And these connectors, one was plugged into the device controller and the other two were plugged to two disks or to a disk and a CD-ROM. Yeah, you probably don't remember what CD-ROM is, right? Because nobody is using cd-roms any longer. So this way, 2 devices were connected over the same bus. By the way, this is also the topology that we used in a very old setup of in networking. So if you had, if you ever heard of coaxial cable, T10 network, Ethernet T10, it uses one huge cable which was running throughout the whole classroom and all PCs were connected to this one particular cable. So they were all communicated over the single, single bus. Another possibility is a ring. This is quite of a awkward topology because in this case the controller has one output and one input, and each device has one output and one input. So basically when any communication needs to happen, the DC sends a frame for communication, which is transferred to device one, then to device 2, then to device 3, and then it goes back to the DC. And each device is interacting with this frame in a manner whether it's designed for this device or not, whether it's allocated, whether it's addressed for this device or not. So basically the DC specifies in the frame to which devices this frame belongs to. And the devices that don't shouldn't interact with this frame, just pass it along. Yeah, it's very awkward, but then again it increases throughput. In bus architecture you have the trouble that if the first device is communicating with the DC, then no other device can communicate with the DC or even these two devices cannot communicate one with another. Well, well, normally this is not what happens. But in networking that might happen in normal, in in disk design it wouldn't happen. So when you communicating with the first disk, the second disk remains idle. You cannot communicate with the second disk at the same time and vice versa. Here you can technically communicate with both, but you need to somehow properly order the right commands and properly read the responses, because when you send a command it takes a few ticks before the response arrives. So it's much more complicated, but it increases the throughput. This is the setup we are using now, which is much more efficient because each device could communicate independently. That's why they switch to this topology. And finally, there are also hierarchical topologies like 3. This is typical for USB. In USB you have one controller with multiple ports and you can plug a hub into a port. Then you plug multiple devices into this hub. So there's there can be some topology. By the way, I remember that there, there used to be this huge fair computer fair called Invex in Brno. I'm not sure if you heard about Brno. That's some place where the usually it's for witness protection program. If you need to disappear, you go to Brno. So that the there was this this affair and first time USB came started to be a normal part of CPUs. They created an experiment and they connected like 120 devices to the same PC. They had like 120 mice there and they, I don't know how many hubs they used, but they all connected these mice into the same PC. Yeah, it was completely useless because you couldn't use any of these mice because just by blowing your nose, you probably moved some of these mice so the cursor was jiggling over the screen all the time. But they just prove it's possible. Technically, OK, so this is about how devices are connected and now how we can communicate with the devices. That's probably the most part because well, we are not teaching about the wiring and about the buses on the hardware level. That's not a part of this course. But we need to to explain how this stuff is operated from the software perspective. That's our business in all days. For communicating with any peripheral devices, the CPUs had specific instructions. For instance, ASICS architecture used this in and out instructions that were used to send or receive data to and from devices. Each device used something which is called a port IO port, usually a device that multiple these ports because one port was for instance used to sending and receiving commands and another port was to come to send and receive data or additional ports were used to indicate errors etcetera. And that was indicated by DX register. So you place the number of the port into DX and you use AL register to send or receive. By the way, how much data we can send or receive by each instruction? What's the size of AL register? It's one byte. Yeah, that's that instruction. Receive or sends one byte of information. OK, you do the math. How many bytes can I process using this instruction when I need to read or write the hard drive? Assume that you have A1 gigahertz CPU and this instruction takes 11 clock cycle. Yeah, not that much, right. So this is very tedious and very problematic. By the way, DX is has 16 bits, 2 bytes. So you can use at most 65,000 ports, not too much as well. But then again, you don't don't need that much port because usually normal devices use only a few ports to operate and you don't have like thousands of devices usually connected to your PC. So that that's this part is fine. The second possibility is to use memory mapped devices. So all the communication is taking place using regular IO like load store operations or move operations in architecture. So you're basically reading or writing in memory how this works. The hardware when it when it boots, the hardware maps its control structures to a specific address in the memory. So the hardware when it boots through UFI or through BIOS, you find out that particular device, like the controller of the hard drive is mapped at this fixed address. So you somehow determine this address and then you have a specification for this device controller. So you know how this structure looks like in memory. You know that the 1st 32 bits are the control word, the command word or something. Then there is an error word or something. So you have a structure, you have the layout how this structure looks in the memory, and every operation, every read and every write triggers some hardware action. So instead of storing or loading data to and from memory, it stores and loads or sends or receives commands from this particular device. Basically, when you write something to the specific location in memory, it's the same as if you sent a explicit command to the device, and vice versa. When you read something from the memory, then you perform the loading operation, the fetch the input operation. With this particular device, the benefit is obvious. You can choose or the device can choose its own width for communication. So basically you don't have to rely on communicating by sending one or receiving one byte. You can write the whole structure. Furthermore, this structure may have some additional space, for instance for data buffers. Let me give you an example. You're communicating with a network device. You probably remember that when you use network, there are some IP packets, IP data grams, right? They have maximum size, right? So it's easy to allocate this maximum size as a buffer somewhere in the memory. And automatically the device not only uses the memory to communicate, to operate the device, but it also gives you a glimpse of its own buffer into your memory. So if you need to send or receive a packet, an IP datagram, you just read it or write it from this particular address in the memory. So This is why today all the devices basically use the memory mapping approach. All right, let me give you an example how the handling with the with the device. Some communication with the device is performed in a few, let's say well defined steps. In the beginning, some application issues an IO request. To be more specific, to be more concrete, let's just say that you have something like. You are reading some data from the disk. This is the file point handler you just opened. And then you have a buffer here and and how much bytes you want to read. Something like that. This is a pure C function which is implemented in in libc. So this is some IO library you are using fine. This function calls. A POSX function read which performs the same task but on the system level. This translates directly into the system call into the syscall. So here we are entering to the kernel mode using this operation. Afterwards here the kernel performs some checking. For example, it checks whether this file handler which needs to be passed over here is valid file handler, whether the reader, whether the application has the privileges to read this file, whether the file exists, etcetera, etcetera. But in the end, at some point, assuming all the checks were OK, it transfers the request to read a specific data buffer from the device. So it asks the device driver to transfer some data buffer from explicit address on the hard drive. And yeah, addressing on the hard drive is a bit tricky. I haven't explained that yet, but imagine that the hard drive also have some addressing mechanism to identifying particular blocks. So it asks for a particular block or even multiple blocks from this device driver. The device driver then actually now we are crossing the threshold from the software to hardware. So the device driver is initiated from the software using some memory, memory mapping communication and then the device driver itself initiates the communication on the wires on the hardware level with the device controller. So basically we are like sending signals to the device controller. The device controller is communicating with the device. That's completely hardware business. And actually as I said, it's none of our business in this class. So there is some communication over the wires and then the device controller needs to somehow report back to the CPU, reports back to the software that something is happen, something has happened. This is the most tricky part this this is the most difficult part because we need somehow to trigger an impulse to the CPU. We need somehow to notify the CPU that the loading of the block is is completed. Technically speaking, the device driver can wait for this, can loop infinitely, waiting when something happens, reading some specific data in memory, some some some memory location, waiting when when it's changes its state to signal that that the data block has been loaded into the memory, and then it reports this back to the regular kernel code. It's possible that reading one block isn't enough, because technically I just cheated. I assume that from this you can immediately infer which data block we are reading. That may not be true. We need somehow to translate first where the actual data for this particular file lies on the disk. And that's the business of the file system module, right? So actually there will be probably much more communication going on. st we need to load the the directory structure, then we need to load some index structure to find out where the actual blocks of this particle file are and then finally we can load the block of the file itself. So presumingly this repeats multiple times to handle all the internal sub requests of the kernel. And finally when all the data are in some memory buffer then the kernel can report back. Sorry, sorry. Can report back to the IO library which gives you the data which returns from this call. And I have my data filled in my buffer. Yeah, I do realize that this was a very top level overview of what's going on. But it's important that you realize that any communication, especially with IO is hierarchical and it takes, it takes many steps to get there. And also it may internally perform multiple independent steps and you don't even know about it. So each F read perhaps performs several accesses to the disk, disk, sometimes even disks. It's possible if you have rate for instance, so that your request is satisfied. As an example I will show you how the most low level handling of IO with a hard drive works. This is a very old example which is using a PAIO. Paio is the oldest interface for hard drive I know and it still uses the old input output instruction. So this one doesn't use memory mapping. It really explicitly sends commands in and out, sends instructions in and out to send bytes to and read bytes from the device. And how this works? I said when I talk about the, when I introduce these two instructions, I told you that they have a DX which is the number of the port. So we need the numbers of the port to fill into the DX when we are communicating. And these numbers actually are different from this table. These are offsets. So I may have multiple Pi O devices and for each of these devices I will need a base port address. So it's, it's something like during the boot, I just query the BIOS and the BIOS tells me, hey, there are 2Ï€ O devices. For instance, you have two hard drives, 2 device controllers and first of them is at the address 80 and the second is on the address 90. OK, no problem, why not? So I know that the address 80 + 0 is the data register data port for the first device, 81 would be the error register and 87 would be the status and the command register for instance. Easy enough. OK, so this is the algorithm. How can I read a sector simplified algorithm? I skipped some other parts which are not that important to explain this example. So first of all we always need to check the status of the device. So first I read this register, I read byte from this port and if this byte signals that the device is idle and fine then I can proceed. Otherwise I need to read the error register and find out what caused the last error of the device and how can I reset it or something if necessary. So let's assume the device is fine and I can commence my reading. First I need to set the count register, how many sectors I will read and the address registers or the address ports. Actually the address splits over multiple multiple numbers. There is a sector number, cylinder number which is actually 16 bit wide, so it's divided into two ports and drive at registers. So these 4 registers, these 4 numbers, these 4 bytes actually together from the index of the sector I'm trying to read or the the first sector of the block I'm trying to read. And then once this, once this is ready, I need to read the status register because sorry, I need to send first, I need to send the command, the command 20 initiates the read. If you don't know that, that's fine. You can find this on Wikipedia. This example is taken from Wikipedia. So there is a table list of all commands I can send to this device. So I write 20 to the status command register. And then I wait and wait. I'm waiting. I'm actively waiting. I'm actively reading the status register. I'm reading this register, this port until it signals it changes state. It gives me a specific byte that tells me that the data are ready, that the reading has finished. OK, usually it's a good idea not to read it eagerly, like in every loop, because it takes some time to read the data from the hard drive. So usually I can take an app, the CPU can take an app, or it can perform this poll and then for 1000 or several 1000 instructions perform something else switch for another thread for instance. So here I'm reading until it completes or it fails. If it fails, then I need to read the error register to find out why it fails. But if it succeeds, then I know that the data are ready in the internal buffer of the device controller with whom I'm communicating right now. So then I need to read the data register, in this case 256 times because this specific IO operates in blocks of this size. And yeah, if I read multiple sectors, I need to read it multiple multiple times. But usually I just read 256 bytes and then check the check the, check the status register again and then I can read another 256 bytes. So I repeat these two operations until I read everything I want it to read. Yeah, just imagine, just count how many instructions this would require. This would require quite a lot of instructions. And then again, I'm reading the data byte per byte. It's very slow and very tedious. But then again, this design was this interface was designed 30-40 years ago or something. So back then, this was the only way how to read data from hard drives. OK, obviously this is not the idea way how to do this. One of the besides the bad communications, besides that I need to access the device byte by byte. There is another problem which is called. I need to perform pulling. I need to repeatedly ask the device whether it has completed. Have you seen the the film Shrek 2? I think it was, Yeah. So if you're traveling to a Kingdom far, far away, pulling it means that you're repeatedly asking, are we there yet? Are we there yet? Are we there yet? So you need to repeat this procedure until someone tells you either yes or either we won't reach the destination ever. So until then, you're trying to ask the question again and again and again. Obviously it wastes resources because if you need to repeatedly read something, if you need to repeat it, pull something. It takes some resources, it takes time. You are actively consuming the CPU cycles. So better ways for device how to notify the CPU that the job is done and that way is the interrupt. So basically interrupt is a mechanism that allows to notify other devices, external devices that something has happened. So the device doesn't have to check all the devices all the time using pulling, but it just waits. And if a device wants something or the device wants to notify the CPU that an operation has finished, like the loading of a block is completed, then it triggers an interrupt. And yeah, the interrupt is really performed on the hardware level. So the CPU had less at least one pin, one electrical pin wired to something that can be used to trigger a electrical pulse that triggers internally the interrupt in the CPU. OK, technically speaking if we have multiple devices, we would require multiple pins because these devices may trigger independently the interrupt. But then again it would be very tedious. So what we are doing is there is a IRQ controller interrupt request controller specific chip which is like a connecting all the devices that want to perform interrupts and aggregate all the interrupts together before it notifies the CPU. So in in the end the CPU requires only one pin for interrupting and then it requires some communication protocol with the IRQ controller. So when an interrupt happens, it just asks the IRQ hey what was the interrupt and the IRQ will tell the interrupt comes from that guy. So there is some aggregation performed, usually in front of the CPU, because it would be quite impractical if the CPU would have to have so many pins just for handling many devices. For interrupts by interrupt I will explain in short order in more detail. By by interrupt, I mean the current flow of instructions of the CPU core is interrupted, so it's suspended temporarily and another code is placed is executed there. That should handle the interrupt OK. So it's like an immediate termination or suspension of the actual code which is running and temporarily replacing this code with the handler. That should do something to correctly answer the device or correctly handle whatever the device needs. By the way, there is another thing that you need to know about device communication right away, and that's DMA. Since many devices need to communicate with memory, need to load or store data from or to the memory, then this special case is handled separately using the DMA controllers. Let me give you an example. Even if I have the interrupt, it will still be very tedious if I need to read every data block from the hard drive independently. Even if I replace this step, yeah, this one reading the status register which is pulling, even if I replace that with interrupt. So I just instead of Step 5, I just ignore everything, ignore this device until this device interrupts and then I will continue with the step 6. And yeah, putting aside that this step 6 is quite tedious, but even then I would always get only 25256 sorry bytes or another block size. So I will always get a very small portion of the data and then I need to restart another load. So it wouldn't be very much useful. Then again, I'm usually transferring megabytes or sometimes even larger amounts of memory from or to hard drive. So the DMA is a more elaborate controller that allows me to transfer huge blocks, huge chunks of data from a device to the memory or vice versa, without any attention of the CPU. So this is going completely transparently, and the CPU is not notified by the device itself, it's notified by the DMA controller. That's some bulky huge transfer that was taking the place on the background has terminated and has concluded. By the way, modern DMA controllers are much more sophisticated because they not only transfer a continuous chunks of blocks, they can perform operations like scatter or gather. Scatter and gather operations are operations where the data are somehow first together. Is that the data needs to be first assembled from multiple places in the memory before they are written in one compact block. And scatter is the other way around, that some compact block is put into many separated buffers or depending which direction you are looking at. The scatter gather is particularly useful in cases like network transfers. You probably remember from networking then if you when you're transferring a stream of data, do you remember anything about TCP protocol? I believe, yeah, I believe you got discovered in the basic networking course. So in you're sending over a stream of data, right? You have these buffers, you are sending over huge bulky buffers of data, but the TCP needs to chop this data into smaller packets because one TCP packet needs to be only like well in a regular Ethernet it should be like 1 1/2 kilobytes or something like that. I don't remember the exact numbers, but something like a KB. So it's split into these small chunks and each of these chunks needs to be put into a IP data gram TCP packet, sorry, which has a header. And then again this TCP packet needs to be put into IP edit datagram which has a header. So you need to put a small amount of data in front of every chunk here, which is very impractical because normally you would need to copy this into a separate buffer and then copy this into a separate buffer. So many copying, but with scatter, sorry, with getter operation, you can specify a separate buffer here for TCP header, a separate header over here for IP header. And then when you're performing the DMA, you just say, hey, take this and this and this, put it together and send it over there, right? And the hardware does this automatically. You don't have to bother with the details. So this is like why it's being useful. Similar, it might operate when you're loading data from hard drive and data on hard drive. One file on a hard drive may be located on multiple locations. You have a chunk over here, then there is a block which doesn't belong to this particular file. Then there is another chunk over here and you want to load all these chunks. So you just put the addresses of all the chunks and one buffer in the memory and it will be loaded together. Together operation will take care of that. That good point. Yeah, technically it might. If this device isn't working properly, then it might be a very, very big security issue. But then again, you are trusting the hardware. So the DMA controller isn't something which is arbitrarily connectable via USB. It's not like that I'm connecting another DMA controller over USB. This is part of the main motherboard. This is part of the system agent basically. So yeah, we are trusting that this piece of hardware is doing its job properly. And of course, if the operating system screws up and gives wrong addresses to the DMA controller, then yeah, the DMA controller will overwrite memory, which it shouldn't over overridden. But then again, it's operating systems fault. So yeah, if you have box in your code, your software won't run properly. That's that's the risk we're taking. But then again, we are in operating systems, we we really should take good care that we don't make these kind of mistakes. But technically, yeah, you're right. So this is how basic communication works. And now let's let me give you a few more details on interrupts. Actually, interrupt is a specific state or specific operation of the CPU. Interrupt is an embedded function of a CPU. By the way, your Rd. Nos are capable of interrupts as well. We are just not using them because we are we don't have the time nor the capacity in this course to cover them as well. We are focusing on something else, but technically they are there. So what is an interrupt? First you need to know that there are two basic types of interrupts. That's what I was talking about so far. So the external interrupt is triggered by a specific hardware source which just pulls the one of the pins, one of the legs of the CPU to signal that something is going on outside the CPU and the CPU should really pay attention to it. By the way, the CPU can perform something which is called masking. It's it's allowed that for some parts, for some pieces of code, the CPU will will say OK for, for this small piece of code. For this part of code, I'm not using interrupts, I'm ignoring all the interrupts. So yeah, there are pieces of code that where the the CPU may run without interruptions, but it's a tricky, tricky preposition because if you ignore the interrupts for too long, it might happen that something really important is going outside the CPU and the CPU isn't paying the attention to it. So it might break things down if you if you're not using it correctly. But then again, it may secure some some very essential pieces of code which really needs to be performed one after another one, one instruction, instruction after another. For instance, if you're handling some delicate communication with the device or if you're handling some very delicate piece of kernel code where you like rescheduling some some threats or something, then you really need to make sure that this, these few lines of code, these few instructions will run as they should without any interruptions. Then you can internally the CPU can trigger something which is called a software interrupt, which software interrupt is usually referred to as an exception. Exception is software interrupt. And these exceptions may be, might be triggered by either hardware. So you're you're running an instruction and this instruction somehow fails or something. Then it triggers internally this exception. Or you can do this deliberately. There are specific instructions like the int instruction. This is a name of an instruction. It's a shorthand for interrupt, not an integer. It's a tricky part. So this is an interrupt instruction. So you can trigger some exceptions manually explicitly to give you some ideas what the exceptions are. This is a list of X86 exceptions. Actually X86 is capable of handling 256 different interrupts. It uses 1 byte identifier for interrupt code. The 1st 32 interrupt codes. The 1st 32 interrupts are the exceptions are the software are the internal interrupts. This is lists of some of them actually. I'm not expecting you to remember them, nor I'm going through all of them. Just to give you some ideas what could be this kind of exception. Well, the first one is kind of obvious, division by zero. It speaks for itself. So if you try to divide perform the division instruction and the second operand is 0, the divisor is 0, then, well, normally the the universe could end or you can create a black hole or something. But in in a computer world, we just handle this as an error. So the CPU say, hey, I cannot, I really can't perform this, this operation because the physicist or the mathematicians or I'm not I'm not sure who would be really mad. So I'm just giving you the error. Don't do this. This is a bad instruction. Another instruction, similar one would be like overflow or invalid op code. Yeah, this is maybe this may be a better example. If you're binary of your application holds some gibberish, some some data that doesn't make sense, some instruction doesn't make sense, the CPU doesn't recognize the instruction itself. That's possible because it might be that that you are not executing code, you're executing data accidentally. Or it may be your compiler is faulty. Or it may be your compiler is fine, but it assumes 2 modern CPU and you're running your code on an older version of the CPU which doesn't have this particular instruction. Yeah, that's all possible. Then this exception is triggered. And by the way, sometimes this exception can be even useful because you can emulate newer versions of your hardware on the older hardware. Just imagine that this invalid OP code is triggered and the operating system, the handler of this exception, finds out that actually the instruction is valid, just the CPU doesn't know it because it's too old. Then it can emulate the instruction. It can somehow try to create an appearance as if the instruction was executed correctly, although it was actually handled by an exception. Now the exception, which is actually 2 exceptions which are very important, but I'm deferring the explanation for later. Are these page fault or general protection fault? These are related to memory mapping and memory protection and we will get to that later. So there, there are some internal mechanisms of, of CPU that allow you to translate virtual memory to physical virtual addresses to physical addresses in the memory and they are very important. So we will get to that in more detail. And the problems with this memory mappings with this memory handling is also handled via exceptions. I will, I will get back to that. These exceptions when we, when we're talking about memory management. Yeah, a few more things. One more thing I wanted to point out. Ah, yeah, the the tricky part is what should happened if handler of an exception triggers an exception, right? Because that sounds like a recursion without the endpoint. So normally it's not allowed for an exception handler to trigger an exception. But if it still happens, there is this special exception called double fault. And double fault is automatically triggered if you throw. If you cause any exception within an exception handler. So the CPU somehow knows. CPU remembers that you are in an exception handler, it's CPU's jobs to remember that. And if you are an exception handler and you still trigger an exception, it causes double fault, which is also an exception, but a very specific exception. Usually operating system can only kill the running application at this point, or kill the driver that caused the exception or something. So at this point the CP, the operating system is forced to take very strong measures against the code that actually caused this kind of failure. What happens if we trigger an exception in a double fault exception? Yeah, we can change this further. Well, actually only to the third level, because there is triple fault. There is a specific exception called Triple fault, but its handler is default. You cannot change that handler easily. I'm not sure even if you can change this handler at all. And if you cause triple fault, it resets the CPU right away. It says, hey, this code is so faulty that even the operating system cannot run properly. So the only course of action we can take is to shut everything down and reset the CPU altogether. OK, so these are the examples of software interrupts of exceptions. And that were the exceptions caused explicitly by accidents. There are also exceptions which are triggered implicitly. Sorry, the other way around. So far I was talking about the implicit exceptions that that are caused by troubles in software. But there are also exceptions you want to trigger manually. You want to actually 'cause them accidentally, like a single step interrupt this exception actually or a break point. Yet another great idea is break point. These exceptions are triggered when you're debugging, so that after instruction or after a break point, the CPU goes into a different mode and stops the execution of your application at certain point. Then it can return back to a different piece of your code where the debugger runs and tells you, hey, we just stop your code there, do you want to inspect it, do you want to do something else with it, etcetera. So these are handled by software interrupts, by explicit interrupts. By the way, there is one thing I haven't explained so far. Yeah, here there are two types of software interrupts. One it's called a trap and one it's called a fault. Software interrupt takes place either, well, it happens during the execution of the instructions. So the question is, what should we do with this instruction? Should we discard it, rollback and and erase everything this instruction did so far? Or should we let this instruction finish and then we perform the interrupt so the trap does? The letter trap triggers the exception after the instruction. So once the instruction concluded, the results are written, whatever the instruction performed. Then it triggers. The exception fault causes the instruction to rollback, so everything is undone as if the instruction hasn't happened yet and it's triggered before the instruction. Obviously we cannot trigger an interrupt in in between instructions. Well, technically we could. Technically it's possible to implement handling of interrupts which would interrupt in one instruction in in the middle. But it's very difficult to implement, it's almost impossible to handle correctly. So they decided, no, we are not doing that. Every interrupt is either before or after the instruction. Even the hardware interrupts are either before or after an instruction. Usually when you're doing a hardware interrupts, external interrupts, you just let the actual instruction finish and perform the interrupt right after the instruction. That's the easiest way how to do that because the rollback, the undoing or of everything the instruction did is kind of tedious. It's very complicated. So I will explain that on a breakpoint. That's a very, very good example, a very favorite instruction of mine. By the way, the software interrupt is triggered by the int instruction, so you can write something like int. In this case, it's the three which triggers the break point. Normally this translates to two bytes. This is CD and this is. This is 1 byte because 1 byte identifies the code of the interrupt. So this goes. The number of the interrupt goes here. However, this particular instruction has a shorthand which is 0 XCC. It's a single byte instruction, which is quite important because since it's one byte, we can use it to place it everywhere in the code and it can override every instruction you know, In normal code we have instructions of different lengths X ASICS architecture MIPS is fine. In MIPS all the instructions take 32 bits no problem. But in Intel instructions can be 1 byte wide or if they can take I think 13 bytes are the most are the most lengthy instructions. So if you want to replace an instruction in your existing compiled code for break points, that's what debugger does. It needs to put a some marker in the code so the break point will break on a specific instruction. So normally it would be difficult because if the breakpoint takes more than one byte, we cannot for instance override this one because we would override 2 instructions together. That's no good. So with this particular setup where the breakpoint is single byte instruction, I can easily put it here or here or here to place a breakpoint on a very specific instruction of my code. Of course the debugger needs to remember what was underneath. It needs to remember what I was replacing. So when the break point takes place, when I enter this instruction, the break point is triggered. And by the way, the break point is a is fault. So break point it's no, it's not there. But break point stops and roll backs, so it ends. It triggers the exception in front of the instruction that was break pointed. So then the debugger can flip it back, can correct it and then rerun the code. OK, so this is another way, one way how to use software interrupts. And finally, and I think this is the last thing we can manage today, is how the actual interrupt is handled. And this goes both for hardware and software interrupts, but I will explain it on a hardware interrupt because that is where it's most interesting. So imagine something happens, I stroke a keyboard letter so the keyboard sends an interrupt to the CPU. Hey, hey, I'm interrupting because I have a very important message to send. The user pressed Y. So what happens? First of all the IRQ controller, the interrupt controller which has many many pins will receive a pin from USB controller which which will signal him. Hey, one USB device I'm I have connected to me is signaling that there is an interrupt coming. So the IRQ will signal via this specific pin to the CPU. Hey we got an interrupt and via some other means via some bus it sends it to some specific register that's not that important. It sends it the code of the interrupt. So the CPU will know which interrupt is coming and every device has its own interrupt number assigned. So basically from the interrupt from the IRQ controller we are getting one byte which will tell us which device or which IRQ number is being triggered is being signaled. And from this number we will we will find out which what is the address of the handler that will handle handle this particular interrupt that will handle this particular exception. Normally these addresses can be fixed, defined by ISA. Or there may be an interrupt table, some specific data structure which translates the number of the IRQ interrupt to the specific pointer to the address where the code that should handle this piece of this type of interrupt resides. For instance, X86 has this table located at the very beginning of your memory, so at address 0 it starts this table. So at address 0 there is the pointer to the IRQ 0 to the exception 0. Actually, because the 1st 32 addresses are dedicated to to software interrupts. So this the table tells us the address of the interrupt handler. That's what we need to know to start performing, to start executing the interrupt itself. So the first step is that the current stream of instructions, whatever it is, whether it is in a kernel mode, whether it is usually it's in application mode, but nevertheless the current instruction stream is interrupted. Usually the currently running instruction is terminate is is let the hardware let it complete and the interrupt takes place right after the instruction that was currently running. Then a privileged switch needs to happen. Because the handler is always performed in kernel mode, it's basically usually part of the of the operating system. So we need to switch to the kernel mode and also the CPU needs to save the most essential part of its state in. At very least it needs to save the instruction pointer of the application that was currently running. It's like a function call. We need to know where to resume the stream of instructions that was running and how. So the essential part of the state. At the very least the instruction point of the program counter needs to be saved somewhere and that somewhere needs to have already pre allocated space. So this is safe to usually to some special register or registers. These registers are dedicated only to interrupt handling. So there is a specific register where the instruction pointer will be copied right during the during the interrupt. Then the code is switched to the, to the interrupt handler, to the to the routine we found in this table. And this handler needs to save. Usually it needs to save the rest of the CPU states. Sometimes it needs to save some registers so they can be later properly restored. Remember that the whole handler needs to keep the appearance as if the code wasn't interrupted. So the the interrupted code that shouldn't know this needs to be completely transparent to the interrupted code. So everything that that could be broken by this interrupt handler needs to be properly safe, preserved. So it is, it's later restored properly. And in between, of course, the interrupt handler performs some useful job. It communicates with the device, it updates some internal kernel structures that some device is ready to read or receive the data or whatever. So it performs its job. Usually this should be a very small routine. This should be very fast because another interrupt might happen. So we need to keep it short so that the interrupt handler isn't interrupted, right? Because that would be a bad thing. So normally we just schedule or play some information to the kernel structures that later it needs to pick up some data from the device or something and then complete restoration is required. So the interrupt handler restores whatever it saves here. So usually the registers and then the CPU restores whatever the CPU has stored. So the CPU actually restores what was saved. Here at least the interrupt pointer is restored and the stream of instructions continues from where it was interrupted. That was the interrupt handling, by the way. This is important stuff. This is something you should really remember how the interrupt is handled in the CPU. Yeah, and that's pretty much a good place to stop for today because the next week we start with processing. Are there any questions regarding interrupts, device controls, device handling, anything related to this topic or anything else related to this course? Nope, no questions. So thank you for your attendance and see you next week.