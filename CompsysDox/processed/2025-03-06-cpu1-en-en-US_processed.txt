Of this stuff. So we are shortening our delay at the beginning. Today we will switch the topic completely and we will start a new topic about CPUs. Basically, we are trying to get a bridge between your conceptions or maybe misconceptions about computers and stuff, and try to bridge this gap all the way down to the CPU. However, we will start from the CPU, but rest assured that there are many other pieces that needs to fit together, like the compiler. So if we start from the conception, let's just say I use as an example a very simple situation. You'd like to upload a meme on the Discord, right? That stuff you usually do, at least in your spare time I hope, so that you have the image and you want to just drag and drop it via browser, for instance. So you have the browser. But beneath this very simple operation, beneath this very simple task, there is a lot of code. Actually the situation is far more complex than this because yeah, there is the browser, which is hundreds of thousands, maybe millions of lines code wide if you include all the plug insurance and additional software that lies beneath the libraries, etcetera. But there is also some additional code like code in JavaScript that runs inside the browser. So the situation is far more complicated than that. But for the moment, let's just simplify it so your intent in the basics can be expressed using some high level programming language like COC Sharp for instance. However, this intent is still far, far from what the CPU can actually process, because here this code is almost readable too, even though you don't know the language. Even though I haven't told you which language this is, you will probably understand it at least conceptually, right? Because I'm opening an HTTP connection to the server, I'm posting a HTTP request. I'm receiving some some response. So basically, you can you can write this down. If you understand computer networking, then you could probably come up with something like this on your own. And if you know what library I'm using, you would probably be able to write this code on your own. However, as I said, this is far cry from CPU instructions. So this needs to be processed and there are several steps of processing I have just simplified. The compiler basically turns this high level programming language into a very low level representation of instructions in assembly language. This is basically the code that can be understood by the CPU almost. There are still some placeholders, like registers for instance, which are still understandable at least a bit by humans. But then you need to be translated for the CPU. But this is very close to what CPU can perceive. Actually, this is just another representation, textual representation of what CPU can actually process. And this is later compiled into the binary code so that each instruction here, each line of this code is interpreted as a sequence of bytes or bits in the final binary file. And this can be directly interpreted by CPU. This can be directly processed by CPU. So next several languages will be mostly about getting this bridge between some high level C code and low level execution. And as I said, we will start from the bottom from the CPUs. So basically CPU is a machine that performs some task. Before we get to that, just a quick revision. You probably heard about some CPU architectures. So just let me revise this very quickly. So the most common model or architecture model of today is the for Neumann's architecture, basically for Neumann, actually this is not the old for Neumann. There are some. There's been some revisions over the time because if you take old von Neumann literally, it's very old. He just operates in the terms of initiating some IO operations. That's not exactly what happens in more realistic CPU scenarios. So basically we have 3 elements connect interconnected together. The CPU, the core which actually executes the code, the memory which is used both for the code and for data. That's the most important fact about phenomenal architecture, at least when you compare it to other architectures. So both the code and the data are in the same memory and also there need to be some other peripheral devices like the keyboard or the monitor etc. So the other devices is also interconnected via one single system bus. So basically every time any of these two components need to communicate, they will use this bus. And that's the, let's say, the greatest benefit and the greatest drawback of this architecture. The benefit is it's very simple. You have only one bus, 1 synchronization to take care of, one place where all the data and all the controls are taking are transferred. However, this is also the bottleneck, because if you want that, if you want multiple devices, multiple parts here to communicate simultaneously, they can't. Well, normally when there's only one CPU core, why would we do that? It's like not really easily conceived. So normally CPU sends some data to the memory or asks for some data from the memory. So usually the CPU is talking to the memory one way or the other. Sometimes the CPU communicates with the devices. And actually, if this mechanism is designed well, the communication with devices is the same as the communication with the memory. We are talking about memory map devices. We get to that later when we talk about operating systems. So the communication with the devices can be, from the perspective of the code, very much same as the communication with the memory, just loading or storing some data via this universal bus. So we are just using different addresses for real memory and different addresses for devices. That's the only distinction we need to do. However, there are situations where memory and input output devices can communicate directly without the interference from the CPU. Let's just take DMA controlled devices into account. These devices are very special devices that can communicate or transfer data directly to or from the memory. So basically they can just send the data over the bus or read the data over the bus without any actual interference from the CPU. So the CPU can perform other tasks. That's the benefit of DMA. However, in this scenario there won't be much of A benefit at all because the bus will be blocked during the DMA. So actually the CPU won't be even able to load the next instruction because remember the instruction is in the memory. So if the bus is occupied then the CPU is stalled anyway because it cannot communicate via memory. And the second issue with this architecture it it doesn't scale well. You've probably heard that there nowadays we have a multi core CPU. So this unit here is actually replicated several times. Even if your cheapest laptops or even if your cheapest cell phones, you have usually at least two or four CPU cores. Even the very low end devices have that. So basically you have this replicated several times. And in servers you can have 10s or even small hundreds of CPU cores. So if you imagine that all these CPU cores are communicating via single bus, that would that would be a very, very bad button like that. They they wouldn't, it wouldn't scale well. So what we need actually is to decompose this bus, this bus further. One important thing I I haven't mentioned yet is the fact that the memory holds both the data and the code, as I mentioned at at the beginning is quite important for several reasons. One of the reasons is that we need to we as a programmer, as low level programmers, need to make some effort to distinguish between code and the data, because having the code and the data mingled together in the same memory may be potentially disastrous. Well actually in the past it was used for beneficial purposes like if we had some special CPU architecture that was not capable of performing advanced tasks like not really advanced tasks, A slightly more advanced task like indirect addressing. If you need to perform indirect fetch in the memory, so you have a index stored as the data and you need to use this index to fetch the data, then one way how to do it on a very poorly designed architecture would be to rewrite your own code. You basically access your own code and put some stuff in there so the code behave slightly differently. For instance, you put a new constant in the code. So yeah, there were like dirty tricks that can be used in this in this manner, but that's not what what we what we are using today because actually it's potentially dangerous. Just give let me give you a very quick example. There is a actually vulnerability which is called the buffer overrun attack. This is a situation which can occur if you have a poorly designed code. Let me give you an example. You have a code where the data are laid in the memory. Thusly, you have a buffer of some values, and into this buffer user input is loaded somehow. It's not like from the network or from the keyboard or something. And in the memory, right after this buffer there is a, let's say return address from a function call, for instance. OK, so if you somehow smuggle in data which are larger than this buffer, and the code is poorly written so it doesn't properly check whether the input fits into this buffer, you can easily overwrite overwrite slightly more memory than the programmer intended. So you basically overwrite this address as well. And if you so happen to guess where this buffer lies in the memory, you might somehow smuggle in an address that would jump back into this buffer. And if you somehow smuggle in instructions instead of data, then these instructions could get executed and you just smuggle your own code into someone else's code, which might be potentially disastrous. If you don't really understand how this works, don't worry about it. I'm just telling you it's possible if you if you're not careful enough. So this is just one way how to attack poorly designed applications. By the way, it's not so far fetched. You probably heard of about the heartbreak Buck. It's not that old, it's like 5-7 years old. It was a big issue in the Internet community. All the servers needs to be patched because there was this this kind of bug in the one of the most profound libraries open SSL, which is basically used for encryption all over the world in in many servers. So yeah, if you find out buck of this nature in a such such essential piece of software, it might be very tricky. OK, let's get back to this, to this issue. So technically there are mechanisms in the CPU that try to distinguish whether some part of the memory is code and should be executed and some part of the memory is just the data and should be read and write, read and write, but not executed. OK, just to compare this, there are also other architectures. The most famous next to kin is the hard work hard work architecture and the difference is that the instructions the program is in a separate memory. If you don't know any architecture like this, just think about Arduino's. Arduino basically uses this architecture because your code in a separate E from actually flash memory. Then the registers where the data resides. So it has separate 2 memories, one for code and one for data. But besides that it has the same problems. So we cannot have just a single bus. By the way, this, this is just not it's, it's not only for the Arduinos, most of the microcontrollers of the day, which are used in your washing machines and, and stuff like that still uses this architecture. OK, so give me more realistic example of, let's say, more recent architecture. OK, yes, Sandy Bridge isn't exactly new. It's, it's more than 10 years old. I think it's like 15 years old. But it illustrates what I want to tell you very well. So computers, the PC systems of today are decomposed so that they have, they have multiple chips. And these chips usually communicate via peer-to-peer buses. There is no single bus, There are buses between each two components and only these two components uses these buses. So there is no congestion. Well, actually there can be congestion if they communicate too fast, but it's not like that one device is blocking another device. Any 2 devices are are using their own separate peer-to-peer bus and the whole system is usually laid out thusly. The main part is the CPU. Actually this this box is everything that's that can be found in the CPU. And now I mean the CPU die, the chip that that's inserted in your motherboard, besides the cores and caches which are obvious it, it also holds a memory controller, so it has its own device that can communicate with the memory. So the main memory is connected directly via unique separated buses to the CPU. And actually the memory controller can be replicated there several times. Usually even in in the cheapest CPUs, you have at least 2 memory controllers. So we have like 2 memory buses and each is for a separate memory channel. By the way, you probably know that you have even in your laptops, you usually have at least 2 memory slots. This is not just that that the vendors can charge you extra, it's there's a practical reason for that. And the reason is that if you have two memory chips inserted, each one can be communicated with separately using a separate bus, which is much faster. You have like you have doubled the access, the throughput, not the access time because the like latency remains the same, but you have doubled the throughput. So in any case, if you're building your own computer or upgrading your own computer, it's a good idea to occupy both slots. It's it's a good idea to have two memory chips on your on your motherboard. Otherwise you're like wasting unnecessarily your memory throughput. And besides the memory bus, which is nowadays like the most, the fastest external bus there is, because you know, this is where the bottleneck is. Because if you have a multiple cores that are running in several gigahertz and all of them want to crunch some numbers, want to crunch some data, then they need to be fed this data. And the latency of of these channels is quite high. It's like 1000 or sometimes thousands of cycles. So you really need to feed the data quickly. So these buses are like the widest in the terms of throughput. The next most important bus is the PCI Express bus, which is today used especially for graphical cards. So if you're gaming fans or you need to do some heavy computations, Super Bowl have your own dedicated GPU. Technically, sometimes like office laptops or office desktops use integrated GPUs on on the chip with the CPU, but that's a different story. Let's just assume that you are gamer and you need to you need a really powerful GPU then this this GPU is connected via PCI Express and that's the second fastest bus external bus there is. Besides that, there is another link. By the way, this a greenish box is another chip. This is really another piece of hardware, not another piece of silicon. By the way, it's called S bridge. If you're wondering where the north bridge is, the north bridge used to be between the South bridge and the CPU and it contained the system agent, which is used for the communication and the memory controller and of course the PCI Express. Usually it was back then it was AGP, but that that's not that important. So it, it holds the more the fastest buses. But in the past, it was discovered that the north bridge, the communication between the north bridge and the, the CPU itself is too slow. So to fasten things up to to speed the things up, they integrated all the components from the north bridge into the CPU die. And thanks to the let's say minimization process as we shrinking the transistors in the CPUs. Nowadays, there's no problem because the memory controller and the system agents are relatively small when you just express them as a how much silicone, how much part of the wafer you need to allocate for them. So it's no problem to put them directly into the into the CPU. So basically now we don't say Southbridge, we just say chipset. That's the assisting chip to the CPU. And this chipset holds every other communication. So it's like a secondary communication device. That's why it's called a bridge, because it's like it bridges communication between the CPU which has like only one universal type of communication bus. Usually today CPUs like Intel use Quick Path Interconnect UPI and it handles all the intricacies of all other types of devices. So special connection with the audio codec which uses which is used to convert the digital and analog signals. So you can hear some sound from your computer or you can record your microphone. There is also BIOS. So there is some extra memory which is attached to it, which can be read and accessed by CPU. This is for instance, where the main boot loader of the of the of the of the PC is. So this is the basic code which is started when the PC is booting up, when the CPU is turned on other devices like floppy Dr.? Yeah, that's, this is very old. Anybody know still what floppy Dr. Just wondering, anybody heard about floppy? Yeah, a few people who did so yeah, it's, it's still, I think it, it's because the floppy Dr. is still used as the icon of saving. All right. It's like, Jesus, they died so they can become an icon of saving. So floppy Dr. is out of the question nowadays. But we still have things like USB. Ah, here it is. So USB buses that that are still very valid, yeah. Now nowadays we have USB 3.1 or so, but devices connected through USB are also here. And of course, perhaps the most important, maybe something that I should should have started with is the drives interconnect. So today we are using SATA. In the past we are using other other communication buses like Pata, Parallel, ATA. And finally, there may be additional PCI Express links. The PCI Express 16 actually is a congestion or concatenation. No, no, no. What would be grouping of 16 PCI Express links, But you can use them separately. For instance, you can use only single PCI Express link, which would be which would have enough capacity to for instance interconnect with the Ethernet adapter, with the networking adapter or with Wi-Fi for instance. If you want more, there are PCI Express for devices like external faster memory storage like if you want if the SATA isn't fast enough for your hard drive. If you use some very special hard drive that's really fast nowadays, you can use PCI Express 4 which aggregates 4 PCI Express links together to communicate with the with the hard drive. Funny observation, most of these buses, USB PCI Express, yeah, this doesn't know the buses that interconnect the audio connect and all other buses that really needs to connect two devices for a longer distance are serial. Serial, which means they usually contain only two or three wires. Yeah, that's, that's kind of a curious because you would say, OK, if we need to boost the throughput of a bus, we can use more lines, right? Like over here, PCI Express 16 uses 16 times 2 lines. So basically why we are not doing that in all other devices, for all other devices? Well, the reason is that designing a parallel bus is much more complicated, much more sophisticated. The problem is time. Just imagine that you are drawing a motherboard. I mean, you are designing a motherboard and here we have your CPU and here we have another device that needs to be communicated with and you choose to use a parallel bus instead of a sequential bus, a serial bus, Sorry. So you need to place lots of tiny wires, one next to each other, right? The trouble is that this wire, this this piece of metal is much shorter than this one. Yeah, it might be just a few centimeters, but remember that the electrical signals travel almost at the speed of light. I'm not sure if it's like a two third or or eight 9/10 or something like that, but it's very comparable to the speed of light. So imagine how much time the speed the light travels if you if you're sending signals in the at the frequency of gigahertz, a few centimeters. So basically you are in trouble because you need additional synchronization circuitry that will deal with this, with this additional problem that there is a, this signal travels quite longer than this signal. Inside the CPU there are many buses that are still parallel. So inside the chip, inside this chip there might be many buses that are still parallel because inside the silicon it's much easier to design them in the way so that they are similarly long. Maybe if you if you take a very close look to your motherboard and you see some parallel buses there, some of them use intentional extension so the wire isn't straight. But it looks like this sometimes if you take a very close look to your motherboard and that the reason for that is they're trying to lengthen one path. So it's it has the if it's like this path, it can be lengthened by by creating small curves in it. So it's similar. Its length would be similar to the longest path. OK, so this is how contemporary, well contemporary from the, let's say very old perspective, but the PCs of of today have very similar design. Yeah, we have a new new version of USB and we're probably using faster lens but and better, better hard drives. But basically the concept remains the same. OK Today we will focus only solely on the CPU. So what is the CPU? First, let me just clarify a few terminological terms. So I've used the word architecture several times, so let me now start using it properly. A CPU architecture. Actually it means two things. I can mean the instruction set architecture, ISA. And this is like the specification, this is like a manual or a contract that says how the how the CPU will be will appear to the programmers, which instructions will be will be there, which registers will be there. So how can I program this this device? Knowing the ISA is what what is what is important for us, for us programmers. Because if you want to write your own compiler, if you want to write your own operating system, then you really need to know the ISA of the architecture. And if you're writing a multi architectural OS like Linux, you really need to make modifications in your kernel so it can be adapted for different architectures. So Linux kernel isn't universal in the terms that you can take the same kernel, same instance and drop it from X86 to ARM, for instance. There are different parts for different architectures, they need to be adjusted, they need to be rewritten specifically for specific architectures. But the specification of the ISA is the only thing we need to know to do that. We don't need to optimize it for specific version of the Intel, for specific generation of the Intel CPU. So you don't have to use very a very particular version of any ARM CPUs, we can just know the universal specification, the ISA. On the other hand, the hardware architecture is the hardware implementation, how the ISA is transferred into the circuitry, if you wish. So hardware architectures to compare ISA could be something like IA 32, which you probably know more as an X86 architecture. This is the most common architecture of today, probably in most of your laptops. I'm not sure if I'm seeing any apples here. HP HP so probably If you have Apple then probably not Apple. If you have Apple that's different architecture. But everyone besides Apple who uses regular PC uses X86 architecture. ARM a 64, Risk 5. So these are the ISA specifications. If you want to talk about hardware architectures, we can say something like Intel. Raptor Lake is the last one. I'm not sure. Raptor Cove, they have this naming. I'm not sure who invented that, but they have the lake and the Cove and one of them is the architecture of the CPU and the other one is the chipset or vice versa. I'm not sure, it's very confusing, but or AMD's and five, that's pretty straightforward. So this is like a specific implementation of a specific vendor that conforms to given ISA. So let me down, let me get down to the basics. What is CPU? So basically CPU is a simple machine. Yeah, when I'm simple and I'm addressing something that that holds billions of transistors, that might be confusing. But by simple I mean it's capable of executing only a very simple commands. These commands, as you know, are are called instructions. And one instructions is something very fundamental like a basic arithmetic operations, like an addition for instance. And of course CPU must hold additional circuitry that assists with this task. Like it has its own memory called registers. This is the memory which is closest to the CPU where intermediate results are hold. It has memory controller so it can communicate with external memory and it has some way how to express IO. So it has some interface that can connect it to the South bridge and through the South bridge to all other peripheral devices. Let me give you an example of an instruction. For many most probably examples, we will be using the MIPS architecture. Just to explain, MIPS 32 is an architecture which is kind of old. You can still find it in devices like rotors for instance, or maybe other, let's say not PC devices, but embedded devices, etcetera. However, it's not that it's not like we are teaching you something completely useless. The MIPS 32 was used as a basis for a RISC 5 architecture. Perhaps you heard of it. RISC 5 is like the modern architecture, which shows a lot of promise and some people say it might replace X86 in a foreseeable future. So it's not a completely waste of your time. It's, it's like a very interesting architecture and basically the differences between MIPS and RISC 5 are beneath us at at this moment. There are some differences, but they are not important for the for the purposes of this class. So if you just learn the MIPS, then you can very quickly learn or extend it to to risk 5. So as an example, I take this instruction which is called ADDI at intermediate if you will. And this is how this instruction is represented in memory. So this is the bridge between this is the last step of this of this gap which we are bridging between the semantics and the binary code. So in the binary in the memory, the the processor will see a 16 bit, 32 bit word, sorry, it's called MIPS 32. So it uses 32 bit words as the as the as a basic width of the word. And each instruction in this architecture has 32 bits. That's one of the reasons why we chose this architecture, because it's very simple to find out where an instruction starts and ends. If you take a look at align 32 bits at 4 bytes, anywhere in the memory which holds code, it will hold an instruction. If you take a look at X86 code, somewhere in the middle, you don't know where to start reading because X86, for instance, I I believe Mr. ASIC told you about X86 in the previous classes. Is that so? Does he still teach X86 in the winter term? So you probably notice that in X86 architecture, the width of the instructions varies a lot. The shortest instructions in X86 takes 1 byte and the longest takes I think 8 bytes, or 13 bytes if you count in the prefixes. So somewhere in between there may be length of your instruction and you couldn't know until you read enough of the memory. So usually you read the first byte, then you read a few more bytes, and then you can read a few more bytes to find out the final instruction. So it's very difficult to decode in compared to this. Furthermore, the instruction itself has a fixed format, so if we chop off the 1st 6 bits, it always holds the OP code. So the instruction, what the instruction is, is stored is encoded in the 1st 6 bits. Just by looking at it, I can immediately tell, ah yeah, it's it's the ATI instruction. Everybody knows that because it's 001000, right? Remember that it will be on the test. I'm just kidding. So this is the prefix of, this is the OP code of the instruction. Next 5 bits and next 5 bits are the addresses. Addresses of what? Because 5 bits isn't enough to address memory. This address could hold registers, right? So basically we can we can address registers. So these two are registers used as operands for this instruction. By the way, if we have 5 bits, how many registers this architecture has? How many registers? This architecture may have guessed from this instruction 32, because we can address five of them using 32 of them. Using 5 bits. to the power of 5 is 32, so this can hold 32 different values. This can hold 32 different values, so we can use 32 different registers and the last 16 bits. It so happens that it's half of the work, 6 bits 5 + 5 is 16 together, so the rest is also 16 bits wide and it holds either another register value, another register address, and the rest is unused. Or it can hold an immediate value, so it may hold a constant. It's like if you write down a literal in your code. If you write number somewhere in your C code, then somehow this number needs to be interpreted in the code. So one way how to do it is that they are using intermediate values in the instructions. So the constant from your code is intermediate, intermediately encoded into the instruction itself. On the other hand, as you can see, this can hold only 16 bits, so you cannot possibly use it to fill 32 bit register at once. You can use it only to fill half over 32 bit register. So it's a small drawback. By the way, isas the architectures can use 1-2 or three operands, this one obviously uses 3 operands 123. It's possible there are architectures that use only two. Can you name 1 X86 architecture, for instance? You've learned it in the Winter term that uses only two. There are also architectures that use only one operon. Sorry, there are also architectures that use only one, but that's a bit tricky because you need some stack machine or something to to compensate for that. Technically you can, you can use zero, but that's not very practical. And technically you can use more than 4, but usually you don't need more than 4. Three are more than enough because three basically can express very simple operations like take one number, add it with another number and and put it in the third number. What more you would like to do? OK, so this is about instructions. And let me explain how the instructions very quickly, how the instructions are processed. So basically there are like several steps that the CPU performs in a loop, forever and forever. First it loads the instructions from the address where every CPU has like a instruction pointer IP. Don't don't mix it up with the IP like in Internet Protocol address. No, no, no, it's instruction pointer. Yeah, sometimes it's used program counter, but the acronym for that is PC, which is also confusing. So pick your guess. I be, I be I will be using instruction pointer mostly. Next step, when you load the word in in MIPS it's very simple, you just load the 32 bits. In X8X that might be much more tricky because you need to load something in between 1 and 13 bytes. Then you decode the instructions. So you basically re represented it from the very compact format the code represents into some internal format which is specific to the hardware architecture. OK, so the hardware architecture have its own ways how to express instructions and the decoder basically converted from the uniform compact format that is specified by ISA into whatever internal format they want to use. That usually translates into direct commands to individual units in the CPU. Then there are three steps for most instructions, loading the operands. So loading the inputs, performing the operation the instruction is supposed to do, and then storing the result somewhere back. This usually involves loading values from the registers, and this usually also stores the value to the register. But some architectures like 686 oftenly use loading from memory or storing back to memory, so that might be also possible. And finally, the ultimate last step is incrementing of the IP. So the IP counter is incremented is is advanced so that in the next loop we will read the following instruction. In MIPS it's very easy, we just add 4 to the IP. That's very simple. In X86 or other architectures it might be somewhat more tricky because we need to find out the width of the instruction and add appropriate number of bytes to the IP counter. Yeah, and of course this is not like a crave into stone, because some instructions may omit some of these steps. For instance, if the instruction doesn't have any output, if it just performs some task, like if it's an output instruction or if it's an instruction that performs jump so it affects the IP, it doesn't have any other result. So there is no storing of the result for this particular instruction. For instance, let me give you a glimpse at how this is executed at the bottom level. This is very simplified design of a CPU. I've just picked a few parts of the CPU. Actually it's like a virtual CPU, not actual one. How it might be implemented just to give you a glimpse at how instructions are executed inside. So there is some unit which is called instruction decoder and usually there are some units which schedule the execution of instructions. I've merged them together in this particular diagram. And then there are registers which you can imagine as some flip flops that can hold values. And there are some additional computing units. I, I just take the adder unit because I will be demonstrating the ADD instruction, the very similar one I just told you about a few slides ago. So I will need the adder unit. And again, this is just a bunch of logical gates. Actually, if you have the time, you can, you can just design an adder using what you know from, from basic logic. It's not that that complex, but we put that aside for this moment. So basically what we are doing here is that we need to transfer the data from these registers to the adder and then transfer the the result back to the appropriate register. What we will be implementing or what I will be demonstrating is this instruction add R3R 1R2, read it from the left to right as if you as if you put it in the C notation. So it's like R3 equals R1 plus R2. OK, just to know the semantics of this of this notation and this translation. This equality is what the decoder does. It translates it into some internal representation which actually comprise multiple steps. This instruction isn't computed at once, at least not on this virtual CPU. Because we need to use the data bus several times. I've intentionally designed the CPU to use only a single bus. Just to give you a glimpse how the things work, usually there are multiple buses and they are somehow interconnected or not. So it's much more complicated. This is very simple design because all the registers, all the units, everything is connected to this bus via these circular objects which are gates and gate is like imagine just like a switch that allows or disables whether the data can come through in one or the other direction. So if this gate is active, it allows this register to be filled from the data with the bus which is currently on the bus. If this gate is on, then it will take the value of the register and it will write it on the bus. So one of the tricky part is that this internal representation must make sure that at any given time only one of these output gates which is writing is open. Otherwise there will be collision on the bus and it can it can result in some damage or some malicious behavior. So normally only one of the output gates should be on. What happens if you want to interpret this micro code, if you will, this decoded instructions. So first operation is let's open GR1. It is a gate from register one output plus GAA, that's gateway of the header unit register A. So basically we are opening this gate for output and this gate for input which results into transfer of the data from this register to this local temporary register of the header OK. The same goes for the next step. We are opening gates from register 2 and input register B on the adder unit. So the next step would be that the data will be transferred from the register 2 to the register B of the adder unit. After that, the adder unit needs to be activated to perform its miracles. So basically the instruction decoder will poke the unit and it will just take these inputs and produce the output in its own register. OK, somehow, and finally we need to write this back. This is like there is some internal synchronization which we are just hiding from you. And finally, the output which is currently reside here needs to be transferred to the register 3, right? So this is the same mechanism. We are opening this gate and this gate and the data will be there. Yeah, this is very simplified view. Usually these steps, some of these instructions may take many more steps. Also, I'm visualizing only the fetching of the operands, the execution and then storing the results. Only these three steps. Now I need to load the value of the IP register into the other unit, add 4 to it and transfer the value of the IP register back to the IP register using the very same mechanism, right? But this is just how this works in the internals. It's not for you important to know this at this level. This is just to give you a glimpse the notion of how things are working internally. If you really want to know how things are working internally, you need to know a bit more about flip flops and logical gates so that you can visualize how these individual parts are actually working at the electrical level. OK, so let's move on. I said that there are some types of instructions. We already know add instruction or two types of add instruction actually. So one of the fortunes of C&C is that is very close to the actual instruction level. Yeah, you may have not noticed that, but it's much closer than Python or JavaScript. So basically, if we take a glimpse of these code fragments, what kind of instructions you think we will need to have in order to implement them in order to translate them to assembly language? Any guesses? Any hints? A comparison operator or something? Yeah, let's start from the beginning. So this will require some sort of comparison so that we can compare 2 numbers to registers or to some things and get the result whether it holds true or false. That's right. We need arithmetical comparison. Anything else, 8 jumps, right? So yeah, what is a jump? We can, after these comparisons, we control? Yeah, exactly. We need to, let's say, skip over some code. We need to alter the flow of the code control. So normally the code is executed sequentially, the instruction pointer is incremented, but we need some special instructions that can advance the instruction pointer forward or sometimes even backwards. So for instance here, if this condition doesn't hold, we need to have, we need to have a mechanism that will skip over this assignment instruction and continue here. OK, so yeah, we need some sort of jumps and actually we need two types of jumps, right? Do we Not necessarily. OK, why not? I don't see why you couldn't repurpose the same jump man for both in this statement and the poor lead. OK, And then which type of jumps are you talking about? So I mean, how, how would your jump behave? How would your imaginary jump instruction behave? If you want only one type of instruction, just just I think please. No, you're right. We need a regular jump in the condition which you want to know. Yeah, that's what I was aiming to. But probably you're right, we actually don't need the regular jump. If we can force the conditional jump to jump as well, then we don't need the regular jump. But that's like a complication in the architecture that's usually too difficult to handle. So usually we have at least conditional jumps and and non conditional jumps. The non non conditional jumps jump all the time. Conditional jump jumps based on a previous instruction, a previous comparison instruction, or a boolean value for instance. OK, so we have jumps, we have comparisons. Anything else that can be spotted right from this code? Yeah, there are multiple arithmetic operations. Yeah, we already know the the edition which is here and here. We don't we haven't met this one. This is this is shift. So we also need some operator, some instructions, sorry for this for this operator. And yeah, we can name them all. So if you just happen to remember which operators binary and unary are in C, then basically you get them all as some form of instruction, almost not too fancy, but additions, subtractions, bitwise operations, shifts, they are all there as instructions. Yeah, that's right. What else? Assignment. What is assignment? Yeah, but technically, what in the instruction level? What would assignment do? Change the value of? Some sort of change the value of of the memory? OK, so if we're talking about memory, then yeah, but I will slightly alter your statement. We need to handle the assignment, but we need to handle the assignment to the memory, not to the register. Because if we just jump back here, this edition is also an assignment. This performs addition of these two values and they are assigned over here. So if I need to assign R2 into R1, the only thing I need to do is place 0 here and I have an assignment. So assigning 1 value from one register to another register can be also done using arithmetical instructions. Technically speaking, actually it's not entirely true. Usually architectures will give you the assignment instruction as a copying value from one register to another register. Because adding zero from here to here from adding zero to this value also would activate or may activate the the addition unit. You don't want to waste that much time and energy. So basically you just want to copy the data, but technically, theoretically not. Technically we wouldn't need that, but we would need, as you said, transfer of a value into the memory. This is probably some array in the main memory. I haven't declared it, but it's kind of obvious this this should be some array in the global memory and I need to fill this value into the into the proper spot in the array. So I need a way how to put the data into the memory and vice versa, right? You can see that probably because, yeah, B&A, they, they might be in registers, but they have to end in the registers somehow in the beginning. So we need to, we need some way how to load the data from the memory into the register and then to store it from the register to the memory. By the way, this is called load store architecture and it's typical for, I wouldn't say cheaper but faster architectures like MIPS X as X architecture used to design the instructions more in a way that you can immediately use memory addresses in normal operations. So you can use a memory address as one of the operands in the in the add instructions for instance. So there are architectures that that go above this the other way, but usually there will be How we can think about the architecture is that we have a load from the memory and store back to the memory. And there's one, one other thing we need to have to complete this, this piece of code, these pieces of code. What else would we need? One more thing and two more things which are tied together. Nope, nobody. We're talking about loading from memory. We're already talking about loading from memory. We talked about loading, Yeah. We need some mechanism to load and store data from and to the memory. So where which one? Bit shift, bit shift? We covered bit shift. I said, OK, we have all the bit wise operators and shift operators. Yeah, I haven't enumerate them actually, but let's just imagine that we can do that because you know what, what's in the C and basically what's in the C is in the in the assembly as well in the instructions as well. It's something to do with arrays that notes have without array, like with with what the there's an array there. Yeah, array is a bit tricky and we get to that. But let's put the array aside because technically this is a arithmetic operations. You need to add and multiply and then you need to load and store. So yeah, it's a bit more tricky, but we can do this, do that with these instructions. We need indirect access to memory, but that's something we get into it in a short order. But there is one fundamental thing that's different. Nobody like reference to the. No, no, no, actually we don't have any references in here. This is a. This is an argument which is transferred via value. So we don't have a reference here. But we need to have a way how to call this function. Because call isn't just another jump. Actually call is somewhat like a jump. You need to redirect your execution pointer, your instruction pointer to another location in your code. But the body of the function is. But doing so, you need one other thing. You need to somehow store where you should return. You know, because when you call this function F, it holds this return statement. And this return statement needs to know where to jump back. So calling a function is not just another jump, it's a jump that also needs to record where to jump and vice versa. Return needs to jump back. So return needs. Yeah, return can be actually handled using an indirect jump, so jump to a address which is stored in a register for instance. But at least the call needs to be handled somehow. So if we have a function call here, this call over here needs to perform this special kind of jump. So if we just sort out all your ideas into the classes, so we have load instructions from memory to the register, we have store instructions from register or immediate value to the memory. Remember, the load operations take some time, so they are usually handled specially or somehow differently in the CPU, because the CPU tries to sometimes, usually fetch the data ahead. You know, as I said before, the data bus to the memory is somewhat long in the terms of latency. So it, it might take even let's say 1000 cycles, 1000 instructions before the data arise from the main memory to the register. That's a long time. From the perspective of the code 1000 instructions, that's a lot. So the CPU is trying to place to detect the execution of this type of instructions early and trying to speculatively fetch the value from the main memory at least to the to the caches. So it's much closer when it's actually needed, right? This is how the CPUs of the data really work. And also the compiler tries to rearrange the instructions so that when you have a fetch instruction in your code, it tries to rearrange these instructions far ahead. So the fetch actually takes place much sooner than the value is actually needed. So that's not another way how to go about this. So basically fetch instructions need to be placed early so we don't have to wait for the data when we actually need them. Move instructions are instructions that move data usually between two registers. In X86 you can also move data between register and memory or vice versa. So basically X86 use only move instructions instead of loads and stores based on the operands. It performs move, load or store depending on what operands you give to the instruction. So arithmetical instructions I told you we basically cover them all. This is normal. Enumeration used the C syntax, so all these are usually there plus minus. Add SAP, no problem. Shifts and all bitwise operations are no problem. They are very fast. Multiplication, division and module law is slightly more tricky. Usually you have them as well, because it would be difficult to write a code without them. You really need multiplication, for instance. However, these instructions often take longer time to execute, so sometimes the CPU is. Sometimes the compiler is trying to avoid generating them, especially if you can replace them. For instance, if you're multiplying something with two, it can be replaced with what type of operation Shift left? That's right. So multiply by two is the same as shift to the left by 1. So if the compiler detects this type of optimization, it it usually takes it because it's it's much cheaper to perform shift than than multiplication. By the way, there are architectures that doesn't have multiplication nor division, very low level cheap architectures like Microchip for instance. If you know Microchip, pick controllers, they don't have multiplication, at least the cheaper ones. And finally, by the way, these are the basic operations. And when I'm talking about multiplications, I'm talking about multiplications of integers or maybe floats, not about multiplication between a string and a number. That's a funny stuff in Python And I don't think any other language has that. So forget about this. We are in C we are in low level, low level instruction level. So don't think about things like multiplying. This actually resolves into many, many instructions because you need to reallocate this memory, register this memory on the heap, copy this part four times. So it's basically a loop. Actually it's a nested loop, 1 loop that goes four times. Inside is another loop that scans this string and copies it character by character. No, this is very complex code. This results into 10s of instructions, at very least not counting the syscall for the memory allocation, etc. So very, very tough, very difficult to implement at the instruction level. If I just give you the assembly language, it would be hard time to implement this. And if you're not here, I will force you to implement this type of operation using only assembly language, because that would be a really, really tough task for you. OK, jumps, you already covered jumps. So jump is a displacement in in the of the instruction pointer as as we covered, unconditional jump always conditional is based on a test which is required prior to the jump. And this test usually involves either equality where the two values are equal, or some sort of inequality between two integers or two floats. There are three types of jumps from the perspective of the target where where are we jumping? Direct there is a explicit address where we are jumping. So that's like absolute value, absolute address in the memory and we are jumping at that very spot. Indirect means that we store the value, the address where are we jumping into the register and then the jump instruction reads the address from the register and jumps to that particular value. So it's like a data dependent, data-driven jump. This is important like this can be used as a return from a function. If you're returning from a function, then you will need indirect jump. And relative jump is a jump that jumps a few instructions ahead or back. Where is this useful? Well, actually using the direct jump requires quite a long value somehow embedded into the memory, right? Because if you have like a 32 bit architecture, then you need 32 bits of place somewhere in the memory, which is the address. That might be tough to handle, especially in MIPS which gives you only 16 bits of intermediate values. So it will be tricky. So if you want that, then yeah, you are somewhat limited in this direct thing. But usually you don't need to jump too far ahead. You need to jump only a small portion, like if you have this kind of loop for instance, then at the end of the loop you are jumping at the beginning. So you need to jump only a few instructions back, for instance. And since you know how wide, how large the body of the loop is, you can count how many instructions back you need to jump. So it's much easier to implement a relative jump at the end that jumps at the beginning of the loop instead of putting an absolute value. And furthermore, if you just place a relative jump there, then this loop may be placed anywhere in the address and it will work very much the same, right? If you just shift this code a few bytes off in your memory, it doesn't matter because the relative value will still be valid. If you have absolute values, absolute addressing in your jumps, then you need to adjust these values every time the code is loaded. So you need to fix it based on where the code actually was loaded. So it's always better to use the relative addresses instead of absolute addresses, at least when it fits the situation. And finally, the call and the return. Actually the return can be implemented by the indirect jump, as I said. So we need a call. And call is basically a different type of jump that somehow records the previous address. It takes the instruction pointer, save it somewhere and then perform the jump. So that's a call. It's like a combination of these two. Technically speaking, we can do this if we can have a way how to record the IP. If we can copy the IP register to a regular register manually, then we wouldn't need call instruction. We can use this instruction plus a regular jump to implement call instruction. So yeah, due to time, I will normally I would just interrogate you whether you can, you can rewrite this assembly, but let's just let's just skip slightly ahead and I will give you directly the answer so we can examine it. So basically let me revisit the code you see before you saw before and this is the if else branch. And this is how it will translate roughly into the assembly language. This is almost X86 assembly, I think. I think almost. I'm not sure if there are any differences. So what this does? What does this mean? I I realized that these acronyms are somewhat confusing. But if you, if you just get the hang of it, it will it will come out to quite, quite easily because J at the beginning always means jump. That's easy. So jump and GE is where when to jump GE means greater or equal. So now, now we're getting hang of it. So jump if greater or equal. And what is being compared a these brackets indicate that the a this is just a symbolic value. It's it's a place in the memory. So it's a variable in memory and the compiler, the assembly compiler know where it is. So the assembly compiler somehow will replace this with an address when it when it's compiled into the binary and and three and the three is like immediate value in your code. So it compares these two values. And if a sorry, it's, it's the other way around. So it's if a is greater or equal and three. So it's it's inversion of this condition actually, then we will jump. So basically, if this condition doesn't hold, we will jump to the next part of the code, right? Because if this holds, we we are executing what's next? So if this doesn't hold, we need to turn this condition, we need to negate this condition. So if this condition doesn't hold, if its negation holds, then we are jumping to the next part of this code, to this block. Let's start with the first part, that is b = 4. So we are storing 4 to the B, usually on the left. The left operand is usually the target. The right operand is what we are assigning. This is pretty straightforward. This is just a rewrite of this line over here. But afterwards we need to perform another jump. JMPMP isn't an acronym. This is really jump because it's unconditional jump. We are always jumping here, and we need to jump at the very end because we need to skip the else branch. OK, it's very straightforward. And so if we skip the else branch, we will end up here. So this whole 3 instructions are these, this is this expression load R1A means we take the a variable A and we put it into R1 register shift left R1 by two. That's pretty straightforward. What we loaded into R1 is shifted by two to the left that that's easy. And then we store what's in R1, that's our temporary value into the C This is the this assignment. That's it. So yeah, yeah, actually load and store I would. I had to take load and store as a placeholders for move. If I want to translate it to the X86 architecture, I need to replace this with move MOF instruction. But that's just a small adjustment I need to make because I want you to explicitly, explicitly see where I'm loading and I'm where I'm storing the values to. And from the memory. This is much more readable. But basically this can be almost directly rewritten into X86, x86 assembly language. So let's start with the second one. OK, this is slightly more tricky, but only slightly so. First of all, we need to initialize the loop, right? First we have a for loop. So first thing we do is we perform this first expression I = 0. OK, there is no I in the memory. Actually compiler decided that I would be placed in register. Because we have spare registers. So let's just put I in register and there is no, no reason why should we bother the main memory unless someone really wants to access from outside from another thread, for instance, the value of I and I cannot imagine why. Then it's perfectly safe that we just keep it local because it would be used very often. So it would be very tedious if we need to just all the time jump into the memory or at least into L1 cache. So we keep it in the register. And this is a bit of bit confusing because we start with the jump, but remember this is a conditional jump and remember the first thing, sorry. The second thing we do in for loop is that we test this condition. We are always testing the condition before the loop. So we also test it at the very beginning before we start the first iteration. So we need to perform this comparison again. This is jump if greater or equal. So greater or equal is inversion of this condition. So if R1, which is our I is greater or equal 5, which is an immediate value, then we jump and we jump at the very end and we don't don't continue with the loop anymore. This is the end of the loop. If we jump over here, we jump, we are jumping off the Cliff and this is the end. And the reminder is the body of the loop. So basically we need to load a into R2. What is a, by the way, in this in C terms, what is a? It takes an array, Sir. Yeah, if it's an array. So what is if I just write a in the code? What does that mean? It's a pointer, That's right. So I'm loading the address of this array in the memory. It's a pointer, so the address of the beginning of the array in the memory is in R2. Then I need to move the pointer to the right point because I'm indexing ith element in the array. So I need to advance this pointer far enough. This is a bit conspicuous because I'm just adding R1, which is I to the R2, and I'm storing into into the R2. Something fishes here, right? What's fishy here? You're adding a number to a pointer. You're adding a number to a pointer. Well, that wasn't what I was referring to, but excellent, excellent remark. I'm adding a number to a pointer. Is that bad? Adding a number to? Sorry, adding a number to a pointer. I just hope I didn't turn my microphone off right now. Is it a bad thing to take a pointer and a number put them together? No, pointer is also a number. Pointer is also a number. So yeah, semantically it might be problem if you're not doing it right. But technically in implementation level, that's fine. They're both integers, no problem there. OK, but something else might be fishy here. Remember what you know about pointer arithmetics in C So how do you advance the pointer a if you want to add I to it? Because this is just a. This is just a nicer form. How to write a plus I and the reference. Yeah, but so technically this looks like just the addition here. Yeah, that's right. But this doesn't always add if I is 3. This doesn't mean we are always advancing the pointer by three bytes, right? Nobody remembers. My goodness, you're right. Yeah, that's right. But that's not the answer I'm. I hope you haven't embedded your answer inside the words. I haven't completely understood. But yeah, the I is advancing the pointer, but if I is 3, it doesn't really mean that we are advancing it by three bytes. How if we have a array, how many bytes we will advance it if I is 3/3 bytes? That's not the same thing as three integers, that's right. So it depends on the type of the of the array. So if the array is array of characters of or or bytes, this will be fine. If this is an array of integers, and it probably is or it might be at least because we are assigning an integer into it, then we are in trouble because we haven't multiplied it over here. So this was like a small landmine I just left there so we can talk about it. OK, so this might not hold if the array is anything else but array of characters or bytes, right? If it's array of integers, we need to multiply R1 because because R1 is I by 4 or by size of the the base type before we add it. OK, so remember that that that's a that's a small pitfall. And then we store R1 I into the address we just computed. So the address of the element we just computed, this is indirect store. We are using a register as an address. So we take the value from the register as the address to the memory and we will store it there to the memory. So this is indirect store. This is direct load. The a will be probably placed here as a direct value. This is indirect store. And finally, we will add 1 to R1 because R1 is I and we need to add 1 to it. This is that's this instruction over here. Finally, we have jump that restarts the loop and that's it. Everybody's clear on that. OK, so let's start. Yeah, this is the just to remind you when you're reading the slides, this is the pitfall. I was, I was pitching it. OK, we have few more minutes. So let me start on registers. And I think that's the place where we end today. Registers because it's a good place to pick next week. Registers. Actually, we have multiple types of registers, as you may have noticed, but actually it wasn't that apparent from the examples. Normally most architectures deal with general registers. A general register is a register. It's like orthogonal for operations. You can place integers there, you can place other values there, you can place pointers there, it doesn't matter. The only thing the general register usually specify is that it has some width. So it has 32 bits, 64 bits, something like that. Some architectures make a distinction that they have unique integer registers and floating point registers. So some registers can be used for integer instructions, other registers can be used for floating point operations. Might make sense. Also, it might sometimes it might be useful to make a distinction whether the address is in the register or not. I didn't make that distinction in the examples before. And some architectures like MIPS don't do this distinction because, as somebody pointed out, pointer is still a number, so it's not that important for US. Pointer address registers and integer registers might be very much the same, but sometimes it's a good thing to make this distinction so that nobody would accidentally use a address register for a regular number or vice versa. Sometimes it makes makes sense. Also, branch registers are address registers, but there are special type of registers which are used if you have the branch instruction. Branch instruction is another type of jump instruction. So it's like you have a condition and you place in the branch register and address of the target jump. So basically you evaluate a condition and then you use the branch register to determine the target of your jump if if the condition holds flags. Again, this is special type of registers where individual bits or bit groups sometimes have a special meaning. And finally, predicate registers are registers which are one bit wide. Yeah, that's right. It's very strange register because you can place only zero or one inside this register, but they can be used to determine whether something should be or shouldn't be executed, for instance. So you can use this predicate registers to control your execution flow or you can use them to store results of your of your conditions. If you're making some tests like the comparison test or inequality test, then you can store the results into predicate registers, application and system registers. This distinction is made at the, let's say conceptual level usually. Well, usually normal registers are used for arithmetic operations, etcetera. Application and system registers are designated for things like argument values. When you're calling a function, return value when you're calling a function. Function arguments, what else? Stack pointer. Yeah, stack pointer and base pointer. So if you have a stack, which is necessary to implement correctly, calling of functions, because without stack you cannot place the return values anywhere. So you cannot have a recursion without stack. We get to that later. So basically you need a pointer, you need a special register that points where the stack is in the memory. And finally there are some additional types like vector registers which are very popular for high performance computing nowadays. You have a register which can fit multiple values like SSE registers can fit for integers or floats. So they have they have 100 to 828 bits in width and they can store for 32 bits float or integers side by side and or. The most modern AVX 512 instructions work with 512 bit vector registers, so you can fit 16 floats or or integers in them. And then these instructions that operate on these vector registers perform the same operation on all values at once. So they are much faster, they can be implemented much more efficiently. Another topic is naming. Naming of the registers is very tricky because some architectures go about it in a way that, OK, we have 32 bit registers. So let's see, we just name them register zero to register 31. And that's probably something I would have I would have done because it's pretty straightforward, although it's not very mnemonic because you need to remember whether you placed your value into register three or register eight. Yeah, that's a bit tricky. But in the end, you can use the registers orthogonally. There are no special registers in such a in such arrangement. On the other way, you can name the registers like EAXEBXECX where ABCD stands for special meanings like A is accumulator, B is base, C is counter, D is data. And they are designed to be specifically used with some types of instructions like the counter is usually incremented or tested, like the data usually holds the result of a large operation, etcetera. Finally, there are, let's say today, I would say obsolete because I'm not sure whether there is a contemporary architecture that still uses it. But technically you can use also a different approaches to naming, naming like stack that you have a stack, you know what stack is like a data structure stack, which can be pushed and popped. So the registers are named from the top of the stack further, so like SP1SP0, sorry is the top of the stack, SP one is the number right below the stack, etcetera. This was used in 8087 Co processor. Anyone heard about 8087 not 6. Anybody heard about that? I think I might have come across about dumbling. Yeah, it's very old. Actually, it's not that old because I'm still very young and this was on when I was learning how to program. But basically you, you probably heard about 8086, right? At least in history lessons, because this is where the X86 architecture came from, because this is the first processor that was worth something that was designed by the Intel back then. And then they designed 286 and 386 and 486 processors. And then came the pantume, right? You know by by the pantume was named Pantume. Why it wasn't named 586? Because they they used the 1st prototypes OF586 and they just take the four eight 6 + 100 and they get 587.000. So basically they they just name it pain tune because there was some flaw in in arithmetics. OK, sorry my side track got too far from this topic. So this was a coprocessor for 0086 and 02. 286 processors and it was designed to perform floating point math so there was like a Co processor extract Co processor just for floating point instructions and it was using the stack organization. So we are so we were pushing the floats on the stack and then you can perform stack operations like take the two top registers and put them together, add them together or something like that. And finally, there is also possibility of aliasing, which means that some register may overlap or may be named by different different names. This is better explained on this example. This is perhaps the most wicked architecture we have that's still being used because it has this type of registers and it's very old and it has so many skeletons inside that wasn't that that couldn't be removed easily. So this is one of the favorite teasing pass. Probably This is why Mr. Reject still uses it in his classes. We don't except for showing you that things can be done worse than you thought. This is the set of registers that were originally designed in the X86 architecture. Actually the first, the very first processor which was 4004 had four registers ABC and D and from these came these registers A was as I said accumulator, base, counter and data. And later when they extended it they need to rename ABC and D because the original registers were 4 bits and then 8 bit 8 bits wide so they need to extend it. So they used AL and AH as a lower bit and higher bit. So these two together form a 116 bit register. They are a bit wise. They form 116 bit register which was called AX and same for BX, BC and DX. So that's what I'm talking. When I was talking about aliasing, there is this AX register and it's covering both AH and AL. So if you are accessing AL you are reading only half of the AX register basically. And if you write into AX you overwrite both AH and AL. And when they extended it from 16 bit architecture to 32 bit architecture, this is where 386 processor came. They just renamed the AX to EAX like extended AX. So this EAX has 32 bits and this lower 16 bit part was aliased AX Yeah. And when when the X86 was extended to 64 bits they renamed EAX to RAXI don't know why R probably as a register. So the RAX is 64 bits wide. By the way, this is not how to design architecture well, because you have only four registers designated for basic computing. There are other 4 registers. 2 of them are designated for special functions. SP is a stack pointer and BP is base pointer. These two are necessary for performing function calls properly. So these two are very specific. And these two, SI and DI, are used as pointer source index and destination index. If you're copying data, they can be used easily. By the way, there is one instruction that has no operands, and all it does, it takes the address of SI, takes one byte from SI, copies it to the address DI, and then advances both SI and DI. One instruction, yeah, that's how this architecture was designed. These are segment registers which are not that important. This is an example of flex register. There is a specific flex register where individual bits have special meaning. For instance, if you perform a comparison operation, some flex like 0 flex or yeah I forget the name right now, there are few flex that are affected by this operation so that you can read these flex and then determine whether the operation was success or not, whether the comparison hold or not. And of course there is instruction pointer or architectures need instruction pointers. By the way, even in Intel they realized how great mistake they did by designing an architecture with so few pointers, so few registers, Sir. So they extended it, they added extra eight whole 8 registers when they extended it to 64 bits. But still we get like 16 registers in total for basic computations and that's really not too many. 16 is a very low number if you try to fit all the data in in a regular application code. Yeah, I have like 2 minutes. So I think this slide is too big for two minutes. So I will keep it for next week. So let's just keep it slow and let's, let's pace ourself. I think we can use this 2 minutes for any questions that might rise at the very beginning of the CPU architectures course. So in Mix 32, how does do you access, how do you load and store from memory? It's a load store architecture. You have explicit instructions where you place a load or store and the data is copied from the memory to a register or vice versa. And another way is there are some instructions that use this immediate values. So you can use this immediate values to basically load constants in your code. So this these are two ways, but it's a load store architecture. So you're not moving data from memory to to registers implicitly. You need to place store or load instruction ahead of your code or at the end of your code to transfer data here and there. Any other question? OK, I think we can finish 50 seconds early. So thank you for your attention and hopefully see you next week. Thank you.