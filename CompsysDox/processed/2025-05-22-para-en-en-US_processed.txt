So hello everyone, welcome to the last talk. Today we need to finish everything so it will be last long run. We have a few more things to wrap up in the in the talk about operating systems. Afterwards we have one full lecture about parallelism and parallel programming. And in the end, I will talk about how the exam will look like and what should you prepare for the exam. So again, like last week, we are we are going to make a short recess, short break in the middle. By middle I mean roughly after one hour and a half. So we will take a break because it won't be possible to for you to stay awake all the time during the whole session. So please try to bear with me. I know it will be a long session, but I will try to wrap it up in the painless possible way. So last time just to just to smoothly continue. Last time we talked about paging and in general virtual memory virtualization. So the paging is now the technology which is being currently used for translation from virtual memory to physical memory. This is just the slide that wraps it up very nicely. So we need to translate an address from the virtual address space into the address into the physical address space. The address is chopped in two parts based on the size of the pages and frames. Pages and frames are the same size, so we need to chop off last n -, 1 bits, last small N bits, sorry of the offset. That's like the address within one page or the address within one frame. And the front part of the address is the index of the page. So the index of the page goes to the page translation table. From the page translation table, we get the address of the physical frame where the page is allocated. And we combined with the offset from the which was chopped from the previous address and this together forms the physical address. That's it. The tricky part is how to translate the page index into the physical address. So basically we have several options. My pointer isn't working, sorry. We have several options, but the most straightforward option would be to have a page table, which means a huge array where for each page there is one slot. So each page has one record one element in this table and they are mapped 1 to 1. So page with the index 14 has the 14th, actually sixty 15th because it's indexed from zero. So sixteen 15th, sorry element in this array and the element holds the address, the physical address of the frame where this page is actually stored in the physical memory. That's it. The greatest advantage and possibly disadvantage of this translation mechanism, well of actually whatever translation mechanism we would use is that some of the translations might not exist, might not be available. So here in this entry over here for the page 111 is actually a special record that indicates that there is no valid address present here, so the translation doesn't exist. That means that when the computer, when the CPU tries to access this memory in this virtual address space, tries to translate this pointer into the physical address, it fails. It throws off an exception, it throws off a page fault which can be handled by the operating system. This may happened because of an error, because your program is doing something it isn't supposed to be doing, there is a bug in your code or something, or it might be intentional. Operating system uses these page faults, for instance, to allocate your memory lazily. So it gives you the memory in virtual address space immediately. But the physical memory is given to page Ï€ page frame by frame as you start using these pages, these frames. So this is one option, one thing that can happen. OK, this is like, this was like a few things to wrap up and this is a real mechanism. This is just the theory because we agreed that this page table would be huge even for 32 bit address space, this page table would take megabytes and that's too much for a normal space. That's waste of memory, especially that for normal applications, for small applications like the applications you're trying to write during your programming courses, using 4 gigabytes of virtual memory is unthinkable. You don't really need 4 gigabytes. So for small applications you need only few megabytes of memory and it will be very very wasteful if you need to allocate this whole page table. So one option you can do is to compress it and compress it using a hierarchy. By hierarchy I mean you chop basically the address, the page address in two. So instead of having 120 bit index into a page table, we split it in half and we have a 10 bit index into the first level of page tables and 2nd 10 bit index into the second level. The benefit we got is that this 10 bit index will require only a page table that holds 2 to the power of 10, meaning 1024 records. And assuming that this is a 32 bit architecture, then one record will have 4 kilobytes, 4 bytes. So one KB of records have 4 kilobytes. Accidentally, incidentally, or by design as you wish. This fits exactly one page. So it's like very well, very well designed so that this page table also fits one page. That's like a benefit. And this first level needs to be always there because that's like the main translation mechanism. And by the way, where this page is located, where this page table is located in memory is stored in a special system register. It's like a hidden register not available for regular applications, but it can be written by the operating system. So the operating system may allocate A-frame and say, hey, this is this is the first level page table for this process, OK. And then whenever this process runs, it makes sure that the corresponding CPU core has this appropriate address stored in this hidden register. OK, so this is like a first level and the second level and there comes the benefit is that the second level doesn't have to be there for all the pages. So basically the translation goes that we find the first translation for the first level in the first page table, then we go to the second level. This is a physical address or a frame where the second level page table is. Then we perform the same translation using the second-half of the page index, finding the appropriate value in here. And here finally resides the address, the physical address. The dark blue is physical address of the frame where the actual data are, where we are trying to locate our data during our translation. So it's like a 2 step process instead of one step process. And as I said, the benefit is that not all the second level page tables need to be there. Yeah, if we allocate all the second level page tables, there will be again 1024 of these four KB entries. So in in, in fact, we will lose lose some memory because we need this extra page, this extra table. So if we need to allocate the entire space, we would waste 4 kilobytes of memory. Yeah, that that's a small price to pay. But if we don't use the entire space, we can omit some of the second level page tables and save space. For instance, if you need only few kilobytes, you allocate only few of these page tables or even even A1, even only one page table at the second level. By the way, if we have only single page table at the second level, how many, how large memory, how large memory block can we allocate using that? Can we administrate if we have only? There is only always only one page table at the first level. That's how it's designed. But let's say we have only the first entry here filled and only the first entry points to the valid A second level page, a table. So there are only two one first level and one second level page tables. How much memory can we administrate using only these two tables? kilobytes means one single frame. So if we have one page table at the second level, we can have no OK, let's go through that step by step. So we have one entry at the second level. How many how? One table at the second level? How many entries? This table has 1024. Yeah, it's it's there is a hint over there. So we have 1024 entries. That means we can address 1024 pages or frames. How much memory can cover 1? And it's it needs to be continuous memory in the virtual address space because these int entries cover continuous indices at the second level. So it's a continuous memory in the virtual address space. Yeah, it can be sparse in the physical address space, but we are we don't care about that. So how large portion of the virtual address space one table actually covers when it covers 1024 entries of pages and each page takes 4 kilobytes? megabytes? megabytes? * 4 kilobytes is 4 megabytes. So basically if we have only one, this second level table, it covers its spans over 4 megabytes of continuous virtual memory address space. And actually it's aligned because it's like these values are all zeros. So the other way around, I could ask how much memory could you address if we utilize this part of the address? So if we use 20 bits of the address, 22 bits, sorry of the address. If we use 22 bits, we cover 4 megabytes. That's the that's the same question just from the the opposite side Ask. OK, so if everybody understood that, then you probably hopefully understand how the translation through the page tables goes. OK, I'm sure. Did we cover this exercise last week? I'm sorry, did did we go through this? OK, so let's go through this quickly or in a decent pace. I'm trying to rush things up a little. So we will finish in a due time. So let's just have it. Let's just have this these assumptions. So we have a 32 bit address space. We use this two level paging which you just saw in the previous slide. We use 4 KB pages and let's assume that the integers size of the integers 4 just so we have all the all the bases covered and we execute this piece of code. OK, so basically there is some pointer to some data somewhere. And we have some variable not very important. And what we are doing is that we traversing this data, we are actually this is like an array, array of integers and we are accessing 2000 of these integers in a continuous memory because arrays are always continuous in C or C So this is like a continuous block of 2000 integers and we are reading them all and just adding them to some sum. This actual operation isn't important. We are focusing on memory operations right now. So let's assume that we are executing this piece of code. What we don't know actually is where this pointer points. It can be anywhere in the virtual memory address space, right? We don't know what this memory is. It can be on the heap, it can be on the stack, it can be in the global variables. I don't know actually, but it doesn't matter that much. I have no assumptions where this pointer point, I just know it's somewhere in the virtual address space. And yeah, because it's an array of integers, it needs to be aligned to the size of an integer, but that's it. I don't know anything else. So yeah, this address is divisible by 4 because integer has four bytes, but that's it. I don't know anything else about this address. So the questions how many data pages are actually read are actually accessed by this operation. We are reading 2000 4 byte indigers. I know this is going to be wrong, but it might be about 2000 well pages. OK page is 4 kilobytes. So 2000 pages would cover roughly 8 megabytes, a little less than 8 megabytes. So that's a huge 2 pages will cover 8 kilobytes and we are reading 8000 bytes. So you might be right. It's not not we are at least in the in the right order. So, so one possible question is 2 pages, would everybody agree or is is it wrong somehow? Everybody agrees with with your colleague in the first, in the second row, volume up, please. I don't hear right now. First, we are focusing on the data pages only. So at the moment, the first question is we are ignoring the translation process. We are just asking how many data pages we are accessing. You agree with two, OK, Everybody agrees with two, OK, I don't, sorry. It might be two. Sometimes it's 2, but sometimes it's not. Well, if you have the data array perfectly aligned with the pages, this is the memory. OK, where this is going, If it's misaligned, then we might be looking at the. Yeah, that that's, that's the. That's my point. So let's just assume that these blocks are pages, right? So one option is that the data structure, the array starts aligned with a page, so it starts where a page starts. In that case, it will take 2 pages, slightly less than two pages. bytes will be here to spare, right? But it also might happen that this data array starts over here right before the end of this page. In that case it will take it will access this page because there will be small amount of data here, then the entire page afterwards, and then some portion of another page here. Probably not. Not entire page, there will be slightly larger trailing space over here, but it will access three different pages, right? So when you when I'm asking these questions, you need to ask yourself how this data pointer can be or cannot be aligned. And if we don't know anything, you need to assume the worse. So if we are asking how many data pages, the answer would be two or three depending on where the data pointer actually points to. OK, so you weren't completely wrong, you have half of the question right, but you need to also consider bad situations or other situations where the pointer might be slightly differently aligned. OK, another question, what is the minimal amount of page faults? So if you have an optimistic scenario, how many page faults will this operation perform? The most optimistic scenario is that everything is there, all the data are allocated, all the translations in the page table are filled properly, all the frames are there, everything is fine. So normally, and this is actually what happens in in. In normal situations, if your code runs for some time and all the data are already cached and everything, then you usually experience 0 optimistically. If everything runs smoothly, you usually have zero page faults. So that's not a very interesting question, right? So the interesting question is what's the worst case scenario? So what is the maximal amount of page faults? The pessimistic scenario? What's the worst case that might happen? Hypothetically slightly different 2000 times and it can completely be run by all. The word is 8000 accessing each individual byte of the integer. Well, yes and no. Well, let's, so let's assume that the architecture reads the integer and as an M1 operation. So yeah, there are architectures that actually reads data byte by byte like Spark. But usually normal architectures like MIPS or X86, which you were presented in these classes read the integer as a one operation. So the integer is read as let's say one load. I've got, I've got a 3rd guest then OK, the maximum number of page faults is 1 because it holds the code. No, I I mean no, no, no, no. Actually assuming that each page fault is duly, let's say, handled by the operating system. Otherwise the the the question has no meaning. So yeah, the page fault will like trigger the operating system that needs to somehow handle the situation. And I'm assuming that the data are valid, like the data are in the properly allocated part of memory. It's not like that you are you have a new pointer or or some invalid pointer that points somewhere to the unallocated part of the memory. In that case you were you would be correct because in that case the data, the first access to the data pointer will cause the the operating system to kill the application. So yeah, but that's not very interesting scenario. So let's say just this code is valid, it it runs properly, it will perform what we are expecting it to do. We just assume that in the background actually transparent to the user. This happens on the background user doesn't actually know it. You might read some statistics from the operating system, like how many page faults happened during some time or so. But technically you are ignorant to this effect. You don't care about how many how many page faults are there during your execution. If you run something in Python, you don't care how many page faults will happen. But now we are studying operating systems, so we are interested in such stuff. So we would like to know how many page faults might might happen. So 1 isn't the answer I was looking for, and 8000 is somewhat or 2000 is somewhat. Yeah, technically that might be possible, but again, it's not completely accurate. So if I want to be accurate, the right number would be Infinity because I don't know if there are enough frames in the system it can accommodate both the data and the code. So what might happen is that the data are there, then when I try to load the, the code is there. When I try to load the data, the code got swapped away. So technically I cannot do both. I cannot fit this code in the memory and it will end up in a. Another thing that might happen is that every time I try to read the data, it will actually swap something from the memory and then during the swap this thread this this process will be paused, another process will kick in, it will then swap again the the array swap memory. So it will be like trashing it will only swap the memory. So the worst, yeah, so but this is like just a hypothetical scenario. Infinity is again not very, not very likely answer, not very interesting answer. So let's let's just rephrase this. What is the I can I can rephrase it in this way? How many distinct pages may cause page 4 in the worst case? So let's assume that each page can fold at most once we have enough frames to accommodate the pages if we need to. Because if we don't have enough frames, then we are doomed anyway. So let's let's not dwell on these awkward scenarios. Let's assume that we have enough physical memory, just it happens that the process was sleeping for a long time or something, so the the memory isn't swapped in or isn't allocated here or something. So we need to properly fill in all the frames. So how many distinct pages may cause the? That's the the most interesting question, worst case scenario. It's not three or one. We are assuming the worst case scenario, so the the largest amount. So if you have like multiple numbers, it will be the maximum of them. So you're saying 3. So yeah, three are for the data pages. But now I'm asking about all the pages. So yeah, we have three data pages. So let's add 3 to this number because we will definitely need three data pages that we need to read. That's it, yeah. But we might need more because there is this translation process. We have two level paging which might also require some extra pages to be loaded. Yeah, if we have two level pages, that gives us two additional like pages that eventually have phones, exactly because the first level table is always there. It has to be loaded. That that's like a rule because if you don't have the first level there, then you're doomed. It won't work. So we don't count the first level, but at the second level. The second level of pages might be swapped away or might not exist yet. So we need to create them, allocate them. The operating system needs to fill them in if they're missing. So how many these second level page tables we need to cover these three data pages? We know that these three data pages are in a continuous region. The area is continuous. So these 3 pages are one after another. They have like incremental addresses, incremental if you chop off the offset. The maximum is what we are interesting in. Yeah, hopefully one because obviously 3 entries would fit 1000 entry page entry table. But in the worst case you are right. If this this page requires the last entry for instance of this table, then these two will be in another table. So yeah, 2 is the is the right maximum. So we need three data pages and two second level pages. Anything else? These situations are always more complicated. Yeah, I know, I know. I'm I'm asking questions like which are somewhat embedded with hints. Because when I'm asking these stupid questions, I obviously the code itself. Exactly what about the code? We need to ask the questions about the code as well. So is the code present? We made no assumptions about that. So theoretically this code also may not be present when we start executing it. It the whole application was swapped away because it was sleeping for a long time, for instance. So if this code is also swapped away, how many pages will be required for this? Again, maximum is the number that that's interesting. So yeah, this is this seems that this would be definitely more than one or two instructions, right? So if this is more than one or two instructions, it may span over again, because if the one instruction is at the end of one page, it can span over 2. And then again, if you take a good look at it, it won't probably take more than one page of code, right? This, this won't be like thousands of instructions. It's just a simple for loop and, and some addition. So yeah, this might span over two data code pages, and two code pages will require how many again? , because in the worst case scenario, the first page may be may have entry here and the second page will require another table. So four additional pages for the code. Yeah, you need to ask this question kind of separately because there might be an assumption that the code is already in the memory. Yeah, if the code is already in the memory, then you're counting only the data or vice versa. And again, I would ask these questions only in situations when it's obvious how large this code could be because yeah, we have no assumptions regarding the compiler, etcetera, but that this code would be larger than one instruction and and it it won't probably take more than 1015 instructions. That's kind of thing that should be clear to everyone. OK, so that was like a few questions. We get to few of these questions at the end when I was when I will talk about how the how the test will be conducted. OK, what if we have a larger virtual address space or even larger physical address space. So if we have more bits to spare, more bits to allocate, then yeah, we need more levels of the page tables. Actually this is what they do in the AMD when they were extending the XA6 architecture into 64 bits and the best solution they come up with was to add two additional levels. So the principle is all the same, we are just adding levels. Yeah, we need to chop the address into more, more segments, more fragments. And yes, we cannot possibly accommodate it all. So technically the 64 bit architecture, at least this architecture is not entirely 64 bit architecture. It can only take first 48 bits of the address and translate it. So yeah, you you're being cheated. We are telling you that we have the 64 bit processors and 64 bit address spaces. But no, in X86 ext to 64 bits there are only 48 bits actually usable for the process. So what are the nothing you can't use? Well, you can use them for additional stuff, but they're like ignored or something. So you can use them for the address. And another thing that changes is that we are using only 9 bits for each entry. Well, The thing is that we are still keeping 4 KB pages, so we want each table to fit one page. Because it's much simpler to handle. Technically this is not like a hard prerequisite, but it's like it's a good one because if you have, if you manage to fit each of these tables exactly into one page, it's much more easier to manage. So we need each of these tables to fit 4 kilobytes. But then again we are using 64 bits for addresses. So when we are using 64 bits for each address, we need to extend these entries. This entry cannot can no longer stay small enough to fit 4 bytes. We need 64 bit entries. Well, not entirely 64 bits we can. We have some bits to spare, but we won't fit it into 32 bits. So we need to double the size of the entry. And we if we double the size of the entry and we keep the whole structure of the same size, we need to half the number of items in this in this page. So technically we are not using 1024, only 512 of these entries and each entry has double size. Each entry takes 64 bits, 8 bytes OK, but everything else is the same. And by the way, how large is each entry can be easily inferred from the bits the architecture used. If we have 32 bits, we can use 4 bytes. If we have 64 bits, we need to use 8 bytes. It's pretty straightforward in this way. But again, this is like a thing you can derive or sometimes you must derive from the knowledge of the architecture. So this is what happens. And that's also why modern architectures don't want to rely on this translation process. And it's completely essential. There are some caches like TLB the translation look aside buffer which actually caches the translations. Because in 64 bit architecture 1 memory translation will go through 4 distinct tables. So additional 4 reads to the memory needs to be performed to translate 1 virtual address into physical address. And that's a lot. So we need, we need these caches, we need the look up side look aside buffers to keep all this data cached. So this this translation isn't performed every time with every access, but only when you're entering a new page, basically. OK, so this is just an example of of a real actual translation mechanism. This is taken from a manual of the AMD 64. And let's move on a few more things to finish. Yeah, this is a formalization of the process and a few more additional information. So yeah, the steps I've already depicted to explain in detail on the graph. So let me skip some points. As I mentioned, the caching is very essential. So basically the first step would be let's check the cache if the translation is directly there. Note that the in the TLB, of course we are caching only the virtual, only the the page part of the virtual address. We are chopping the offset as a first step even before we try accessing the caches because it would be wasteful to add the offset into the cache that that's, we don't need that only the page index part is being translated. Then we go through the page tables. If we miss, if we miss the TLB, we go through the page tables again. The page tables themselves can be in the regular caches. So again, caches might help. And note that in these tables there are some additional bits. I haven't spoke, I haven't spoken about it in in a detail. But if we check, yeah, if we check this, then you might note that the translation itself takes the translation entry itself takes 32 bits in this in this architecture or 64 bits on 64 bit architecture. But only first part, only the 1st 20 bits without the offset is actually required because the offset is provided from here. So we need only the 20 bits of the physical address to be translated. So we have 12 bits to spare here, right? So for what? We are using these bits. So first we need a valid bit. That's very important. We need some bit that will indicate whether this entry is valid or not, whether this translation exists or not. Yeah, whether this is a valid value. So there is one extra bit, one bit which indicates this is a valid value. This is an invalid value. And also we need some additional bits, not. Most notably, we take the excess bit and dirty bit. Yeah, and the excess bit says it's updated every time the page is accessed. So it's set to 1 every time this translation occurs, and dirty bit is set every time a write operation occurs, so every time a modification to this page occurs. And this is done automatically by the hardware. And finally we assemble, we concatenate the offset and the and the physical address which is translated. Yeah, this is just the formalization of the translation. I can skip that for for time reasons. I've already explained that on the on the on the diagram. So I hope everybody got that. If you don't, then you have it one more time spelled out on the on the slide and what happens during the translation if something goes wrong. So if something goes wrong and what, what might go wrong? The most often, most typical thing that goes wrong is that the invalid bit, the valid bit is set to 0. It's the the entry isn't valid. So the translation fails because somewhere during the the the look up process in the page tables it comes across an entry which is not valid so it cannot be translated. So what happens is that this the fault is risen so the then exception within a CPU is thrown and the handler of the operating system is executed. So the CPU switches into the kernel mode and starts executing the routine that was provided by the operating system. And this routine, the 1st order of business, it is that it needs to determine why this happened. Because yeah, one option, as I mentioned before, is that something bad happened in your code. You are trying to do something you're not supposed to do by an accident, by design, it doesn't matter. So the operating system is there to protect you or in this case, maybe to protect the other processes from your process. So technically, if an unauthorized access is detected, you are trying to access a memory where you shouldn't be accessing or. This might be also happening when you're trying to write into the read only memory. That's another flag that might be there, mark some memory read only so it cannot be written. This is usually used for the constants or for the code because you don't want to overwrite your code once the application is executing. Also we have the execution bit, so the pages that holds the code are marked with execute bit, so the instructions may be loaded from them, but the data are have this bit set off so the data cannot execute code. So these are types of errors that might happen and if they do the operating system has no option but to kill your application. Usually you get some report, some you might get some dump or something. But technically if your application, if your command line application ends up with segmentation fault or Linux, it's signal 11. If you end up there then this happened. You are doing something wrong with your virtual memory. You are trying to dereference a wrong pointer. Another option is that the translation is actually valid. The address should be valid, but the translation does not exist yet. So the operating system was lazy or have had some reasons on its own why this translation isn't filled in yet. But it's a valid address, it should be filled in. So in that case, the typical 2 examples would be one that I've already given you. You've allocated a huge block of memory, but this memory is given to you only at the virtual level. You don't really get the memory physically. And the operating system uses this mechanism to slowly, lazily give you the memory as as as it goes, as as you access the pages. Because technically it might happen that you allocate one terabyte of memory. It's not forbidden, right? But you don't want to access it all. You just want to have a good feeling that you like own one terabyte of memory and in the end you just access a few few pages. So only these few pages will be actually assigned a physical frame where they will reside and the rest will be like virtually there. You can theoretically access them if you want, but if you do, you will be provided the memory up to the point when you run out of the memory. But if you don't access it entirely, it might work. Your application will run. Another possibility is that the pages were swapped away so they were offloaded to a persistent storage, like to the hard drive, so that we have more room in the physical memory. For instance, your application wasn't running for some time and the operating system decided it needs more memory, more physical memory for other processes. So it takes your pages and saves them to the hard drive and mark their places as not mapped because they were swept away. So when this happens, the operating systems say, Oh yes, I've just moved these pages, move these frames out of the main memory into the hard drive. So yeah, now, now it's time to load them back. So what happens here is that the memory is provided either as a newly allocated memory, as a new free frame, or it's loaded from the hard drive, from a persistent storage, for instance. So the operating system performs this operation for you and when it's when it does it re executes the instruction that causes the fault. So your applications kind of transparently gets the memory where it was, where the operation where the data operation was trying to read or write the data. Yeah, you may theoretically detect these operations of the of the operating system. By measuring time, for instance, if you carefully measure time, you might detect some discrepancies, like when this happens often, frequently, then your code is kind of running slower than it should be. So yeah, there are ways how to detect that some something is happening. But otherwise, besides timing, this goes completely smoothly, seamlessly, transparently for your application. So you don't want to detect that it happens automatically by the operating system. So yeah, when this happens, the operating system needs to find a new frame, create the mapping and either load the contents of the frame from the hard drive or fill the frame with zeros or something if it's allocating new memory. So it will give you the data if you give you the memory and construct the corresponding page table entries, etcetera. The tricky part is that we need to find a free frame. Yeah, I told you before that we have enough memory. So we have enough frames. Yeah, usually we do, but sometimes we don't, and sometimes we run out of frames. And what happens then? Well, this is the easy part. There is one frame waiting for us, so we just take it. But it might happen that all the frames are being currently used. By the way, there is a saying, if you're not using 100% of your physical memory, then you're under utilizing your PC, right? So the operating system actually is trying to utilize as much frames as possible for various internal things. Like the operating system might cache various files from the hard drive in the main memory. They may not be that important. So every time this happens, it just dumps some of this cached data from the hard drive, throw them away and reclaim the frame for your process. But technically, by the way, sometimes try to run top process on your Linux and you usually find out that there is a very little amount of free memory. Most of the memory is being used. Sometimes you can see that most of the memory is being used for IO buffers. That's what the system is doing. It's allocating the memory for itself so it can cache data from the hard drive if the memory isn't used. And that's a good thing because you are actually utilizing your hardware to its full potential. But when this happens, we need to find the victim for swapping. And if this happens, we need to use some algorithm for replacement, save the victim to the persistent storage, or possibly throw it away if it's if it's no longer used, and then remove its mapping from the TLB. That's again something that might happen if you if you're trying to swap in things. You also need to take care of the caches. Some some somehow dump the TLB entries so they no longer contain invalid values. OK, few page replacement algorithms. By the way, these replacement algorithms work for other things as well, for instance for cash cache replacement. So it's like every time you have a limited resource that can accommodate only copies of few things that these replacement algorithms can be, can be used. Actually there is something called optimal page algorithm. It replaces the page that will not be used for the longest period of time. Yeah, this is a future tense. So if we have an Oraculum, if we have some divine power that can tell us which of the pages, which of the frames which are currently allocated will not be used in the future for the longest possible time, then that's the best victim to choose because we will run for the longest time without actually needing it. And that's something we cannot do. So it's like a hypothetical, it's a theoretical algorithm. It's like an optimum towards. We are trying to strive, but we are the only thing we have is the past data. So we are trying to assume from the past data how the pages will be used in the future. So it's like kind of like reading the cards or tea leaves or something. OK, few examples how this works. Very, very simple example. Let's let's utilize the excess bit and let's have a like an arrow, like a clock. So we organized all possible frames in a circular fashion. And this arrow points to always points to 1 frame and it moves as a clock. So it moves as it goes. And every time it encounters an sorry, every time it encounters a page sorry A-frame with access flag set, it resets this flag to zero and moves on. So it resets this flag to zero and moves. And once it finds a page A-frame with 0 flag. So it this frame wasn't accessed, this page wasn't accessed recently because it already has its access flag set to 0, so it takes this one. So it tries to take the 1st frame which has a page with a set to 0 because this page wasn't used recently. Actually there is an extension of this algorithm called not recently used replacement algorithm by recently. We just have some flex that indicate whether a page was recently in some small period of time in the past, whether it was used or not. In this extension we are using not only the access flag, but The Dirty flag as well. Because it's a good idea to distinguish whether the page is only accessed for reading or whether it was actually modified. Because when it was modified, it probably needs some write back. It needs to be copied back to the hard drive or something. So technically we just divide the pages in four classes based on these two bits. Yeah, quite easy actually. We just take these two bits as values. And yeah, these are the classes. So if the page wasn't accessed for some time, there are two lower classes for the access pages, there are two upper classes. And The Dirty flag is like the helping deciding factor which class we will select. And the algorithm is very simple. We are trying to take a random page from the lowest occupied class. So first we are trying to look at are there any pages which with both flags set to 0? If they are, we just pick random page, random frame from this set. If not, we are we move here and we're trying to pick a page from this set. If not we are moving upwards. And finally, if there are no pages with these flags set to 0, we will pick one page which was recently accessed and recently written. It's not much, but it's honest work. OK, least recently used is a slightly more difficult. Again like the previous algorithms, it tries to derive something from the past. And what is this? It actually, it tries to prioritize the page that was just actually right now used. So technically what we can do is every time a page is accessed, it's moved to the front of some queue. So you're maintaining a ordered list or a heap or something of of pages. And every time one page is used, you move it on the top. So you know that on the top there is the most, there is the least, the most recently used page. And on the bottom, on the end of the list there is the least recently used. So if you maintain this list at the end you will find the least recently used page. The thing is that this is very complicated. I'm not sure if you ever implement this algorithm. It's very useful algorithm. It's used in compression, for instance, but it's difficult to do so because you need to find the page in the list or in in any other data structure. Then you need to take it from, extract it from the list and then move it to the front. Yeah, it's it's somewhat difficult. So there are some approximations that can be used instead of that. But still this is not very frequently used. Sorry, sorry for the word. This is not one of the first choices because it's quite complicated. There are some some versions how to implement it using some caches or some bit matrices in in hardware. But again, this is very complicated. Another quite complicated but slightly less complicated algorithm is not frequently used and by frequently notice the change in the word. The frequently indicates that we are trying to compute some sort of frequency how the pages are being used. So some sort of a number that will indicate whether the page is used frequently or less frequently. So it won't be a simple bit, it will be a number. And this number is usually derived or somehow computed from the excess bit. So every once in a while, periodically, this process scans all the pages available and all the pages which has this bit set to 1 have this counter, this frequency counter incremented. So basically this bit is kind of added to the to the frequency counter. And when it's added, it's always cleared. So they take the bit, they add it to the counter and then they clear it. So the excess bit, by the way, the excess bit may be set to 1 multiple times. The page might have been accessed multiple times before this, before this two epochs of this. Actually happened. But this is like accumulated like a aggregation of the excesses. We are only concerned whether this was the page was accessed. We don't know how many times during this period of time, but there's like an aggregation of information. No problem. And when we try to find the victim, we are trying to find the victim with the lowest counter, lowest frequency counter. So that's the page that was in the past least frequently used. Again, this is kind of complicated because you need to scan all the pages and take all the counters and find minimum. But still finding minimum is somewhat easier than moving a page to the front. So it's, it might be easier in some, some cases, but then again, it's again, it's not ideal. And also there's one problem that needs to be solved because if you have a page that was very frequently used at the beginning and then it was, it will be used no more, then it will create a problem because this page would remain in the memory forever because it has a huge frequency counter. So we need to introduce aging. So each frequency counter needs to be somehow decreased over time. And what happens is again, periodically, usually with a greater. Than this one, we somehow decrease. For instance, we can divide them by two because that's a simple shift. That's a like a very easy operation to perform. So there's another algorithm that can be used for a page replacement. A few more things that can be derived from paging. By the way, if we have the memory virtualisation, sharing memory between two processes is like a thing that is already implemented there. You just need to properly set the page tables. So if you need the 2 processes, 2 applications share some data, easy thing to do. You just make sure that the page tables, the translation process points to the same data in physical memory. So if you want that application one in the virtual address space one share some data with application 2, the only thing you need to do is that you create appropriate mapping for the application one and for the application 2 so they both access the same frames. That's it. Nothing else needs to be done, OK? So it's like a very easy, if you want 2 applications to share a data structure, this usually happens automatically, transparently for the code. Because usually if you run a piece of code in Linux, let's say, you definitely need some libraries like libc at least. At the very least you need libc. So this library is required by every application. OK, no problem. It needs to be in the physical memory only once. And every application creates this mapping to this part of physical memory and that's it. So it can be shared among all the applications. Yeah, we need to take some precautions. For instance, it should be marked as read only for all the applications. Otherwise one application can rewrite the code for another application. That's really bad, but that's the only thing. We need to make sure it's there, but otherwise it's pretty straightforward. Otherwise, the translation mechanism, which is embedded in the CPU will take care of all the technical details of this process. So it's just a matter of proper setting. Another thing that we can do is memory file mapping. What's memory mapped files? What happens here if we take part of the virtual address space and we say, hey, I'm saying that this part of virtual address space actually mirrors a file on the hard drive. Again, this is just something I need to tell the operating system. And the operating system will ensure that every time I'm accessing this memory, it will load the appropriate data from the hard drive to my memory. So it will automatically allocate some data frames, load the data from the hard drive to these data frames, and provide appropriate mapping so I can access the data as if I'm just reading them from the file. And vice versa, I can modify this memory and the operating system will ensure that when time comes this data will be transparently written back to the hard drive. So it works. It might work both ways actually. This mechanism is like one of the most frequently used mechanisms in operating system, because even if you're trying to read the file using the old fashioned F open and F read and other operations, somewhere underneath the operating system will perform this, at least on the operating system level or somewhere in some library you are using on top of underneath your code. So this will always happen. There are no other mechanisms used nowadays in modern operating systems, so it's always there. By the way, if you're allocating new memory, it's also handled using this mechanism because there is some swap file or some helping files for memory allocation. So this mechanism is always there even if you don't know about it. Few more things to wrap up the operating system. So yeah, before I have these few final slides about virtualization, are there any questions regarding memory translation, memory virtualization, memory management? I just missed a lot of stuff like we'll have to rewatch the. Well, yeah, hopefully you will do it in a due speed so I you won't waste too much time on it. But then again, everything is pretty straightforward. So if you just run through the slides, I hope that everything will kick back to you and you will just refresh your memory because it's not that complex. To be honest. It's not that complex. It's pretty straightforward. You just, if all the pieces fall into place, then you'll be fine. There are no special complexities like in mathematical analysis. You know where you really need to know the proofs and everything. No, this is quite straightforward if you just think it the right way. About the computers. OK, a few final slides about operating systems. This is a very huge topic, so I'm opening it only for a few moments. And the modern operating systems, modern systems needs to employ some sort of separation which goes beyond normal memory control. So we need to separate individual applications, individual processes into more secure containers, more secure sandboxes than what the regular memory virtualization will do for us. So what we are, what we can do is that we introduce virtual machines or virtual containers into the operating systems. Just to clarify, virtual machine is a machine or like an illusion of a machine which is emulated inside your regular hardware machine. How this can go well? Imagine the easiest thing you can do is that you take your Linux, Windows, whatever you're using and you install application like VirtualBox for instance. You can do that for free and it's like a regular application. But this regular application will create something like a virtual PC inside your regular PC and you can run it. It looks like you're running an application with one window, but this window is actually emulation of the screen of the PC which is inside. And you can install a different operating system for instance inside. And the whole disk of this virtual machine is stored as one file of your regular operating system. So technically, you created an emulated PC inside your regular PC and you can do whatever you want in, in this virtual environment. I mean, you, you can destroy it without worrying about too much because the only thing that happens is that you, you just erase one file from your, from your regular host system and you erase the whole virtual machine. It's very easy. So if you need to do some experimentation, for instance, a few weeks ago, I need to set up some Chinese hardware, some reader from China, which was very cheap. And it has the software also made in China, which, which can set up this, this device which can configure it. And this, this software was distributed only as a very small exit file, which I needed to download for a very suspicious website. So at this point, yeah, no problem. I just run a virtual virtual machine. And in this virtual machine, I have no, let's say, anxieties about running this piece of software, because if anything goes wrong, I just delete the virtual machine and my original system will be intact, it will be fine. So this is 1, let's say, use case for which we can use the virtualization quickly, serious and intent, but maybe not so much the subject. So it's obvious to me that you could put a virtual machine inside of another virtual machine. About how many layers deep could you go before? I think theoretically infinite, but you're limited with memory. So this virtualization takes some overhead. So in the end we will run out of memory because yeah, you need some extra memory to manage this virtual box. So if you run a virtual box in a virtual box in a virtual box, then in the end you will run out of memory and in the end it will be unimaginably slow. So if you do just one layer, you're fine because current hardware has a huge support for virtualization. So if you're trying to run this VirtualBox as I just described, what really happens is the D applications running in this virtualized operating system run in the same way your applications in your regular operating systems run, meaning they run the same instructions they are. Only the kernel course are redirected to the virtual operating system. So only the kernel mode of the CPU needs to employ some additional security layers and current CPUs allow that. So they allow you to run in a virtual kernel mode. I'm not being very precise, it's slightly more complicated, but technically you are like, have a regular operating system and then you can run a virtual kernel mode of the CPU. So it runs almost as fast as the regular instructions, but certain operations are intercepted and the the driver or the the execution control is turned over to the original operating system so it can emulate the virtual operating system or it's it's operation. So it's quite fast that that's my point. There is a huge support in that. In hardware, if you go into the nested levels, you are losing some of the support and in the end if you go deep enough you might end up with complete emulation. You can, you can always do complete emulation. There are tools for that like QMO, which actually allows you to emulate a CPU completely. Like the instructions are executed in the virtual machine, right? But that's very slow because for executing each virtual instruction you need to perform 10s or even hundreds of regular instructions. So there is at least one order of magnitude slow down for that. You don't want that. You want at least your applications to run almost on par with real hardware, so that each regular instruction in your code in application code is really one instruction on the CPU. Not entirely, but almost. I guess a rough number on let's say I have a laptop, normal laptop, about how many layers I, I don't know, I, I think you can do safely 2 levels, 2 levels isn't too much to ask for. So I think on a regular laptop, even on a regular laptop, you probably will be fine with two levels, but I don't know. And actually who cares? I, I really don't, don't need more than one layer for me. So, and, and most people don't. So yeah, 2 levels are maybe sometimes, sometimes you might need 2 levels maybe, but that's it. Don't worry about something that might be intentionally trying to get out. No, no, yeah, we can go to security, but maybe another time because that's a huge topic, but it's it's I wouldn't say it's impossible, but it's unlikely. OK, so this is one way to go. We can have a full virtualization, complete virtualization of a of a complete CPU. By the way, we do this all the time with the servers. We have this virtual infrastructure in our school of computer science. And most of the servers you are accessing, for instance, if you're accessing our web page, the web page runs on a some web server which runs in a virtual machine on this virtual infrastructure. Because they're only like, I, I think 5 servers, but these 5 servers are hosting at least 100 or maybe more than 100 Now, real virtual servers, because most of these servers are web page servers or similar similarly trivial things. So they don't need that much memory, They don't need too many CPUs. So we can manage to squeeze them on five bulky, but still only 5 blade servers. Another option is a process virtualization. We don't have to virtualize the entire operating system, but still we can benefit from virtualizing single applications. This is what happens if you have a special platform, Java or NET does this. So if you compile your Java code or if you compile your C code, you won't get real instructions. You will get byte code or you will get Silk code, which are virtual instructions designed for this virtual Java virtual machine or for this NET CR. So they are designed to run in a virtual environment. There is an application, actually the Java VM. Java Runtime is a virtual application that is taking this bytecode and executing it 1 instruction at a time. And then again, there would be a huge problem with performance because if really one bytecode instruction would be emulated by this virtual machine, it will take at least several, maybe 10 instructions. So there will be like an order of magnitude slowdown in the execution. We don't want that. Actually, this is what happened when you when you, when you try to execute Java One or Java 2. If anybody happened to know Java 1, I don't think so. It's like 30 years actually. Yeah, it's 30 years ago, almost exactly. So yeah, that's very, very slow. So what happens is that they try to compile this byte code into regular instructions, but they usually do it on demand. That's what this JIT goes. This JIT is just in time compilation. So it means that as you execute your code, the pieces of code for you are calling a new method in Java or in C#, it doesn't matter. So what happens is the the internal runtime says, hey, I, I haven't seen this method before, so let's compile it and it creates the the compilation on demand. It provides the real instructions on the target platform for that particular method. And then it injects it somehow into your runtime. Yeah, it's it's crazy, but it works. Actually JIT was originally used in Java compilation and still being used in Java and C# prefers AOT which is ahead of time compilation, which means when you're trying to run your C# program, the compilation happens all at the beginning. So, yeah, the startup of AC Sharp application takes a long time because at the beginning it takes the Silk code and quickly transpose it, translates it into the machine code. This translation is much faster than full compilation because it's just like one assembly interpretation into another assembly interpretation interpretation. So there's almost one to one mapping between the instructions. Yeah, please don't, don't tell this anyone to to don't tell this to anyone who actually do this type of translations because it's much more complex than I would just said. But but technically you can imagine that they are in the bytecode or in the Silk code. They are operations like add to numbers. So yeah, it can be translated into one or very few instructions of of the real CPU. And another thing you can do is containerization, which means that you instruct your operating system to create a sandbox, to create a small, let's say fence around some process. And this process is then executed in a safer manner. For instance, you can say, hey, this process is running only on the CPU, not accessing any other cores, or is given only this amount of memory, it cannot eat more memory than this. Or there is a virtualization of the file system, so the container actually is using only few folders from your file system or some virtual folders in your file system, so it doesn't access everything your computer has. And this is actually what is currently the most popular thing you're doing if you ever use things like Docker. You people have at least heard of Docker, right? Yeah, a few people are nodding. So yeah, Docker is a tool that performs this containerization. It's an application that helps you instruct the operating system how to wrap something into a separate sandbox so you can safely run it. And Docker has some additional features like the file system virtualization. So it allows you to run completely different file, file system or even completely different like a Linux distribution. If you have a Ubuntu, you can run completely different Linux distribution in this in this container. And it's also widely deployed in in industry because if you have a huge farm of servers, web servers or my SQL servers or whatever servers, you run that again, you want to deploy them easily with the same configuration, with the same libraries and everything. So again, you, you wrap them in a container and just deploy them on an infrastructure. And the infrastructure is only set up to run containers. You probably heard of Kubernetes, for instance. That's another buzzword. That's by the way, if you don't know Kubernetes, then there's a small reminder. H in Kubernetes stands for happiness. OK, so how this works, just a small visualization. Quick one, I'm, I'm really wrapping up. So this is how things run on a normal hardware. So you have a regular hardware, let's say say 6 architecture. On the top of the hardware there is the operating system. The operating system manages the hardware and provides the abstraction. Yeah, I'm closing the circle. I talked about this at the beginning of the operating system part. And the IT, it creates a layer between the hardware and regular applications. There might be many of these applications. Virtualization takes this and just moves it one layer upwards. So there is this virtualization layer which provides virtualization of the hardware for the operating systems. So technically the operating system isn't accessing the hardware directly, but through some virtualization level which allows some level of control. And it also allows a separation of multiple operating systems, multiple virtual machines running on the same physical hardware. So this is what what's it all about? Yeah, there are many things that needs to be resolved, like how we can virtualize CPUs, how we can virtualize RAM, etcetera. Some things are easier, like RAM, because we can just separate the RAM into multiple boxes and use the virtual address translation to properly manage the memory, memory separation. That's easy. With the CPUs, it's slightly more complicated, but as I mentioned before, there are mechanisms embedded in current CPUs that helps us with that. And with the disks or IO that's completely different, you usually need to virtualize them completely. So technically, if there is a hard drive over here, then hard drive over here is actually a file on some file system on this hard drive. So it's like you create a completely different abstractions for files or for iOS. OK, that's it. Any more questions regarding operating systems before we swap to the sorry, you mean this, this picture? What was the question? You can imagine you can, you can abstract that the hard drive is basically a huge, huge array of blocks, right? A file is basically a huge chunk of bytes, right? So if I have a file on a regular file system on my regular Windows laptop, I can say, OK, I just managed to store a file system of another operating system in this file. I just say, OK, so I just managed to allocate the file by blocks. It doesn't matter because bytes are smaller than blocks, so no problem. So I just allocate large enough file so it can encompass. It would act as a virtual hard drive for a different operating system. And then the virtual layer will just transcribe all the iOS that goes to the hard drive directly into this file and vice versa. That's it. It's not that complex in the at least if we look at the basics, it's not that complex. The hard part is how to do this efficiently and and how to manage this. The storage and usually the virtualisation doesn't require allocating the entire file beforehand. Because if you have like one terabyte hard drive, virtual terabyte hard drive usually not using the entire hard drive, right. So you don't want to have a huge one terabyte file over here which will be mostly filled with zeros. So there are some techniques how to compress this, etc. And they are difficult, but if you look at the basics, this mapping is trivial. You just store the data over file system, which is like a huge set of blocks into the file. That's it. Yeah, that's a completely different story. If you use something like a virtual box and you actually set up shared folder, it goes completely the other way around because it's like you need to run a specialized application in here and there is a special, let's say, API in this virtualization layer. And this application is somehow aware of this special tunnel. And underneath the virtualization layer will allow you to access some files from the real underlying operating system. And this application will create an appearance. They kind of are created in the virtual. So it's much more difficult, it's much more technical, but you need the help of the both virtualization layer and you need some virtual application or usually it's implemented as a module or as a driver in your operating system, which you need to install additionally in your normal operating system. Usually there are some virtual tools, virtualization tools you need to install in the hosted system. So you get this kind of support and then this virtual part of the operating system, this special part is communicating via some specialized API which is not normally present in the regular hardware. So this is much more complicated. OK, let me do this. Let me do it this way. Let's take a break now because I have only 15 minutes till the regular end of the class. So let's take a quick break now and then we will continue with another topic, OK? So if anybody needs to go to the restroom or something, please do now and we will resume in five to 10 minutes based on how fast it will be. So let's resume our session with parallel programming. That's the final topic. Yeah, unfortunately we need to rush it slightly. So I won't dwell on details, but rest assured that there is another, another course called Programming Parallel Environment where we can go through this stuff in more thorough manner. So this is just to give you a few glimpse about the most important thing that's happening in the hardware for for past 25 years. So you may have noticed that your hardware is equipped with multiple CPU cores. If you buy a laptop, it has multiple CPU. If you buy a cell phone, it has multiple CPU cores. So this is like the basis for this for this course, for this lecture. And you need to be aware of this because if you want to utilize the CPUs, this hardware properly and efficiently, then you need to utilize all the cores. And then you need to resort into the parallel programming, a few terms at the beginning. By parallel computing or programming, we refer to calculations or execution of code that really happens in parallel. So there are multiple execution units or processes or something that are carried out. They are computed, they are running simultaneously. At the same time multiple things are happening. This happens at multiple levels. You can do this at the bit level. Actually, the simplest example would be bitwise AND or bitwise OR. It's an operation that performs some bit manipulation on all the bits of one large word, right? It happens on a larger levels on instruction level. 1st is that there is a pipelining, superscalarity and pipelining within the CPU. So the CPU core is executing multiple instructions simultaneously. You've seen the examples of the CPU architecture so you know that there are like 5 front end decoders. 5 instructions are actually being decoded simultaneously. There are like 8 pipes, 8 ports with with different pipes. So there are multiple execution units that can crunch numbers at the same time. Also there are vector like instructions. We haven't talked about that much, but there are also specialized instructions that allow you to perform the same operation like addition or subtraction or even bit wise operation, etcetera on multiple registers or multiple data blocks or multiple values at the same time. And finally, most importantly, there is a task parallelism which allows you to execute multiple tasks, multiple pieces of code, multiple threads, multiple processes at the same time. And that happens thanks to the fact that we have multiple CPU cores, multiple execution units that are running independently within your PC, within your host system. There is also another term, concurrency or concurrent computing. This usually refers to the situation where there are multiple computations. Imagine processes that are available. They might run simultaneously, but we don't actually know that they are being executed, that the code is actually being computed by multiple execution units at once. It also is possible that you have concurrent computing without parallelism. So we have like a multitasking, but you have only one CPU. So it's like an appearance that they are running, that they are running in parallel, but it's only an appearance. Usually the concurrency is defined as a potential for parallelism and there are many things for consideration. Most important one is how this is happening. So in normal situations you've encountered in this class is that we consider all these processes, all these threats, all these units of scheduling that they are sharing the same address space. This is like one type of parallelism, but this is the only type we are considering in this class mostly. But sometimes you also may have a parallelism that utilize different memory spaces, like distributed computing, for instance. So now here we will focus on shared address space. That's more interesting because in shared address space, it might happen that 2 processes, 2 units of scheduling access the same data at the same time. And that creates potential problems that you need to address. That's why it's more important, more interesting. Usually we'll talk about threats because. Normally in regular operating system, in regular environment, this happens when you have one address space, one process, and within this process you execute multiple threads. So each thread can be scheduled on a different CPU core, but they are all accessing your one memory space, they are all accessing your data structures, all accessing the same heap. And the difficult part here is also scheduling, because scheduling is always difficult. We already covered this in separate class and operating system, so I'm not going into the details again. But remember, scheduling is always tricky because you don't know what the scheduler will do next. And currently the scheduling is preemptive. So at any given time it might happen that your process, your threat is being suspended in the middle of of of your code and another thread kicks in, another thread is scheduled on on the CPU. So the your threads may interleave their instructions unpredictably. The most pronounced problem that rises from the set up that you have multiple threads, multiple scheduling units running on the same data, running in the same memory space is called race condition by the word. By the way, the word race isn't from some racial issues. It's from the race as as a competition as if you're running or driving cars or something. So it's it's race like the the threads are competing who will first cross the finish line. OK, so just to clarify, this is not some ugly word. This is a regular word. And what happens here is that you have a piece of code, a data structure accessed by multiple threads, but these threads perform some operation which is divided into multiple steps. And it happens that if these multiple steps, if they are interleaved in the threat improperly, they may cause some damage, they may broke the data structure. By the way, just to clarify, we do have cache coherence. So in the hardware there are some mechanisms that prevent some very ugly situations from happening because the data are always accessed through caches. So they are always accessed with cache line granularity. Yeah, it helps, but it doesn't prevent some malicious scenario. So it doesn't prevent race condition. It's slightly helps, but it doesn't completely solve anything. And if we have a architectures like MIPS or like RISC 5 where there are load instructions and store instructions explicitly in the code, it makes this problem more pronounced because even a very simplest operation, let me give you example, let's just have an integer A and I perform a plus plus trivial operation, one line code of code. So in load store architecture, this actually translates into load, right? Let's say to register T1 to T0, it loads the variable a I'm writing this symbolically. Then there is some add operation like Eddy which performs something like this, right? Writing in MIPS. And then you need to store the value back. So there's something like 3 instructions. So it's not one atomic operation, it's divided into 3 separate steps, even something this simple. And the trouble is that after each instruction, the thread performing these instructions might get suspended, right? So imagine that piece of this code or piece of this code is executed by multiple by two threads. For instance, let me give you an example from the previous slide. So let's just say that two threads are performing this operation. We have a. Sorry, I haven't explained this. We have a linked list and the structure remembers pointer to the roots to the first element of the linked list. And this push front actually replaces the front node with another node. So it takes the root pointer, append it after the element being pushed, and then replace the pointer to the first element, right? Assuming that that the caller duly allocated this new element. So I'm not handling allocation here. So what happens if 2 threads simultaneously push two different new structures at the front? So this is the current state, this is the the initial state of this data structure and this is what what's happening. So we are having 2 threads, 1 is pushing A and another is pushing B. So one possible outcome would be that the first thread comes first. It takes the pointer to the current root, so the A next points to the current root and then replaces the root so it points to A. Afterwards comes thread 2 and does the same with B. So B takes pointer to A which is current root and then updates the root. OK, no problem. It might happen that thread 2 was slightly faster in the race, so it goes the other way around. So first the thread B, thread 2 will push the B and then thread one will push the A. However, both outcomes are valid. We don't know which which threat comes first. There is no guaranteed order. So both outcomes are fine. We are counting on this. This is like both outcomes are desirable ones. If we need some form of ordering, then we need to completely redesign our operations. We just push the items as they come, as they, they're spawned by the threats. So we, we don't really care which of these two outcomes will, will, will happen. We just happy that either of them didn't damage the data structure. Everything's fine. Both are still valid linked lists. You can traverse them and everything is OK. But it also might happen that these two threads are competing are running this code almost simultaneously, so they are interleaving these instructions 1 by 1. So what happens is that they both take the pointer to the root at the same time or virtual at the same time and then one of them will replace the root and then the second one will replace the root. So in the end, we will end up with a data structure which is not no longer valid. This is like a, this is damaged data structure. We don't want this. We will lose one of these two nodes because it wouldn't be pushed properly into the linked list. So this is something that shouldn't happen. And yeah, it it might happen in any other way that there may be a different outcomes of this. So there are several other options. What what might happen? So if we have this piece of code that updates some data structure do something which is kind of a critical, we cannot allow that multiple threats perform this operation simultaneously in this manner because some damage might come out the the data structure may be broken or something. So this piece of code is called a critical section. We will, we can designate this code a critical section. And what happens is that we have tools, tools that usually ensure mutual exclusion, which means that within the critical section, there could be only one thread at a time. So there are some mechanisms. I will introduce them that will ensure mutual exclusion of threats within this designated part of call code call critical section. By the way, it might be 11 routine, the code might be scattered among multiple routines. It it doesn't matter. It doesn't have to be 1 continuous piece of code. The code can be scattered among multiple pieces of of your functional design or something, so it might be more complex. But for this let's just assume it's something like one function or one block of code. OK, what we need to do is perform synchronization. The synchronization is an unfortunate word because it's very overloaded. What I mean by synchronization, we usually mean the process synchronization or the data synchronization. But sometimes we need we talk about the abstraction, about the concept, sometimes we talk about the actual implementation. So yeah, the word synchronization is somewhat overloaded. The basic terms are process synchronization. And by process synchronization I mean that multiple units of scheduling, multiple threats, for instance, need to perform some operations, some handshakes, some protocol. So they agree on some order of things. So for instance they will perform a handshake and they will agree that I will execute this method 1st and you will execute it second and you will wait before I terminate the execution or something like that. So they will agree on how they will proceed with some instructions. Data synchronization is usually a higher level of abstraction which is designed to protect data structures, and they usually tend to either keep multiple copies of data coherent or somehow otherwise maintain data integrity. So like in this case, it will maintain that the data integrity will the the pointers of the linked list will remain in the right order. So they will allow only this version or this version. They won't allow any of these versions of of the data structure to happen. However, synchronization comes at cost. The obvious cost is it will have some overhead. Everything we do have some overhead. So yeah, synchronization isn't for free. We need to perform some additional operations. We need to invoke the hardware instruction in certain way. And everything has overhead. And also synchronization if used improperly can lead either to deadlocks, I'll explain them later, or to starvation, which means that the processes needs to wait for a long time or perform idle operations or something. So let me start with synchronization primitives, because they are the tools we need to use to perform synchronization. So we need these tools to do something in parallel code, so we make it safe for parallel execution. These primitives are usually implemented by operating system and they are provided to some libraries. So usually, for instance, if you use C#, you already know C#. Right now that there are multiple classes, multiple tools in C# which are technically translated into some system operating system calls, but you can use them for synchronization. They divide into two basic groups, active and passive. The active ones are actively waiting which means they are consuming CPU whilst waiting. They are eagerly trying to perform the operation just like knocking on the door waiting for the door to open. OK and the most important thing is they are actually executing instructions while waiting. They are usually using some testing conditions in in a loop. So they are performing a loop which tries to do some operation and and repeatedly fails until some door is open, some lock is released or something. This is usually used for short short pieces of code. If you have a short pieces of code where only few instructions happen, this is a good choice because yeah, if we perform a few additional instructions to knock on the door, it usually doesn't matter. Passive or blocking primitives usually suspend the unit of scheduling, suspend the threats that are running until the resource, the critical section or something becomes available. So this works also in hand to hand with the operating system scheduler. So this allows a threat to be blocked, to be put out of scheduling until the resource, the desired resource becomes available. This is useful for situations when the waiting might be long, when the resources are locked or allocated for longer periods of time. So this way the threats is put out of its misery temporarily. So it won't consume the resources, it won't consume the CPU instructions. For that we need some hardware support. And the basic hardware support is via atomic instructions. The basic atomic instruction, the one actually the only one we need in the end is called test and set or compare and swap. These are two names for the same thing. Task is usually used in the parallel programming. Test and set is usually used in operating systems, but it's the same instruction. It performs the same operation, OK. And what it does is it's given a variable. This is like the T stands for some type. It's like an integer, for instance, OK, it's a small enough variable. The T isn't something large, it's not a data structure. It's a integer, float, something very small. And you are giving it two additional parameters, the old value or the expected value. So we already assume that this variable has some value and we have some replacement for it. So we know what's currently, we should know what's currently in the variable, and we have some new value, we want to replace it. So what we are doing is that we test that this assumption still holds. It's still, it's true that the variable holds this old value, and if it does, we proceed. If it doesn't, we terminate with failure. So this atomic instruction can only proceed if the value within this variable still holds with our assumption here. Because if it doesn't, what, what, What does it mean if? If this is false, it means that someone else been there first, some other threat came there and perform an update before us, right? So if this happens, we need to stop our operation and try it again, rethink our position or something. So this is like a safety guard that prevents other threats to perform modifications before us. And if it happens, we will fail because someone else got into the line before us. And if it doesn't, if this holds, if it hasn't been modified, we can proceed. We write the new value into this variable, we just assign it, and then we report success. And the trick is that this code, which looks like a huge piece of code, there is a, if there is a return, it looks like a function call. This is actually just a seep SODA code for an instruction. This is performed by one single hardware instruction which makes sure that all the data operations, the whole thing happens atomically. So no one else, not even another core can access this variable when this instruction is being executed. And that's the whole point that this huge piece of code, well, huge from the perspective of individual instructions, these, these all operations are performed at once by the hardware. That's what, what's what to give us the edge, that's what to give us the advantage. And using this instruction we can easily implement active waiting waiting primitives because we can just use it in a loop to test some condition. For instance, there might be a variable that guards the critical section and when a thread wants to enter the critical section, the thread needs to write its ID into this variable. So initially the variable is 0 and I'm trying to write my ID in it, so I assume it's zero and my value is my ID. And when I succeed, I know I wrote my own ID which is unique into this variable, so no one else got there before me, right? And when I leave the critical section, I reset the value to 0. That's it. That's how I implement one type of active primitive. And using active primitives I can easily implement within the kernel of the operating system. I can easily implement more complex passive blocking primitives. OK, the easiest thing I can do I just described. It's called spinlock. It tries to write some some variable, usually using this SAS or TSL. So it can it can enter some small piece of code, perform some modifications and then retreat. The most fundamental passive primitive is called semaphore. Yeah, it's not widely used actually, but it was the first primitive ever designed. So we always starting with the semaphore. But I will simplify the semaphore on the next slide into a mutex. And the mutex is the one which is currently being used for the passive passive blocking. So semaphore works in a similar manner like a semaphore on railroad. You know, if you have a railroad, there is a train going next to the semaphore and when it does, the semaphore drops. So no train can enter the trail before the train reaches another marker or something like that. So here it's slightly more complex. The semaphore has a counter, and the counter is initially set into how many threads can simultaneously enter some critical section. So it's more versatile because it can it can allow multiple threads, but the number of threads is pretty fine, and we can limit to one if we want to. But this counter is usually initially set to number greater than 0 and there are two operations, down and up. Down is used when you're entering the critical section when you're trying to allocate the resource, and up is used to release the semaphore. This is the same way with the semaphore. When the train passes, the semaphore goes down and when it leaves the the the part of the rail which is protected, then the semaphore goes up and the down operation it always test the counter. The counter is critical and if the counter is greater than 0, it's decremented. So we are counting how many threads has passed this this down marker, this this semaphore and if there are no more slots available, if the if the counter reaches 0, then the the US denotes unit of scheduling which you can imagine as a threat. So you can push the current thread into a queue which is associated with the with the semaphore, and then the thread is blocked, it's suspended by the operating system. And vice versa, when we performing up operation, we need to check whether the counter is still 0. If the counter is 0 and they are waiting some other threats in the queue, then we just stop one of the threads from the queue and start and allow it in immediately. So we unblock it and the thread is allowed into the critical section. By the way, if you call down, if your thread calls down, it will either immediately enter the section or it's blocked until it can enter. So once you emerge from this call, you know you can enter the section, you know you can perform whatever you need to do. So this happens if your thread gets blocked, then other thread will unblock it in the app operation and otherwise if there is, if the counter is either above 0, so we we have nothing, nothing in the queue or the queue is empty. So it's the edge case where the counter is still 0 but no more threads are waiting in the queue. Then we simply increment the counter. There is no one else to wake up, no one else to unblock. Is this clear or was I too fast? OK, and obviously to make this work, these operations need to be need to be made atomic. So these but not by hardware, but by operating system, by additional locking, by additional atomic instructions. But both operations are considered to be atomic from the perspective of threat execution. That's why this this whole thing works. We are now not usually using semaphores because they are kind of obsolete, but we are frequently using mutexes. Mutex is a semaphore which has its counter sets to 1. Easy, because usually you want to. You want to allow only a single thread into the critical section. You want allow only a single thread to do something. So if we set the counter to one, we will get something which is more often referred to as a mutex. And again, we usually refer to the up and down operations as a lock and unlock. It's slightly more readable than down and up because yeah, not everybody knows nowadays what semaphores are for, because we don't have the regular semaphores on the railroads anymore. We have lights. And so lock and unlock is probably more to the point, a few other examples. A barrier is a primitive that allows a rendezvous of multiple threads so that the barrier is executed by all the scheduling units, all the threads that wants to meet at a certain point. For instance, let's say that the threads are cooperating on some initial part of your work there. They're initializing some data structures at the beginning, and you need to make sure that all the threads have finished before you proceed with the computation. So you place a barrier between the initialization part and the computation part of your code so that every thread that was initializing something will meet on the barrier before all the threads are allowed to proceed after the barrier. Yeah, in various languages you also have some specific constructs. For instance, in C and Java you have synchronized and locked. Which are the most simplest ways how to introduce critical sections or locking to your code. Basically in the locked section of code there is some mutex in behind, hidden mutex, hidden lock somewhere in behind. So this is just the way how you can denote critical sections and the the C# environment. The C# runtime provides the locking mechanisms provides the synchronization primitives for you for free. So it's it's much easier in higher level languages to perform this type of synchronization. On the other hand, it's not a good idea to write a lock or synchronized everywhere, because if you do, you might end up with a code which will be very slow. Or even worse, you can you can reach into a deadlock. Yeah, Speaking of deadlocks, deadlock is a very specific situation you can create if you lock, use synchronization primitive inappropriately. These are like real world examples of a of a deadlocks. Yeah, I, I always wanted to try this one somewhere, but I've never got this this question anywhere, unfortunately. But maybe you do so. So if you do some hiring interviews, you can try that. This is a very, very illustrative way how to depict a deadlock. If you, if you watch this, if you decode this image properly, none of these vehicles can move. Yeah, at least none of them can move in the direction they they would like to. It's possible for some cars over here to turn right, right, and just avoid this, avoid this mess. And actually until some of these cars do so, until some of these cars will actually turn somewhere and and use a different Rd., they won't, nobody will move, right. So how to define deadlock more formally? There is actually formal specification for that. And informally deadlock is a state when the group of units of scheduling, group of threats and group of some resources by resources. You can, you can imagine critical section protecting some data or mutexes or something are in a state where everybody is waiting for an action which needs to be performed by someone else. It's like an informal definition. Formally there are 4 rules, they are called Kauffman conditions. And if all these four conditions are met, then you are in deadlock. So first of all, there is a mutual exclusion of the resources. So we really have some locking mechanisms, some resources that allow that only one thread, only one unit of scheduling is accessing that particular resource at a time. 2nd is all the threats, all the units of scheduling are performing hold and wait. So they are accumulating resources. When they get a resource, they are holding that resource and they want more. So they are trying to accumulate several of these resources, at least two. And there is no preemption, which means you cannot remove a resource from a threat from a unit of scheduling which was previously allocated to it. So once a threat locks a mutex for instance, you cannot break this lock. You cannot unlock it without causing some external damage, without violating semantics of the threat, for instance. And finally and most obviously, there is a circle in the weighting graph. So if you depict a graph where the edges are between units of scheduling and resources and thread between thread and resource means that this thread holds this resource, sorry, this thread wants this resource. And between resource and thread is that this resource belongs to this thread, is held by this thread. So if you create this graph and you detect a cycle, a loop in this graph, then you're in deadlock. OK, there are several ways how to prevent deadlock. But before that, before I get to them, let me give you a very similar example, practical example of what can happen in a real code. So let's say I have two mutexes, yeah, in C# for instance, and I think mutex, I'm not entirely certain. These methods might might be named slightly differently, but basically the mutex have two operations, lock and unlock. So thread one performs lock and lock on both mutexes and thread 2 performs lock and lock on these two mutexes. But unfortunately it takes mutex 2 first and mutex one second. So what might happen is that thread one performs this lock and then thread one is scheduled away, it's paused. Thread 2 comes in and performs lock on the second mutex and then it tries to look on the 1st mutex but it can't because it's already held by thread one. And then thread 1 resumes and tries to look at the second mutex but it can't because it's held by thread 2. So they both waiting until the 2nd until the resource they want is released. Is it possible to create a deadlock where each thread is only controlling 1 mutex at the time? Well yeah, if you have a regular normal mutexes, you can try to lock the same mutex twice. So yeah, you can have one mutex and one thread is trying to relock the mutex. So technically, yeah, you can create it with one thread and one mutex if you don't have recursive locks. But that's just a technicality. So as you see, this is what can happen and actually the solution to this problem is rather simple. Actually in this case it's simple. It may not be always possible in your code to do so, but if you have code like this, it's simple. We just need to flip these two lines. If you can impose an ordering on all the locks you have in your application. So you have like all the mutexes have the unique numbers and you say hey, if you need to lock multiple mutexes, you always need to do that in that order. So you need to always do that incrementally. So if you want to lock M1 and M2, you always need to lock M1 first and then lock M2. If you say that, you're fine because if there is imposed ordering, there will be no deadlock because 1 lock will be covering all the locks after it in it's in this ordering. If you didn't catch that, no don't no worries. It's like a technical thing that you can came up to that later when we talk about more detailed locking in programming parallel programming courses. To conclude this part of lecture, there are few synchronisation problems. It's like a allegories and it's a good for you to know them because they usually depict some situations in the real. Like actually the allegories are like from a real world and they depict some situations in the programming world. But if you know them, you can think about programming and locking, parallel programming and locking in a certain way, which is helpful. So that's why we are introducing these these allegories. Let's start with the simplest problem and probably the most profound problem in parallel computing, and that's producer, consumer. So imagine a situation that you have two entities. One of these entities is producing something and another entity is consuming it. So yeah, regularly in the real world, this would be like a factory and this would be like a customer, right? And in between them there is a store, there is a warehouse or something. Because we are usually not going into the factories to get our food, our cars, our clothes, whatever. We usually go to stores or to warehouses or somewhere which is closer to us. So imagine that if the factory is producing something to buffer the outcome, because usually the factory has a steady output and the customers act more randomly. So you need to put everything in some storage place so the factory can produce its load per day and also the customers can can access this this goods on demand. The trouble is what if the factory is producing too much, then the warehouse gets full and in that case the factory needs to stop. And vice versa, if the customer comes to the warehouse and it's currently empty because it's still waiting for the shipment from the factory, the customer will wait. And this allegory just explains a buffer in between two processes. So what happens if you have a buffer and two processes, 1 is outputting something and another is consuming something? This usually happens in a pipeline. If you have two stages, 2 follow up stages in a pipeline, then sorry I haven't explained pipeline in other contexts done in ACPU, but you can imagine. So this is like a buffer which tries to mitigate the problem that both processes are running in different speeds. Yeah, and sometimes it doesn't work correctly because if one of these two processes is much faster than the other, then it will always wait for the other. Another problem is dining philosophers. Imagine a situation that you have multiple philosophers around the circular table and each philosopher has a bowl of a Chinese food in front of them and there are two chopsticks across each bowl. So basically you have a bowl, chopstick, bowl, chopstick, etcetera. And as you probably know, a philosopher can do only two two things, think and eat. So they are sitting there and thinking and once they get hungry, they want to start eating. And what they need to do is they need to pick two chopsticks because I'm not sure if anybody here trying to eat Chinese food, but it's almost impossible to eat Chinese food with one chopstick, right? So you need 2. So you can properly pick all the good meat, good stuff from the meal and imagine situation that all the philosophers reached the same conclusion and meaning that they all get hungry at the same time. So they all perform the following algorithm. They will grab the chopstick at their right hand and then they try to grab the chopstick at their left hand, right. So if they manage to do this all at once, they will create a deadlock because everybody is holding the right chopstick and the left chip left chipstick isn't there anymore. So they're waiting for the left chipstick to be placed there. But because they're all holding the right one, it's a deadlock. So this is just another visualization of the loop, which is inherent in the Kauffman conditions. So in this case, the loop might be much wider. And yeah, you might say, hey, this will never happen, right? Because it's so improbable that multiple philosophers will reach to the same at the same point to the conclusion that they all want to start eating at the same time. Well, it will happen sooner or later, because yeah, it might not happen in real life because the people are too unpredictable. But machines are much more organized. So if these are not actual philosophers but they are processes, it will happen sooner or later. Yeah, I can give you many examples from real world. But programmer said, hey, this super unprobabilistic conditions couldn't meet all together. And if somebody says that, it usually happens twice a day, so you can rely on that. So OK, let's augment this algorithm. Let's say, OK, it's a bad idea to hold your chopstick forever if the second chopstick isn't there. So let's augment this algorithm. Let's say they take the right chopstick, and if the left chopstick isn't there, they release the right chopstick and try again. Seems reasonable, right? Well, it's, it is reasonable because again, in normal world it's it's unlikely that they will be all performing this task at the same pace. So one of them will finally grab the right chopstick and the left chopstick at the same time, which will break the circle and allow the others to eat sequentially or partially sequentially, but at least it will break the deadlock sequence. But if they are very organized and they are all proceeding at the same pace, they are all of the same age, for instance, what will what will happen? They will take the right chopstick and the left isn't there. So they will release the right chopstick and then, then try again. They will take the right chopstick and the left isn't there. So they will repeat this routine all over again and again and again, which is even worse than deadlock because they not only cannot, they can't eat, but they also consuming a resources that they're, they're spending their energy moving there right up and down. So it's even worse than deadlock. So these are things that we need to consider when, when dealing with operations like this. And by the way, one solution could be put a salt glass in in the middle and say hey, everybody who wants to eat first needs to put salt in the into their meal and then then they take the chopsticks and then they release the salt. OK, so if there is only 1 salt in the middle, only one will win the salt and the the person with the salt will start eating and then it will release the salt. So someone else can try that. So place a small lock aside before the grand locks can be taken. Another problem is a reader and writer. When considering locking, there are usually more complex scenarios in the real world. Let's assume that we have a data structure. Yeah, this is less of an allegory and more of a programming problem. So let's say we have a data structure. And usually what happens is data structures are more often read than written to. If you have a database, you much more often perform reading operations on the database than updates of the database. So it's a good idea to separate these two types of operations because readers can work simultaneously. And that's a benefit. You don't have to lock the readers out each other out. You can allow all the read simultaneously in. That's a good idea because you have more parallelism, you have more performance. On the other hand, if you have a writer, then the writer must be mutually exclusive to all other writers and also to all other readers. So a writer must wait for all other readers to finish before a writer can go in and perform modifications of the data structure. And since this is a very profound, a very common problem, we have a specialized primitive called Reader writer Lock, which is designed specifically for this operation. It works similarly to mutex, but when you're locking this structure, you are specifying whether you would like to perform a reader's lock or a writer's lock. So whether we are trying to read or write the data structure, the tricky part is how to make this fair and efficient. Because imagine that if you have multiple readers and then the writer came in and writer asks for a lock. So the writer needs to wait because it's a lock, so it cannot acquire the lock. So it waits in a queue until all the readers are done. That's how it must be. But imagine that while the writer is waiting, another reader came in. Should this reader be allowed to read or not? And the answer is obviously difficult because if you allow it to read the data structure, you may increase the performance because it it might happen that this writer will will read it, read the data very quickly along with all the other finishing readers, and you just enabled another reader to just jump in and and do whatever it needs to do. Then again, if you allow every incoming reader in, then the writer will starve because it's possible that it will never be allowed to perform the update. So it's a tricky question. And finally, the last problem is sleeping Barber. Yeah, you may notice that I need a haircut, but there's just a coincidence actually. This is about Barber shop. And at least in in this terminology, I think the Barber shop is about shaving the beards, right. So in the, in this scenario, there is a Barber shop and the Barber has one chair for a customer which is currently being served and multiple chairs, chairs for customers that are waiting in queue. And what happens is when the the new customer comes in, then the Barber is sleeping usually because he has nothing to do. So the customer wakes the Barber and the Barber immediately starts shaving the customer. And if customer comes and Barber is already at work, then the customer takes one of these chairs. And if customer comes in and all the chairs are full, then the customer needs to go away. Yeah, again, this is a very simple scenario, but it completely it accurately describes what happens at a web server. For instance, if you have a web server and a new request comes for a for a web page, then it wakes the waiting thread that's locked on the on the socket, and the thread starts to serving the answer to the to the to the incoming network request. If you have a multiple request, multiple requests, they need to be queued somewhere, so they wait until the 1st request is served before they take their turn. If you have too many requests, you need to have a fixed number of chairs, otherwise your server will run out of memory because it will be holding too many requests in a queue. So at some point you need to start rejecting the requests right away. And that's what usually happens if you have overloaded web server, you got rejected, your request got rejected right away. They aren't waiting in a queue. So these problems were to illustrate some of the common scenarios, some of the common situations that happens in a parallel or concurrent environment. Sorry about rushing through this lecture. I was trying to not not prolong this, this session too much. So are there any questions regarding parallel programming? No, not at this point. OK, let's move on to the final part. Few final words and few examples about testing questions. So first of all, just a quick reminder what was covered in this course. First of all, C++ will not be explicitly examined during the test. But still you need to know C++, C++ at some level at least to understand some questions or to express some pieces of code. So you won't be coding during the test, but you need to at least read the C++ during the test. Then we got several categories. Yeah, in CPUs we cover things like instruction encoding and execution. So you need to know about that. Again, you don't need to know from the top of your head the whole instruction set of a MIPS. That would be very difficult. But if I show you some MIPS instructions and say, hey, this is the instruction that performs branch if equal, which means it it jumps. If these two registers are equal, then you need to be able to use this instruction or you need to be able to assemble a few instructions or read few instructions when you're explained what these acronyms are. And you need to also know to how these instructions are mapped to the higher language. So you need to be able to translate from C++ to assembly and vice versa at some fundamental level. And of course, things that are related to that, you need to know how registers work, how registers are used with the instructions obviously, and some basics of the hardware architecture so that there is some pipelining and stuff like that in the memory. We covered some fundamentals regarding addressing in DNS. Yeah, this is like a rehearsal for you, because that was already covered in the previous course. How the data structures are represented in memory, how the memory allocation works, and some technical details like how caches works or how NUMA works. In the programming languages, we mainly focus on compilation and runtime. So you need to know how compilation works, linking, dynamic linking, how the program is loaded into memory when it's executed, etc. How the memory is organized with an executed program so that there's some code segment, some some data segment, heap stack, stuff like this. How things are organized on the stack, which goes hand to hand with calling conventions. Because when you're calling a function, how the data, how the arguments are transferred to the call callee from the caller and how the result is from the callee transferred back to the caller, what's on the stack, what's in the registers, things like that. And also we covered some basic things about portability and, and, and virtualization and, and garbage collector. Yeah, the the largest part was covered by operating system. So yeah, we got several points, devices, processes, file system and virtual memory. It's like 4 topics, 4 main topics. And basically you need to understand the basic principles in all. So for instance, you need to know how interrupts work, to report that something happens on a device to the CPU. That's a very important thing. You need to know how scheduling works. So you need to know about processes, threats, you need to know some algorithms, you need to know how file systems work. So we got 2 examples. For instance, if I show you FAT file system, you should know how to use it, how to read it. And the same goes for virtual memory. There are some algorithms within virtual memory, how the address is translated from virtual address space to physical address space, how new page is allocated, how new frame is allocated, when new page is required to be accommodated, et cetera. And in the final parallel processing part, which we covered today, yeah, the fundamentals like what's race condition and what's deadlock, that's obvious. You need to know what that is, and you need to know some synchronization primitives, how they work. If I ask you how the task instruction works inside, you need to be able to write it. If I ask you how the semaphore works inside, you need to be able to write the down and up functions, et cetera. So I asked you about revising the previous lectures from the throughout the semester. If you have any specific questions regarding anything we covered this semester, fire away. I think I'm screwed. No, no you aren't. Well, if you, if you're screwed, that better be screwed here in the first year then then wait to the other years because things are getting tougher. So I'm sorry to bring that to you. But we are still at the very basic level. The follow up courses will go much deeper. So don't look at it this way. You just need to pull through. You need to muzzle it. No questions. OK, I will ask again at the end. Let me give you a few piece of information about the exam. So first of all, the exam, the exam dates are already in CIS. So you can you can enroll for the exams. We expect there will be no other dates unless we detect some major problems like many of the dates were colliding with other other exams or something like that. So unless we detect some major problems, which is like a systematic not minor errors, then we are not planning to plan any other dates. We are not sure whether we will be able to write any. Sorry I I was talking about for I've been talking for too long time so I'm not pronouncing correctly anymore. Sorry, my speech processor got damaged. Sorry, the we are not sure whether we will be able to provide any terms in September. So please do not rely on September terms. We will try to muster maybe one term in September for real emergencies for those who really weren't able to attend during the the summer part. But try to squeeze in, there are like I think 19 dates so everybody can pick their own favorite date. And I hope that you will find some dates that are not colliding with other exams. Then again, each date has only 20 places, 20, The capacity is limited. So yeah, make sure you enroll sooner than later. Again, I told this at the beginning of the course. The credit is not required for the test. The credit is completed separate from the exam. So you can visit the exam independently on the credit. If you fail the credit, no problem. We will acknowledge. We will, right? We will copy your exam results from this year to the next year. We don't want you to attend these things twice. If you just pass exam this year and you don't finish credit this year, then you can retake only the credit next year, no problem. This exam is focusing on understanding, so if you just memorize the slides or memorize the lectures, it probably won't help you. So it's focusing on whether you understand how the algorithms, the data structures, anything we describe to you works. So questions are usually designed in a way that you need to apply something. Yeah, this is the how the memory looks like, trying to apply this algorithm of memory allocation of address, translation, whatever, and tell us the result, compute the result or something. So yeah, I know this looks like additional extra skill, but it isn't an extra skill. It's just an direct application of what you should know. So maybe together with some high school math, Yeah, you need to count to maybe not to tend to more, but yeah, you need to be able to add to 3 digit numbers together, stuff like that. Yeah, that's like a normal skill to do. You need to be able to read English. Yeah, that's another skill that that should be obvious. But beyond that, you the applications of the knowledge is direct. So you just need to know the stuff from the from the lecture and be able to apply to a very simple example, simple problem. The structure is as follows. You can, you have 10 questions, 20 points. So obviously each question is 2 points, right? No, actually there are one point and three-point questions. But for that later it's a test. The most important thing, it's a test, which means we are counting only the answers, not any intermediate thoughts, processes, whatever. We just collecting the answers and most of the answers are written as a you just check some options. You just select an option, you just put some options in order or you write a number or a few numbers, that's it. And we taking only the answers. I know it's tough. It's it's more difficult than if we are just if we would interview if you if you perform oral exams. But then again, to be completely honest, if you write a code and you made a small error like you just add +1 incidentally at the end, it made happen that the two trains collide or the plane falls or something. And nobody will care that that you just make a tiny mistake. But the whole procedure of your deduction, reasoning or the rest of the code worker was correct. No, the one mistake matters. So that's why we decided to take this test this way. And you will be given enough time to double check, triple check how many checks you need, your answers. So take your time. Normally the test is designed for 65 minutes. That's the number we just calculated using some common metrics for the amount of text and amount of questions. But it's a soft deadline. So it's like, yeah, after after hour and 5 minutes or so, we will start start gently reminding you that it's time to wrap things up and finish. But if you still need 10 more minutes to double check your answers, you will get 10 more minutes no problem. Yeah, you will get how much time you need within a reason. Yeah, we can stay in the labs till the next day. The the labs are allocated for some amount of time. But it really does. We don't really care if you take few more minutes or no, it's up to you. So take your time to do the stuff properly, take your time to double check, triple check your answers because that's what you should do in a real life. So we want to you to do this on the test as well. This is the grading scale. So it's a slightly, it might be strict or it might look strict, but then again, it's a test. So many of the questions are kind of easy. If you see the answers and you know the stuff from the lectures, you should be able to pick them easily enough. So yeah, this looks harsh, but our past experience indicate that we set this scale properly. Actually, many people that did well during or had regular questions during the semester ended up with mark one or was mark 2. And people I have never seen throughout the semester usually end up with mark 4, which is a good indicator for for me that the test scale is set properly. OK question times since it's a test that some part of the test is covered by simple single or multi choice answers. So you select one or multiple options from the answer list and again, you need to make this correctly. So yeah, in the single best answer it's obvious you need to pick the right answer. In the multi choice test, you need to make all the check boxes that should be checked, checked and that shouldn't be checked are unchecked. So you need to create the proper combination of all checked and unchecked answers options to make the answer correct. And by the way, this the single option tests are the single option questions are called best answer, which means they might be multiple options which are seemingly similar. And in these cases, you need to pick the best one, the one that answers the question the most accurately, right? So sometimes it's slightly more difficult than a regular single answer test because if you have like 3 options and two are completely idiotic, then you can kind of deduce that the third option is the best one. But sometimes you can have like two or three options which are seemingly correct, so you need to read them carefully so you pick the right one. Sometime if a part of the answer, part of the option isn't completely correct, then obviously it's not the right answer. It's not the best answer. Then there is a multi choice with order. So you need to pick the right options and place them in the right order. This usually is used for the coding questions if you have like 10 lines of code. So you need to pick the right lines and put them in the correct order so the program makes sense. That's it. That's how we test coding. But by the way, by coding I mean you don't need to design A specific algorithm or something. You just usually need to assemble something that was previously on slides or you need, you should know from the slides. And finally, there are some tests questions where you need to answer with a number or multiple numbers. So there are text boxes and you fill one or multiple numbers in it. Like compute how many page faults will happen. In this scenario. The questions are divided into two parts. One are quiz questions. Quiz questions aim more directly to the point regarding the knowledge. So basically there is a question, there are some answers and you need to answer the question by remembering some stuff from the slide. So this is less application, more about memorization and they are only one point each, so they are less valuable. And basically this is an example of such a question. The question will read the instruction set ISA defines, right? And it's a single best answer. So if you really know what is ISA about. And yeah, I assume that some people don't know that at this point because you didn't bother to read through the previous slides and to watch the previous lectures, right? Yeah, I understand that. But you should do that before the test. And before the test, when you will be properly refreshed and your memory will be properly joked, you will arise to what conclusion? Specific parsing of individual phrases were kind of often not no, there there is no trickery like don't look for tricks like missing dashes or missing commas or something like that. No, the the best answer should stand up to you if you know the stuff. So it's it's not like we are missing an apostrophe somewhere and that's that's what causes the the problem. Yeah, if, if, if there is a source code, you need to read it carefully, because sometimes a a; In the source code can make a difference, right? But in the text, in the regular text, there will be no trick rates. It's not about the syntax, it's about the semantics. If you find out a typo or something in the text, it's just a typo. It's not like a incidental error. So are there any? Are there any guesses what should what the correct answer should be? The problem is, I don't remember what exactly ALU stands for arithmetical logical units. So this is the electrical stuff that actually does something with the numbers. Compute addition of two numbers or something like that does that. This is hardware, yeah. The instruction set architecture, I don't think that defines hard. That's right. So it's not any of the one. That's right. So, so you can, you can rule out anything with Alus. You can rule out caches. Yeah, that's it. I'm not sure if I'm reading everything right now quickly. So yeah, if you if you detect any hardware strictly hardware related to a word, then you can rule that out because hardware architecture isn't part of ISA. That's one thing. You can you can do another thing. So now I have to figure out if the instruction semantics is a specific phrase or if it's like a general phrase that needs to proxy. Well, it technically it can be both because sometimes we need to use exact terms in in the questions. But here it's it's a normal term. So it's like a semantics of the instructions what the instructions do. It definitely includes construction semantics and registrants. Yeah, it must. That's right. That still needs us. Well, you mean this one? I think we fall underneath that are possessed. But I know I feel confident about memory management. I don't know how people like the IO, I understand, but if you don't have defined how the CPU handles the memory, how it accesses the memory and how it's performed, the virtual memory translation, and if you don't define how the CPU should handle the IO, at least on the abstraction level, then the CPU wouldn't be able to do anything. I feel like I feel like you're describing. Maybe I'm thinking of some other higher level content, but I feel like there's something that like we've said, there's something that technically the instructions that architecture doesn't define and believes, like technically there's some additional term and all would be like, yeah, don't, don't try to look at it too harshly. It's like we are not trying to trick you. So your initial, your soaked up information, I understand. So your initial assessment is was correct. You need to rule out everything that's related to hardware specific design. OK, so you rule out the Allus, you rule out the caches, you rule out the micro micro instructions and you end up with this, this and this. So we have three options left. And if you check them carefully, you will find out that none of the things that are left in these three options are related to hardware. So they all need to be in the ISA. So the most precise 1 is in this case the longest one because it defines everything. This is, this is too short. This is, this is missing memory management, for instance. This is missing something else. This is missing instruction semantics, for instance. Yeah, right. So in this case, the most complete answer is the the right answer. However, as you can see, the longest answer isn't always the right answer because this one contains a few things that shouldn't be there. So you need to rule out the wrong answers. And then probably the most precise answer, in this case the longest answer, because it enumerates everything important, is the right answer. OK, you will be allowed to ask questions. Yeah, you will be allowed to ask questions, but you might not receive the answers. So not receiving that. If you if you ask something that's that you should know, then you won't get the answer. But yeah, you can of course you can ask questions. I just, I I know that I have the lean really heavy on that on the quick phrase, this kind of bluntly, the kind of exam that has the potential to give me a panic attack, which I recognize this is being sorry about that. I'm the subject matter. This is not Yeah, I I understand that I had a panic attacks from from mathematical analysis because that was something I cannot learn. But yeah, sorry. So let's move on The another 5 questions would be for three points. So they together they will form the 20 point barrier. And another example would be a numeric question. So another typical example could be how many page faults at most will occur when we copy 2 kilobytes of memory in user space. So it's a similar question to what we actually answered today. You need some set of assumptions. So we need to properly said that there is a 32 bit address space. That's very important. What is the size of pages? That's again important. There is A2 level paging again that's important. So we need everything set up. This is like the easiest thing we can set up because this is actually the the example from the slides. But these numbers may vary. We can use different size of pages, we can use three level paging for instance. So yeah, we can use different different setups, but this is the most simplest 1 and we assume there is enough physical memory so each page fault occurs only once. Once we satisfy a page with appropriate frame, once we allocate the mapping, then it won't happen again with this particular page. OK, the operating system is simple, which means that every time a page fault happens, it allocates. It provides mapping only for that one page because in theory, when a page fault is detected, the operating system might be smart and provide mapping for some other pages like to provide some future mappings that are expected to happen. But no, it's not happening over here. The operating system is simple and per one fault allocates only one frame for one page and we are ignoring the instruction fetching. So let's assume the instructions are already in memory. OK, No, fully recognizing that is not standard. So when a page fault happens, it doesn't just correct like the single byte that was like it corrects the single page. The entire page gets correct. In this setup, as I describe it here, we assume if a page fault happens, so a page isn't, the mapping isn't properly set for one of these pages. The page will be filled in properly fixed by the operating system and the instruction will be restarted. So once a page, this is like a question. Another way how to ask this question will be how many pages will be accessed, how many unique pages will be accessed throughout this operation? Data pages, we are ignoring instruction fetching. That's another way how to pose this question. Let's go through that. So first of all, there are, sorry, 2 kilobytes of memory, so 2 kilobytes of memory, Yeah, No, no, no, we have 4 KB pages. So first of all 2 kilobytes. 2 kilobytes can span over how many pages? Just two pages? That's right, we cannot possibly span 2 kilobytes over more than two pages. The best way how to imagine this is imagine that the last byte is at the at the boundary of page. So last byte is here and then count how many pages it will cover. So if the last byte here then the rest of the block will fit another page without problem. Because we have 4 gigabyte page, we are copying the data, which means that this is the source and there is another buffer somewhere else where the same happens for the data destination. So the destination where we are copying the data will also be covered by two completely different pages. We don't know whether the source and the destination are next to each other or at the opposite sides of the virtual other space. We don't know that. So we need to assume the worst, that they are widely apart, so we need to compute them independently. There might be a different assumption. It might be we are moving the 2 kilobytes in the memory right after the 2 kilobytes. No problem. In that case, we are counting like 4 KB block, right? So you need to assess this. And finally, when you need to access these and these, you do the same stuff with the page level table, page table. So we need to ask ourselves how many page tables at the second level we will need to address these and these, right? And the maximum, of course. So it's eight in this case, because we have like 4 data pages and each entry for each data page can be at the separate second level table page. So in this case it's 8. And if you understand how the paging works, you will be easily able to arrive to this conclusion phrase user space. Now I suspect that I don't know because I mixed the lectures. Yeah, maybe we should write a virtual address space, but that's the same thing. User space is the virtual address space. It's the space where the application runs. So yeah, maybe I should rewrite it on the slide, it might be more clear. But yeah, this means virtual address space, of course. See, that's why subjects like this kind of a lot of the times terms are overloaded. I see, I see. Practical question #3 or work again. This is like something we've done several times. So assume that you have this structure in C first item is corrector, then there is double and there is int. And we have array of these structures so that the array of 20 for instance. And the question is what is the offset of this item? So if I write SA 3 dot count so I'm accessing this field of the 4th 3 means 4th right element in this array. So what is the address of the beginning of this integer from the beginning of the array? Is it best fit or is it just first? This question is exact. There is no best fit or sorry. Yeah, is always one byte. That's actually you should know that. But yeah, cars are always cars are like the smallest types there are. And yeah, we need to know the double is 64 bits and integer 32 bits, just to clarify, because this is not specified by C So this is additional clarification that is required. One bit for the car, 8 for the double, yeah, four and four for the in. That's right, that is so, except I'm not sure that's how it's supposed to be counted. Well, you can count it like that, but it won't help you much in this case, not fairly for. So let's figure out what's going on here. First of all, there is some padding, right? Because we need all these data blocks. All these variables needs to be aligned. Character is always aligned because it takes only one byte. Double is needs to be aligned to its size. It means to 64 bits, it means to 8 bytes and count the integer here needs to be aligned to 4 bytes, remember. So that's what I'm asking about. So that's what we need to do here. So imagine how this will be laid out in memory. There will be this character type. It will take 1 byte and then we need to add padding so the next position is divisible. We assume this one is divisible by whatever. OK, so this one will be divisible by 8. So we need to place additional 7 bytes here, virtual non used bytes so that the following double is properly aligned right. This will take 8 bytes. This address is divisible by 8 at least, so it's also divisible by 4:00. So we need no extra padding here. So the int will be immediately here. So what's the total size of this data structure? Isn't there a specific padding on the bar right? Should it I do not? OK, so since we are placing this data structure into an array, we need to make sure that not only this double is aligned, but also the double of the following structure. So the structure itself needs to be aligned to its largest type inside, so in this case to 8 bytes. So we need to wrap it up so the end, actually not the end but the beginning of the next one. But that's the same thing here is aligned to the size of eight. So we have like a 8 + 8 plus and we need to place additional 4 bytes here so it's properly aligned. So the two yeah, so well, it's on the slides just like 10 lectures before this one. And total size is 24 bytes 8 + 8 + 8 pair each structure S So if we know that, what is the answer to this question? 24 * 3 + 8 + 8, so it's 24 * 3 because we need to count all the previous structures before the the 4th one. That's right. And plus we need to this offset, so plus 8 + 8. That's right. You do the math. Yeah, you are allowed calculators during the test. So sorry You can use the one on on the laptop on on on the PC no problem. So everybody's clear on this one. Which one? This one So so 3 * 3 * 24 is fine. OK, so we skip 3 * 24 and finally we are accessing this this particular field. So we don't we have to skip everything that's before that. It's three times this structure plus this part of the structure which is before the count. So we take how much, how many bytes take these two fields. So this is the first one takes 8 and the second one takes 8. So yeah, two more questions. FAD data structure, This is more like a direct application of what you should know, but again, we need to apply some common sense to it so that you're properly aligned with everything. So the setup is we have a FAD 16. The 16 here, as you should remember, means that the data structure uses 16 bits per index, but it's not that important in here. It's important if I give you hex dump, but if I give you the table with numbers, it doesn't really matter because it just tells you additionally that all these numbers are 16 bit numbers and each cluster, each data block has 2 kilobytes, right? So each element in the Fed table represents 2 KB block on the data area. And what we are doing here, we are opening a file, a text which has some entry properties in fact. And the most important part extracted from this is that it starts at the cluster 3 and the size of the file is 7654 byte of course. And we are performing these two operations. So the user opened the file, then it seeks to offset 3210 and then the user reads 1000 bytes. And my question is write all the indices of all the clusters from the from the data store which will be loaded to satisfy this operation, assuming the fat table itself is already cached in the memory. That's what usually happens in operating system because the fat table is accessed all the time, so it's already in the memory. We are reading only the data box. We are not reading the fat table. First cluster has index 2 because before the 1st cluster there is directory entry, but that's not important. The first cluster in the Fed table starts with index two. You you can read it from here. 00 actually means in the in the Fed table, 0 means empty again. That's something which was described previously on slide and -1 is the indicator of end of the chain, just to remind you. So how shall we proceed? Sorry, speak up, please. I don't hear you first Gust is here. That's right. So what we? Actually, we are not reading the first cluster because we are seeking, you are talking about travel through the fat table if I'm. So let me put that aside for just a second because first we need to know which part of this fat table we're actually going to need to read. So first let's just read this. Imagine that the file is a continuous block of bytes and it's stored chopped by the size to the cluster by the size. So we have like 2 kilobytes, 2 kilobytes, et cetera. And 1st we need to know which part of the file will be actually read, because we are skipping this. So we are skipping 3210, which means we are skipping entirely the first block. This is just two kilobytes and we ended up somewhere in the middle of the second block, right? Then we read 1000 bytes. 1000 plus 23210 is still here. Or does it cross the border to the next class? It crosses here. So we are reading this part. So we will need to load. Since we are loading always the whole cluster, we will need to load the. If I number it from the zero. So the zero is here we skipping the zero. We need the 1st and the second cluster of the file loaded and now we need to translate which actually data blocks correspond to these clusters. So now we are resuming what you are saying. So just to explain to everybody, we first cluster is 3 and we are skipping that. So we are ignoring that. But we need to read the next cluster from the position three. Position 3 is here, so we read 10. So the second cluster, cluster number one is actually placed at the data block 10. So we are reading data block 10 and we also need to move on as you suggested position 10 exactly. So we need to find out the number of the next cluster which is 15. So the second or third data block block number 2 is stored at the data block 15. If we need to continue, then we just move on. 15 is over here, so it moves to 14, and 14 is here and holds -1 which means we are terminating the link chain over here. So this is a pointer chase. We are just traversing the data structure, the linked list, to find out which data blocks are there. But remember, we need to find out which of these data blocks are actually being read. That's perhaps best to start with. This is the first step. And by the way, if we need only these two blocks, we don't have to chase the pointer all the way down. Once we find out that the second, sorry, the third data block is at the data block 15, the third cluster is at the data block 15, we're finished. That's it. So the answer will be 10 and 15. The size is also the size also matters because we need to perform a mental check whether these two numbers together are still lower than this one. Yeah, but that's just to verify. In this case, the size is somewhat slightly redundant information because we don't care. But if you're if you're writing, for instance, we might be interested in that because the size might indicate when the file is growing, for instance. But in this case, this is just to mentally verify that we are still within the the the file for because if we perform seek operation beyond the file, it might have some repercussions if we if we exceed the size. Last week finally, yeah, finally few words about instructions. This is actually a simple 1, so and a question like this might rise. We have a piece of code in assembly language in MIPS instruction set. And this is like this is a simple 1. So we can we can make it quick and it's comprised 2 additions and one subtraction, right? And both editions are operating on different inputs. Remember this is the output, this is the input, this is the output, this is the input. So we are taking two different inputs for each edition, creating two different distinct temporary values. And finally, we're taking these two temporary values and subtracting each other. Which line of code corresponds to this assembly? Yeah, this is easy. The 4th 1, you can go the other way around. Since we have two ads and one sub, only these two lines actually qualify. So even to perform a quick pre filtering, only these two lines seems to be legitimate for this kind of operations. Yeah, Obviously we tend to play slightly more difficult pieces of assembly code, usually involving some jump because that's where things get interesting. But basically this is how you proceed. You just identify the instructions, try to map them on the basic operations. And then we need to identify this is completely different from this, right, Because we need to perform the additions first and then the subtraction not perform to this could be mapped to this set of operations. OK, that's pretty much it. Are there any more questions regarding anything? Yeah, nothing will be read because the question is which clusters will be loaded. So the if the user tries to read beyond the end of the file, no clusters will be loaded. The the the operating system will handle it some way. Actually, it doesn't matter how it will be handled. No clusters will be loaded because there are no clusters after the end of the file. Then you need to follow this chain to the end and enumerate all the clusters. So sorry well if if the read operation needs to be satisfied, you need to find out which clusters need to be loaded to satisfy this read. Only the reads and writes matter. Seeks are just jumping. Seeks are ignored basically. So he could just move you through this pointer chase, but they don't load any clusters. OK, any more questions? No, everything is clear. Hopefully a lot of stuff of money, but I couldn't even articulate the question necessary to get cleared up. If you need consultation later, don't afraid to ask me. We can set something up. But in the end, you need to just go through the materials yourself. Try to try to, you know, put them in in the right order so that you can understand them. But as I said, if you have any questions later, just write me. OK, great. So if there are no more questions, let the Mayhem mayhem begin. Good luck to you all. I hope I will see each and everyone of you exactly once during the exam.